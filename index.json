[{"authors":["admin"],"categories":null,"content":"I am a budding Data Scientist, passionate about all things Artificial Intelligence, Statistics and Deep Learning. I am typically learning about new developments in these fields, or writing about my personal undertakings in these fields.\nI also enjoy reading up on Astrophyics, Finance and Sports Analytics in my free time. Or basically just reading anything.\nMostly I\u0026rsquo;m just debugging my code.\n","date":1589500800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1589500800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vishnubharadwaj00.github.io/author/vishnu-bharadwaj/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/vishnu-bharadwaj/","section":"authors","summary":"I am a budding Data Scientist, passionate about all things Artificial Intelligence, Statistics and Deep Learning. I am typically learning about new developments in these fields, or writing about my personal undertakings in these fields.","tags":null,"title":"Vishnu Bharadwaj","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://vishnubharadwaj00.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://vishnubharadwaj00.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://vishnubharadwaj00.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://vishnubharadwaj00.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"","date":1596412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596412800,"objectID":"79e125cba728bf2ea0b720e891dc9a63","permalink":"https://vishnubharadwaj00.github.io/project/satelliteimage/","publishdate":"2020-08-03T00:00:00Z","relpermalink":"/project/satelliteimage/","section":"project","summary":"A multi-class Satellite Image Classifier (Deployed as a Web App)","tags":["Deep Learning"],"title":"Satellite Image Classifier","type":"project"},{"authors":null,"categories":null,"content":"(Originally published on Towards Data Science)\nI knew I was doing something wrong.\nI had built up a repertoire of interesting, practical projects. I had a couple of online courses to showcase what I had learnt. I even built a portfolio website to showcase all my projects and articles (which you can access here).\nBut, I still felt a gaping hole in the knowledge I had. A huge, gaping, chasm-sized hole. I felt like something major was missing from the equation.\nThat’s when I came across some absolutely amazing playlists on YouTube on building an End-to-End Data Science project from scratch. Some great examples are this one by Ken Jee, this one by Daniel Bourke and this one by Data Professor. (Shout out to all of them for some absolutely brilliant content!)\nI realized I needed to get started on an End-to-End Data Science project, stat (pun very much intended). Getting hands-on and diving right in, though uncomfortable at first, is the best way to learn something new.\nAnd so I dove straight in.\nUntil I got stuck. Again.\nAnd again.\nAnd again.\nBut finally, after hours of toiling, a couple dozen Stack Overflow searches (thank the heavens for Stack Overflow) and a lot of banging my head against the wall out of frustration after running into a wall of errors (this didn’t help as much), I was done.\nYou can check out my final project here. It’s a Data Science Salary Predictor (inspired and guided by Ken Jee’s playlist). I’m also working on a couple more of my own, this time with no reference or guidance, to really push myself beyond the perceived limits of my understanding.\nDuring the process of building these projects, I learnt a whole lot, not only from a Data Science point of view, but also about how to tackle the problem of structuring such a project in the first place; by making a lot of mistakes and learning from them, again and again.\nHere are some of the mistakes I had made before, and what you can do to avoid these mistakes in your next project:\nMistake #1: Not making my projects End-to-End. Typically, most of the time spent on real-life applications of Data Science is on Data Cleaning and Preparation. In fact, it’s widely been acknowledged that about 80% of the time spent is on cleaning the data and transforming into a form suitable for further analysis.\nAs annoying as it may be, Data Cleaning is an essential step in the Data Science life cycle. Not all the datasets you might work with in a real job or internship will be as cleaned and ready to use as a Kaggle dataset.\nIt might be very messy, and it might possibly be your job to clean it up and make it ready-to-use.\nAnd an equally important part of the project is productionizing and deploying projects. There’s a quote I’m seeing more and more of these days:\n Your project shouldn’t end its life in a Jupyter Notebook.\n Building projects and products that can be accessed by other people is one of the main uses of Data Science.\nA bunch of code in a Jupyter Notebook is typically of no practical use, but building a simple Web Application using a powerful and easy-to-use tool like Streamlit or a slightly more complex tool like Flask, to showcase what you built, just that little bit of extra effort, makes it a lot easier for people to see what you’ve built.\nIt’s very important to start with strong foundations and end with a tangible, practical product.\nPro Tip: Get your own data (web scrape if you have to). Clean it, preprocess it, do some feature engineering. And don’t forget to productionize your project with Streamlit or Flask.\nMistake #2: Not spending time on the things that matter most. Pareto’s principle (also known as the 80–20 rule) states that, for many events, around 80% of the effects come from 20% of the causes. Similarly, in a project, around 80% of the value comes from just 20% of the things you do. Equivalently, what this also means is that around 80% of the things you might do don’t really add a whole lot of value (around 20% only).\nI used to spend a lot of my time thinking and analyzing about the different options or different paths I could take with my project.\nI shouldn’t have.\nI should have just gotten started with something. I could have dealt with the after-effect of having chosen the wrong option, later.\nA simple example of where I’ve wasted a lot of time: Cloud providers.\nIf you’re with me so far, productionizing a project using Streamlit or Flask is very important. But also, deploying that project so that it can be accessed by everyone is also equally important.\nAnd there are a lot of options out there to do this.\nI started obsessively analyzing and researching all the different options to help deploy my project. Looking up all the differences between AWS and GCP, the pros and cons for each, what instances they provide, what costs I might have to incur, and so on.\nUntil I realized the mistake I was making. I just had to deploy it, period. It didn’t matter where.\nSo, I deployed it on Heroku for free. (They allow one free deployment, which I graciously used).\nNext time, I’ll deploy it on whatever I find to be the easiest. No second thoughts.\nPro Tip: Stop thinking, start doing. Use the 80–20 rule to your advantage.\nMistake #3: Not planning what the end result will look like. If I’ve managed to convince you by now that productionization and deployment are invaluable steps in your project life cycle and about the power of the 80–20 rule, then another equally important and frequently overlooked step is planning what the final result is going to look like.\nOkay, let’s assume you’ve agreed with my logic so far and you’ve decided you want to build a web application showcasing your project.\nBut what is that application going to do?\nAsk yourself questions about the UI, such as:\n What are the inputs it takes from the user? In what form are you going to take in the inputs? What’s the end result your model is gonna spew out? What form is the output going to be in? Are there any additional features that I could add that will make the user experience better? Are there some things I should avoid at all costs?  During my Data Science Salary Predictor project, I made the grave mistake of not thinking about the final interface and its layout. When I finally got to that stage, I saw that the user’s categorical inputs would make no sense to my model, so I had to go back and change certain categorical variables into ordinal variables, wasting some valuable time in the process.\nAnswering these questions at the beginning will save a whole lot of time later on, and it will avoid you having to go back and forth between the different steps unnecessarily. I use the word ‘unnecessarily’ here for a reason, which brings me to Mistake #4.\nPro Tip: Have a rough idea of the end-result when you start, in whatever form it may be. Sketch it out somewhere if you have to.\nMistake #4: Not circling back to previous stages.   The Data Science Life Cycle (Credits: Chanin Nantasenamat/Data Professor)   Typically, any project can be categorized into these 5 major stages: Data Collection, Data Cleaning, Exploratory Data Analysis, Model Building and Model Deployment.\nBut a major mistake I made was thinking that the process was a linear series of steps, when in fact, it’s an iterative and often, cyclical process.\nAs an example, during my Data Science Salary Predictor project, when I was at the Model Building stage, I thought of a couple more interesting features that I could have added, to give as inputs to my model.\nSo what did I do?\nI went all the way back to the Data Cleaning and Preprocessing step, had to do some additional feature engineering, and had to perform some Exploratory Data Analysis on the new data, and build another model with the additional features.\nSometimes, these circling backs might be completely unnecessary, such as in Mistake #3.\nBut sometimes, you might have a new insight after having worked with the data for a while and in that case, circling back is the right thing to do.\nPro Tip: Plan as much as necessary before hand, but circle back if needed. That little effort might make your project a lot better, for all you know.\nMistake #5: Not introspecting and evaluating after the project is deployed A step I neglected completely, and what might actually be the most important step of them all, is evaluating your project after you’re done.\nSeeing what went right, what went wrong, and how to improve next time.\nI learnt this about this very important step from Daniel Bourke’s video after the Airbnb project he had embarked upon.\nTake some time, analyze all the steps of your project, introspect, learn from them and move on.\nThis article wouldn’t exist without this step. The mistakes listed here are those that I learnt from my projects, and I’m almost sure that anyone embarking on a project of their own will commit certain mistakes along the way. Learn from them, make sure you don’t repeat them next time, move on.\nPro Tip: Go back and see what went right and wrong, and learn from them.\n “It is okay to mistakes, as long as you learn from them.” — Anonymous\n And most importantly: don’t forget to have fun along the way.\nFeel free to reach out to me on LinkedIn, check out my GitHub for the projects I’ve done, or my personal website for all my work.\n","date":1595944183,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595944183,"objectID":"82e5ddec8cfc95f1c51a0a92209cfe32","permalink":"https://vishnubharadwaj00.github.io/post/datascienceprojectmistakes/","publishdate":"2020-07-28T13:49:43.976Z","relpermalink":"/post/datascienceprojectmistakes/","section":"post","summary":"The mistakes I have made along the way, and how you can avoid them in your next project.","tags":["Data Science"],"title":"How NOT to Build a Data Science Project","type":"post"},{"authors":null,"categories":null,"content":"(Originally published on Noteworthy)\nI was struggling.\nI was halfway through the second year of my undergraduate CS degree. And I wanted to quit.\nThere were a lot of personal reasons I wanted to quit, but one of the main reasons was that I felt stuck and unable to attain my full potential. Growing up, I had always been told, “Hey, you’re a smart kid. You have a lot of potential.” I had always had a loosely structured work ethic; I worked twice as hard as others to get the same result as them. But this never really sat right with me.\nDuring my degree, I felt like a hamster in a wheel, running in place with no real destination. Always working hard, only to get mediocre results. I was getting really tired, really fast, and I had the “rational” thought of “Hey, why don’t I just quit?”\nLuckily for me, somewhere around the same time I was thinking of quitting, I went to a bookstore and came across a book. And immediately, the title of this book really struck a chord with me:\nSo Good They Can’t Ignore You.\nI thought to myself, “Hey, that’s what I want to be: really good at whatever it is I do.” So I picked it up and the book right next to it, “Deep Work”, also by Cal Newport, and it’s safe to say my life’s never been the same since. (Read till the end if you’d like to know how I’m doing now.)\nI have since purchased two of his other books “How to be a Straight-A Student” and “Digital Minimalism”, which also had an equally significant impact on me.\nHere are some of the things I’ve learnt from these books and how they might help you as well:\nWork accomplished = Time Spent x Intensity of Focus (From How to be a Straight-A Student) This was one of the foundations for the book “How to be a Straight-A Student”, outlining the unconventional strategies that college students used to study a lot less but still ace their college life. The key part of this formula is Intensity of Focus, which basically refers to how much concentration and focus you’re putting into a particular task.\nHaphazardly flitting across different tasks or multitasking are all ways to diminish focus, and is basically the equivalent of completing your assignment with both your hands tied behind your back. Instead, putting in many short intervals of high focus to complete a task is much easier and takes a lot less time in the long run.\nEver since reading about this, I have used this technique for every test or exam I’ve prepared for. I adopted the vastly popular Pomodoro technique, where I study for 25 minutes with high concentration levels and take a 5 minute break after that to recharge. 4–5 of these sessions and I’m done for the day, having gotten a lot more done than if I had just continuously studied for hours together.\nPro Tip: Use the Pomodoro Technique. Now.\nActive Recall is the only studying that matters. (From How to be a Straight-A Student) Active recall, or Quiz-and-Recall as Cal calls it, is the polar opposite of rote memorization or flipping through a textbook a couple of times. Active recall consists of understanding the concept at hand and then explaining the concept out loud, without any help or referral to your textbook or notes. It’s a variation of the Feynman technique, named after legendary physicist Richard Feynman.\nIt’s hard. It’s uncomfortable. The act of trying to force information out of your brain is painful. But over time, you get used to it. And guess what? If you can explain it well without any help, you’re done. That’s it. It is brutally time-effective.\nDuring the 25-minute Pomodoro sessions, I try and do as much active recall as possible. Although initially uncomfortable, I was able to save a ton of time with this method. In fact, thanks mainly to this method, I was typically done studying the day before an exam or test, and so I’ve never had to pull an all-nighter ever. I’d usually spend the evening and night before relaxing by watching TV or reading.\nPro Tip: Explain things out loud when studying. Nothing else matters.\n“Follow your passion” is terrible advice. (From So Good They Can’t Ignore You) It’s very likely that you’ve heard the phrase “Follow your passion”. But in the book “So Good They Can’t Ignore You”, Cal makes the argument that skills matter way more than passion in finding a career that is meaningful and satisfying. Putting in the time to build important, valuable skills should be the mainstream way to build a career that you end up loving.\nThis particular piece of writing really resonated with me. One of the reasons I wanted to quit my CS degree was because I didn’t feel “passionate” about the subjects I was learning.\nI was almost certain that there was a different career trajectory out there for me and this was not it. But after I read this, I realized that if you don’t have a pre-existing passion (I didn’t), then passion is simply a byproduct of building important, necessary skills.\nIt’s not the destination, it’s the journey.\nPro Tip: Don’t worry about passion. Passion doesn’t matter. Only skills do.\nThe Idea of Career Capital. (From So Good They Can’t Ignore You) According to Cal, there are typically 3 traits that make a career compelling and enriching: Creativity, Impact and Control. Being stimulated creatively, having a significant impact and having a degree of control over your own career are some of the things that people ideally want in a career.\nAnd to gain these things, Cal introduces the concept of Career Capital, or rare and valuable skills, that you can exchange for these traits. Like a commodity like oil or gold, these skills can be leveraged to build a sustainable career. Without the necessary skills, asking for these factors in a career is close to impossible.\nHow do you gain these skills? Deliberate practice. Deliberate practice consists of a well-defined stretch goal, a little beyond your current capabilities, and full concentration and effort practicing that skill repeatedly to attain that goal, while getting constant feedback. Active Recall, as discussed above, is one important form of deliberate practice.\nPro Tip: Gain rare and valuable skills using Deliberate Practice.\nEmbracing Boredom. (From Deep Work) Deep Work was arguably one of Cal’s more popular books, and created the largest ripple effect, making the discussion and criticism of current technological use more mainstream. According to Cal, Deep Work is the ability to perform activities in a state of distraction-free concentration that push your cognitive capabilities to their limit. Deep Work is becoming increasingly rare, and yet is increasingly valuable, a winning combination for those who can actually practice Deep Work.\nIn one part of the book, he talks about actually embracing boredom and making that a more significant part of our lives. According to him, Deep Work is not an overnight decision, but rather like a muscle that has to be trained. So if Deep Work is equivalent to working out at the gym, boredom would be adopting a healthy diet and staying away from junk food. They need to coexist to have any kind of effect.\nOnly by removing the constant distractions around us can we be preparing our brains for the type of hard thinking that Deep Work needs. As Cal says, “Take breaks from focus, not distraction.”\nPro Tip: Be bored, for a fixed period everyday. Let your mind wander, without any distractions. Be intrigued by the thoughts you come across.\nSchedule Every Minute of your Day. (From Deep Work) Planning is a crucial part of productivity. Without proper planning, it could seem like you’re ‘busy’, but without any major impact. We sometimes run on autopilot, not really worrying about what we do with our time during the day. But over a longer period of time, this turns out to have a really negative impact.\nTo sidestep this major problem, Cal suggests time-blocking, a planning method in which you plan the entire day using blocks or chunks (Time chunking doesn’t have the same ring to it though). It’s a simple method but it is highly effective.\nI adopted this and struggled with it initially, because I always underestimated the time for a given task, and had to switch up my entire schedule sometimes. On some days, interruptions I hadn’t planned for might pop up, which requires some planning around.\nBut again, over time, I started to get the hang of this technique, and it’s allowed me to really make the most of my days.\nPro Tip: Plan your day by Time Blocking.\nQuit Social Media. (From Deep Work/Digital Minimalism) Probably the most controversial statement of Cal’s has been ‘Quit Social Media’. He is a huge critic of social media and their highly engineered manner to grab as much of our attention as possible. Having never had a social media account, Cal had a very unique insight into how these apps and devices were wreaking havoc on our brains. He’s even given a TED Talk on this topic.\nThis piece of advice is the one I have prioritized over any other, simply because I was also getting bogged down by social media. Seeing everyone else’s happy lives made me question the happiness of my own, when the truth is that that was not the full picture that I was seeing.\nSo, I quit. I quit everything. The first week or two were really difficult. I kept picking up my phone, only to realize I’d uninstalled all the apps. Keeping in touch with friends became a little bit more difficult. I felt like I was missing out on something.\nBut after the first week or two, I felt free. I had so much more time on my hands now, to do the things that were really valuable to me. As a millennial not on social media, being the only one among my peers not in this hyper-connected state, I had a unique edge over my peers. More importantly, I started enjoying the little things in life. The things people usually missed while looking into their screens.\nNow, it’s been more than 2 years since I quit. I have only a select few technologies that I use because they’re valuable to me such as Whatsapp for my college groups and LinkedIn for professional purposes like sharing my projects. Now, I’m really starting to wonder how I ever was on social media. There have been times where I reinstalled some of the apps, only to get disillusioned after a few hours, and uninstalling them again. I’m happier without it.\nPro Tip: Quit social media, either for a fixed period of time, or indefinitely. Push through the initial pain, and see remarkable changes immediately.\nSo, how’s my life now? A million times better. I decided to push through and continue with CS, and graduated last month, with a pretty good GPA. I was also accepted into a highly ranked university for my graduate studies (hint: Go Boilermakers!). I’ve picked up quite and mastered quite a few important skills in Data Science, one of my main interests (and dare I say, passion?)\nI’m happier, way more productive, not on social media and enjoying the important things in life.\nThank you, Cal Newport, for saving my career before it even started.\nCheck out Cal Newport’s blog, Study Hacks, or any of his books, for some great content. Feel free to reach out to me on LinkedIn, or check out some of the Data Science projects I’ve done on my GitHub.\n","date":1594902917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594902917,"objectID":"422ae5cbd6c3556b0ab038e507c63fd8","permalink":"https://vishnubharadwaj00.github.io/post/calnewport/","publishdate":"2020-07-16T12:35:17.292Z","relpermalink":"/post/calnewport/","section":"post","summary":"And what you can learn from my journey.","tags":["Productivity"],"title":"How Cal Newport Saved my CS Career","type":"post"},{"authors":null,"categories":null,"content":"","date":1592006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592006400,"objectID":"54b96e89d436c4c727cb233df158950b","permalink":"https://vishnubharadwaj00.github.io/project/datasciencesalarypred/","publishdate":"2020-06-13T00:00:00Z","relpermalink":"/project/datasciencesalarypred/","section":"project","summary":"End-to-End Data Science Salary Prediction in the US (During COVID-19)","tags":["Machine Learning"],"title":"Data Science Salary Predictor","type":"project"},{"authors":null,"categories":null,"content":"(Originally published on Towards Data Science)\nI miss the NBA. The suspension of the NBA due to the COVID-19 pandemic is just one of the small disruptions in the world today. But like many other sports enthusiasts, I miss the thrill of watching live sports.\nSo I dealt with it, the same way any Data-loving, basketball enthusiast would: I dove into the stats, and built some graphs.\nI used Tableau (something I only recently learnt how to use, but absolutely love already) to build these plots, and a dataset I found on Kaggle, which contains information about all players who’ve played in the NBA from the 1996–97 season to 2019–20 season.\nBefore I even started, without creating a single graph, I immediately stumbled upon my first interesting insight:\n There have been only 2235 unique players in the NBA since 1996.\n That’s an insanely low number, for 24 years of basketball. And that’s why we keep hearing experts say that getting into the NBA is like “winning the lottery”.\nBefore the NBA College basketball is the typical recruitment point for the NBA, with most players turning pro after a successful college basketball career. Tournaments like NCAA March Madness are extremely popular, and players who shine in these tournaments catch the eye of NBA recruiters.\n1. What are the most successful colleges, in terms of getting players to the NBA?   Number of Players from each College who’ve played in the NBA   These are the top 20 colleges in the US in terms of players who have gone on to play in the NBA.\n The top 3 colleges are The University of Kentucky, Duke University and the UCLA are the most successful colleges with 168 players going on to play in the NBA.\n Players are typically drafted from colleges in different draft rounds. Being a first-round draft pick, means you’re the best of the best.\n2. But does the draft round affect how many games the player ends up playing per season?   Draft Round vs Average Games Played per Season   Not necessarily.\n1st round picks only play the 3rd most number of games, while 7th and 3rd round picks play more games on average per season.\nBut in terms of average points per season (the colors in the plot), the 1st round picks contribute the most points. Could this be because 1st round picks are rested a bit more to score more efficiently in crucial and important games for the team?\nAnother interesting insight, being undrafted helps you play more games than being a 4th, 6th or 8th round pick. Surprising.\nThe Biggest of the Biggest So the player has been drafted. Chances are he’s really tall and really strong. The NBA’s known for having the tallest and strongest players, real athletic specimens.\nIn fact, the average height and weight of the players since 1996 are 200.8 cm (6'5\u0026rsquo;’) and 100.6 kg (221.7 lbs).\n3. Who were the biggest players out of all of them?   NBA’s biggest players, in terms of Height (cm) and Weight (kg)   As most of the players in the NBA do come from the USA, it’s definitely no surprise that some of the biggest players are from the USA as well.\nBut it looks like one of the biggest ever was Sim Bhullar from Canada (who was actually the first Indian-origin player in the NBA), who was 2.26m (7’5’’) and 163 kg (359 lbs).\n Shaquille O’Neal, known as one of the best “big guys” in the NBA, was 2.15m (7’1’’) and 145 kg (319 lbs). Yao Ming (the unlabeled orange icon in the graph) was also one of the best “big guys”, and was 2.29m (7'6\u0026rsquo;’) and 141 kg (310 lbs)\nThe Most Important Team Statistic? Teams can be measured in many different ways, each bringing out different insights. But arguably, one of the most important is “Net Rating”. Net rating refers to the team’s point differential per 100 possessions.\n4. Which team has the best and worst Net Rating?   Average Net Rating for all teams since 1996   Turns out, the San Antonio Spurs have the best Net Rating on average, since 1996, with +3.24, meaning on average, for every 100 possessions, they’re ahead by 3 points. Only 2 other teams have positive Net Ratings, the Miami Heat and Oklahoma City Thunder. The worst team is the now defunct Vancouver Grizzlies, with -8.39.\nAnother surprising insight is that the San Antonio Spurs were also the oldest team on average, over all the seasons. Does experience, in the form of having an older team, actually work? That’s probably the basis for another analysis.\nThe Best Offensive Players (Disclaimer: These statistics are from 1996 onwards, so with only about half of Michael Jordan’s career recorded here, he’s not on these lists, but with a bigger dataset, you can be sure he’d be right up there.)\nThere are so many metrics that individual players can be measured by. For an offensive player, the player in charge of trying to create as many baskets as possible, the most used and analyzed metrics are Points and Assists.\n5. Which players have the most Points and Assists?   Total Points vs Total Assists (Color indicates Total Games Played)   LeBron James, in a complete league of his own, a complete outlier, well above the rest of the pack with a whopping 34,027 points and 9,280 assists. Kobe Bryant (RIP Mamba) is the only one who can come close to LBJ with 33,633 points and 6,319 assists. (I Googled Michael Jordan’s stats for completeness and he would come somewhere around third, with 32,292 points and 5,633 assists)\nOther notable players on the list are Dirk Nowitzki, with 31,561 points, and 3,667 assists, and Allen Iverson, who’s played the least number of games out of everyone on this graph, with 24,380 points and 5,622 assists in just 914 games. (In comparison, LeBron’s played 1,256 games, Kobe’s played 1,346 games and Dirk’s played 1,522 games).\n— — — — — — — — — — — — — — — — — — — — — — — — — —\nUsing points and assists in a different way can bring out more insights. Using total shooting efficiency of a player and the percentage of assists by the player while on the floor, a completely different set of players show up.\n6. Which players had the highest Shooting Efficiency and Assist Percentage?   Scoring Efficiency % vs % of Assists Contributed (Color indicates Average games played per Season)   John Stockton seems almost like an outlier, with a high shooting efficiency and high average assist percentage. But the repetition of certain players in this graph, such as Steph Curry, James Harden, LeBron James and Chris Paul, showcases their superior shooting ability. Also younger, current players like Ben Simmons and Nikola Jokic, are definitely on the rise and will soon dominate these statistics with consistent perfomances.\n— — — — — — — — — — — — — — — — — — — — — — — — — —\nPlay usage percentage is an estimate of the percentage of team plays used by a player while he was on the floor. Typically, when the play usage percentage increases, the scoring efficiency decreases for a player. The real superstar players are those who can keep both these numbers up, turning into very efficient shooters.\n7. Who were the most effective shooters?   Scoring Efficiency % vs Play Usage % (Color indicates Average games played per Season)   Joel Embiid has the highest average play usage percentage, while Boban Marjanovic has the highest average shooting percentage.\nBut the most effective superstar shooters seem to be Kevin Durant, James Harden and LeBron James, due to their all-round ability in using plays and having a high shooting efficiency.\nClosely following them are Anthony Davis, Shaquille O’Neal and Steph Curry.\nThe Best Defensive Players Although typically overlooked, defensive players are those who do most of the grunt work and actively contribute to the success of a team. Without a great defensive player, no matter how many great offensive players you might have, you’ll end up on the worse side of things.\nRebounds are another important metric for measuring the efficiency of these defensive, lockdown players. A Rebound refers to a player retrieving the ball after a missed field goal or free throw.\nThere are 2 types of rebounds, offensive and defensive. A superstar defensive player ideally has a high number of both types of rebounds.\n8. Who were the best players defensively, in terms of rebounds?   Average Offensive Rebound % vs Average Defensive Rebound % (Color indicates Average Games played per Season)   With at least 10 games played per season on average, and a minimum of 2 seasons, these were the top 10 defensive players in terms of rebounds.\nThe best defensive rebounder was Chris Boucher, and the best offensive rebounders were J.R Giddens and Torraye Braggs.\nBut what matters most in a defender is their all-round ability, and the best players in terms of both offensive and defensive rebounds are Dennis Rodman and Andre Drummond.\nDouble Double Trouble Trouble Another metric used very often is the “Double Double”, when a player has more than 10 points in 2 of the following metrics: points, assists, rebounds, steals and blocks.\nMost Doubles Doubles are typically made of points, assists and rebounds, which I’ve referred to here as “Most Common Double Double”.\n9. Which players had the most seasons averaging the “Most Common” Double Double?   Number of Seasons averaging the “Most Common” Double Double   Dwight Howard (14 Double Double seasons) narrowly edges over Tim Duncan (13 Double Double seasons) to have the highest number of seasons averaging the “Most Common” Double Double.\nAnthony Davis, a younger and current player, also being on this list indicates that with a couple of great seasons, he has a very good chance of moving up this list.\nAnd it’s amazing how Chris Paul features in this graph as well, showcasing his brilliant shooting ability as well as his all-rounded abilities.\nThe Evolving Game? There have been many conversations on how the game has changed over time. Many people think the aggressiveness and intensity has decreased over time, while others believe that the game has evolved into a more graceful and watchable format.\nArguably, some of the most important metrics used so far were Points, Assists and Rebounds.\n10. How much have these important metrics (Points, Assists, Rebounds) actually changed over time?   Points, Rebounds and Assists Over Time (1996–97 to 2019–20)   Total assists per season and total rebounds per season seem constant for the most part, with a slightly increasing tendency.\nTotal points per season, on the other hand, has significantly increased. The 1996–97 season had a total of 3,540 points whereas the 2018–19 season had 4,565 points. Even the incomplete 2019–20 season already has 4,434 points, well on track to go past the 2018–19 season, becoming the highest scoring season of all time (whenever and however the season resumes). This increase in points could be attributed to the rise of the 3-Pointer revolution, with more and more 3-Pointers being scored now than ever before. It could also be due to the fact that there are a lot more high scoring matches now than before.\nConclusion I hope you were able to take away some kind of insight from these graphs, because I certainly did. It’s just another way of showing that the best way to learn is by doing.\nI’m open to any kind of suggestions or modifications to these visualizations, or ideas for new ones. Definitely do use them in whatever basketball conversations and arguments you have, as you see fit.\nAnd I hope it helps fill the void left by the lack of sports right now. (Come back soon, normal life)\nFeel free to reach out to me on LinkedIn, or check out my GitHub for other projects I’ve done.\n","date":1591434467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591434467,"objectID":"62605fdfbf2dc8042845ceac7f5f638c","permalink":"https://vishnubharadwaj00.github.io/post/nbaviz/","publishdate":"2020-06-06T09:07:47.714Z","relpermalink":"/post/nbaviz/","section":"post","summary":"A Data Science Approach to Cope with Missing Basketball","tags":["data science"],"title":"10 Unique Visualizations of the NBA","type":"post"},{"authors":["Vishnu Bharadwaj","Dr S.Vigneshwari"],"categories":null,"content":"","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://vishnubharadwaj00.github.io/publication/journal-article/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"A Statistical approach to analyze the disparities in literacy rates among the Indian population, using Indian Census data.","tags":null,"title":"Statistical Analysis of Literacy Rates using Indian Census Data","type":"publication"},{"authors":null,"categories":null,"content":" Stock Prediction is a versatile and extensive field on its own. With increasingly sophisticated computational capabilities, Stock Prediction is becoming a more and more important application of fields like Machine Learning, Deep Learning and AI.\n This project deals with web scraping Tesla stock prices from the Yahoo Finance page, using Beautiful Soup and Selenium, and using Recurrent Neural Networks (particularly LSTMs) to build a Deep Learning model to predict future stock prices.\nImporting the necessary libraries:\nimport os import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from matplotlib import style import matplotlib import pandas as pd from datetime import datetime  Since this is executed in Google Colab, the Google Drive containing the web-scraped data is contained here.\nThe code for the webscraping, executed with BeautifulSoup and Selenium, can be found here.\nfrom google.colab import drive drive.mount('/content/gdrive', force_remount=True) root_dir = \u0026quot;/content/gdrive/My Drive/\u0026quot; data_dir=os.path.join(root_dir,\u0026quot;tsla.csv\u0026quot;)  Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com\u0026amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob\u0026amp;response_type=code\u0026amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at /content/gdrive  Creating a function to plot Time Series Data:\ndef plot_series(time, series, format=\u0026quot;-\u0026quot;, start=0, end=None): plt.plot(time[start:end], series[start:end], format) plt.xlabel(\u0026quot;Time\u0026quot;) plt.ylabel(\u0026quot;Value\u0026quot;) plt.grid(True)  Reading in the data, using Pandas:\ndf=pd.read_csv(data_dir) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 Date Open High Low Close* Adj Close** Volume     0 0 Apr 29, 2020 790.17 803.20 783.16 800.51 800.51 15812100   1 1 Apr 28, 2020 795.64 805.00 756.69 769.12 769.12 15222000   2 2 Apr 27, 2020 737.61 799.49 735.00 798.75 798.75 20681400   3 3 Apr 24, 2020 710.81 730.73 698.18 725.15 725.15 13237600   4 4 Apr 23, 2020 727.60 734.00 703.13 705.63 705.63 13236700   ... ... ... ... ... ... ... ... ...   248 248 May 06, 2019 250.02 258.35 248.50 255.34 255.34 10833900   249 249 May 03, 2019 243.86 256.61 243.49 255.03 255.03 23706800   250 250 May 02, 2019 245.52 247.13 237.72 244.10 244.10 18159300   251 251 May 01, 2019 238.85 240.00 231.50 234.01 234.01 10704400   252 252 Apr 30, 2019 242.06 244.21 237.00 238.69 238.69 9464600    253 rows × 8 columns\n There\u0026rsquo;s a year\u0026rsquo;s worth of stock prices here, from April 30, 2019 to April 29, 2020, with a few days\u0026rsquo; data missing.\nBut first, there is some Data Cleaning that needs to be done:\nThe columns \u0026ldquo;Close\u0026rdquo; and \u0026ldquo;Adj Close\u0026rdquo; have additional * symbols which have to be removed:\ndf.drop(labels=\u0026quot;Unnamed: 0\u0026quot;, axis=1, inplace=True) df.rename(columns={\u0026quot;Close*\u0026quot;: \u0026quot;Close\u0026quot;, \u0026quot;Adj Close**\u0026quot;: \u0026quot;Adj Close\u0026quot;},inplace=True) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Date Open High Low Close Adj Close Volume     0 Apr 29, 2020 790.17 803.20 783.16 800.51 800.51 15812100   1 Apr 28, 2020 795.64 805.00 756.69 769.12 769.12 15222000   2 Apr 27, 2020 737.61 799.49 735.00 798.75 798.75 20681400   3 Apr 24, 2020 710.81 730.73 698.18 725.15 725.15 13237600   4 Apr 23, 2020 727.60 734.00 703.13 705.63 705.63 13236700   ... ... ... ... ... ... ... ...   248 May 06, 2019 250.02 258.35 248.50 255.34 255.34 10833900   249 May 03, 2019 243.86 256.61 243.49 255.03 255.03 23706800   250 May 02, 2019 245.52 247.13 237.72 244.10 244.10 18159300   251 May 01, 2019 238.85 240.00 231.50 234.01 234.01 10704400   252 Apr 30, 2019 242.06 244.21 237.00 238.69 238.69 9464600    253 rows × 7 columns\n Next, it would be much easier if the order of the data was reversed, since the data was originally arranged as latest to earliest.\nThis would make it easier to use the earlier data for training and the later data for validation/testing.\ndf=pd.DataFrame(df.values[::-1], df.index, df.columns) df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Date Open High Low Close Adj Close Volume     0 Apr 30, 2019 242.06 244.21 237 238.69 238.69 9464600   1 May 01, 2019 238.85 240 231.5 234.01 234.01 10704400   2 May 02, 2019 245.52 247.13 237.72 244.1 244.1 18159300   3 May 03, 2019 243.86 256.61 243.49 255.03 255.03 23706800   4 May 06, 2019 250.02 258.35 248.5 255.34 255.34 10833900   ... ... ... ... ... ... ... ...   248 Apr 23, 2020 727.6 734 703.13 705.63 705.63 13236700   249 Apr 24, 2020 710.81 730.73 698.18 725.15 725.15 13237600   250 Apr 27, 2020 737.61 799.49 735 798.75 798.75 20681400   251 Apr 28, 2020 795.64 805 756.69 769.12 769.12 15222000   252 Apr 29, 2020 790.17 803.2 783.16 800.51 800.51 15812100    253 rows × 7 columns\n Creating a seperate Data Frame to visualize the stock prices better:\nadj_close=df['Adj Close'] adj_close.index = df['Date'] adj_close.head()  Date Apr 30, 2019 238.69 May 01, 2019 234.01 May 02, 2019 244.1 May 03, 2019 255.03 May 06, 2019 255.34 Name: Adj Close, dtype: object  type(df['Date'][0])  str  The \u0026ldquo;Date\u0026rdquo; column in the original Data Frame is of type \u0026ldquo;String\u0026rdquo;, but it has to be of \u0026ldquo;Datetime\u0026rdquo; format, to make it easier to plot it in the x-axis.\ndates=df['Date'] dates1=[] for date in dates: dates1.append(datetime.strptime(date, '%b %d, %Y')) dates=pd.core.series.Series(dates1) type(dates[0])  pandas._libs.tslibs.timestamps.Timestamp  Plotting the Adjusted Close Stock Prices over the last year:\nplt.figure(figsize=(13,8)) plt.style.use('fivethirtyeight') plt.plot(dates1,adj_close) plt.title(\u0026quot;Tesla Adjusted Close Prices\u0026quot;,loc='left') plt.rcParams.update({'font.size': 14})     Plot of Adjusted Close Stock Prices   There\u0026rsquo;s a huge up-tick in the price sometime after January 2020, which was when Tesla announced strong fourth-quarter financials, which exceeded all expectations.\nThe dip in the price in March corresponds to the COVID-19 pandemic and the financial crisis that it brought with it.\nIt does seem to be on the rise now, so it will be interesting to see if the model can predict all these ups and downs.\nCreating Series and Time arrays, and converting them to the right type:\nseries = np.array(adj_close.values) time = np.array(dates)  series=pd.to_numeric(series,errors='coerce',downcast='float') series  array([238.69, 234.01, 244.1 , 255.03, 255.34, 247.06, 244.84, 241.98, 239.52, 227.01, 232.31, 231.95, 228.33, 211.03, 205.36, 205.08, 192.73, 195.49, 190.63, 188.7 , 189.86, 188.22, 185.16, 178.97, 193.6 , 196.59, 205.95, 204.5 , 212.88, 217.1 , 209.26, 213.91, 214.92, 225.03, 224.74, 226.43, 219.62, 221.86, 223.64, 219.76, 219.27, 222.84, 223.46, 227.17, 224.55, 234.9 , 233.1 , 230.34, 230.06, 238.92, 238.6 , 245.08, 253.5 , 252.38, 254.86, 253.54, 258.18, 255.68, 260.17, 264.88, 228.82, 228.04, 235.77, 242.26, 241.61, 233.85, 234.34, 228.32, 230.75, 233.42, 238.3 , 235.01, 229.01, 235. , 219.62, 215.64, 219.94, 226.83, 225.86, 220.83, 222.15, 211.4 , 215. , 214.08, 215.59, 221.71, 225.61, 225.01, 220.68, 229.58, 227.45, 231.79, 235.54, 247.1 , 245.87, 245.2 , 242.81, 244.79, 243.49, 246.6 , 240.62, 241.23, 223.21, 228.7 , 242.56, 242.13, 240.87, 244.69, 243.13, 233.03, 231.43, 237.72, 240.05, 244.53, 244.74, 247.89, 256.96, 257.89, 259.75, 261.97, 256.95, 253.5 , 255.58, 254.68, 299.68, 328.13, 327.71, 316.22, 315.01, 314.92, 313.31, 317.47, 317.22, 326.58, 335.54, 337.14, 345.09, 349.93, 346.11, 349.35, 352.17, 349.99, 359.52, 352.22, 354.83, 333.04, 336.34, 328.92, 331.29, 329.94, 334.87, 336.2 , 333.03, 330.37, 335.89, 339.53, 348.84, 352.7 , 359.68, 358.39, 381.5 , 378.99, 393.15, 404.04, 405.59, 419.22, 425.25, 430.94, 430.38, 414.7 , 418.33, 430.26, 443.01, 451.54, 469.06, 492.14, 481.34, 478.15, 524.86, 537.92, 518.5 , 513.49, 510.5 , 547.2 , 569.56, 572.2 , 564.82, 558.02, 566.9 , 580.99, 640.81, 650.57, 780. , 887.06, 734.7 , 748.96, 748.07, 771.28, 774.38, 767.29, 804. , 800.03, 858.4 , 917.42, 899.41, 901. , 833.79, 799.91, 778.8 , 679. , 667.99, 743.62, 745.51, 749.5 , 724.54, 703.48, 608. , 645.33, 634.23, 560.55, 546.62, 445.07, 430.2 , 361.22, 427.64, 427.53, 434.29, 505. , 539.25, 528.16, 514.36, 502.13, 524. , 481.56, 454.47, 480.01, 516.24, 545.45, 548.84, 573. , 650.95, 709.89, 729.83, 745.21, 753.89, 746.36, 686.72, 732.11, 705.63, 725.15, 798.75, 769.12, 800.51], dtype=float32)  series.dtype  dtype('float32')  There are about 250 tuples, so a 80-20 split is somewhere around 210 for training set and 40 for testing set. The window size (for creating a windowed_dataset) and batch size can also be specified here.\nsplit_time = 210 adj_train = series[:split_time] adj_valid = series[split_time:] dates_train=dates[:split_time] dates_valid=dates[split_time:] window_size = 16 batch_size = 32 shuffle_buffer_size = 50  A function for creating a windowed dataset can be specified here. This is particularly helpful because it helps to create specific sized data slices, to train on them, make predictions, and subsequently learn from those predictions as well.\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer): series = tf.expand_dims(series, axis=-1) ds = tf.data.Dataset.from_tensor_slices(series) ds = ds.window(window_size + 1, shift=1, drop_remainder=True) ds = ds.flat_map(lambda w: w.batch(window_size + 1)) ds = ds.shuffle(shuffle_buffer) ds = ds.map(lambda w: (w[:-1], w[1:])) return ds.batch(batch_size).prefetch(1)  A function for forecasting data, given the series and the model can be specified here:\ndef model_forecast(model, series, window_size): ds = tf.data.Dataset.from_tensor_slices(series) ds = ds.window(window_size, shift=1, drop_remainder=True) ds = ds.flat_map(lambda w: w.batch(window_size)) ds = ds.batch(32).prefetch(1) forecast = model.predict(ds) return forecast  Creating our windowed training set:\ntf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) window_size = 64 batch_size = 256 train_set = windowed_dataset(adj_train, window_size, batch_size, shuffle_buffer_size) print(train_set) print(adj_train.shape)  \u0026lt;PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float32, tf.float32)\u0026gt; (210,)  Specifying the layers of our Keras model.\nIt has 1 Convolutional Layer, which complements the windowing of the dataset. Simply through extensive trial and error, the configuration of 2 LSTMs (64 and 32 nodes), and 3 Dense Layers (24, 12 and 1 nodes) was selected here.\nmodel = tf.keras.models.Sequential([ tf.keras.layers.Conv1D(filters=60, kernel_size=5, strides=1, padding=\u0026quot;causal\u0026quot;, activation=\u0026quot;relu\u0026quot;, input_shape=[None, 1]), tf.keras.layers.LSTM(64, return_sequences=True), tf.keras.layers.LSTM(32, return_sequences=True), tf.keras.layers.Dense(24, activation=\u0026quot;relu\u0026quot;), tf.keras.layers.Dense(12, activation=\u0026quot;relu\u0026quot;), tf.keras.layers.Dense(1), tf.keras.layers.Lambda(lambda x: x * 400) ])  Before actually running our model in its entirity, it might help to specify a learning rate, so the model can be run for a specified number of epochs, and seeing how the model does, a optimized learning rate can be found. This hyperparameter tuning will prove to be very useful later.\nlr_schedule = tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-8 * 10**(epoch / 20)) optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\u0026quot;mae\u0026quot;]) history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])  Epoch 1/100 1/1 [==============================] - 0s 40ms/step - loss: 269.6010 - mae: 270.1010 - lr: 1.0000e-08 Epoch 2/100 1/1 [==============================] - 0s 2ms/step - loss: 269.5738 - mae: 270.0738 - lr: 1.1220e-08 Epoch 3/100 1/1 [==============================] - 0s 2ms/step - loss: 269.5186 - mae: 270.0186 - lr: 1.2589e-08 Epoch 4/100 1/1 [==============================] - 0s 2ms/step - loss: 269.4341 - mae: 269.9341 - lr: 1.4125e-08 Epoch 5/100 1/1 [==============================] - 0s 2ms/step - loss: 269.3180 - mae: 269.8180 - lr: 1.5849e-08 .... .... .... Epoch 96/100 1/1 [==============================] - 0s 2ms/step - loss: 72.6451 - mae: 73.1451 - lr: 5.6234e-04 Epoch 97/100 1/1 [==============================] - 0s 2ms/step - loss: 117.5780 - mae: 118.0779 - lr: 6.3096e-04 Epoch 98/100 1/1 [==============================] - 0s 2ms/step - loss: 81.4628 - mae: 81.9628 - lr: 7.0795e-04 Epoch 99/100 1/1 [==============================] - 0s 2ms/step - loss: 105.4503 - mae: 105.9502 - lr: 7.9433e-04 Epoch 100/100 1/1 [==============================] - 0s 2ms/step - loss: 81.7803 - mae: 82.2790 - lr: 8.9125e-04  Plotting the results of the limited run of the model:\nplt.title(\u0026quot;Loss vs Learning Rate\u0026quot;) plt.xlabel(\u0026quot;Loss\u0026quot;) plt.ylabel(\u0026quot;Learning Rate\u0026quot;) plt.semilogx(history.history[\u0026quot;lr\u0026quot;], history.history[\u0026quot;loss\u0026quot;]) #plt.axis([1e-8, 1e-3,135,250])  [\u0026lt;matplotlib.lines.Line2D at 0x7f3e82620048\u0026gt;]     Plot of Loss vs Learning Rate   There\u0026rsquo;s a huge dip in the learning rate somewhere between $10^-7$ and $10^-6$, which helps the gradient descent process in training, helping the model to learn quickly and more effectively, so that can be fixed as the learning rate, and the model is run completely this time:\ntf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) model = tf.keras.models.Sequential([ tf.keras.layers.Conv1D(filters=60, kernel_size=5, strides=1, padding=\u0026quot;causal\u0026quot;, activation=\u0026quot;relu\u0026quot;, input_shape=[None, 1]), tf.keras.layers.LSTM(256, return_sequences=True), tf.keras.layers.LSTM(128, return_sequences=True), tf.keras.layers.Dense(128, activation=\u0026quot;relu\u0026quot;), tf.keras.layers.Dense(64, activation=\u0026quot;relu\u0026quot;), tf.keras.layers.Dense(1), tf.keras.layers.Lambda(lambda x: x * 500) ]) optimizer = tf.keras.optimizers.SGD(lr=3e-7, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\u0026quot;mae\u0026quot;]) history = model.fit(train_set,epochs=350)  Epoch 1/350 1/1 [==============================] - 0s 7ms/step - loss: 330.9964 - mae: 331.4964 Epoch 2/350 1/1 [==============================] - 0s 901us/step - loss: 314.5646 - mae: 315.0646 Epoch 3/350 1/1 [==============================] - 0s 849us/step - loss: 295.7275 - mae: 296.2275 Epoch 4/350 1/1 [==============================] - 0s 986us/step - loss: 277.2317 - mae: 277.7317 Epoch 5/350 1/1 [==============================] - 0s 835us/step - loss: 244.2985 - mae: 244.7985 Epoch 6/350 1/1 [==============================] - 0s 860us/step - loss: 225.8815 - mae: 226.3815 Epoch 7/350 1/1 [==============================] - 0s 836us/step - loss: 208.5272 - mae: 209.0272 Epoch 8/350 1/1 [==============================] - 0s 2ms/step - loss: 184.0172 - mae: 184.5172 Epoch 9/350 1/1 [==============================] - 0s 895us/step - loss: 158.5788 - mae: 159.0788 Epoch 10/350 1/1 [==============================] - 0s 988us/step - loss: 140.6364 - mae: 141.1363 ... ... ... Epoch 101/350 1/1 [==============================] - 0s 927us/step - loss: 48.9045 - mae: 49.4025 Epoch 102/350 1/1 [==============================] - 0s 833us/step - loss: 47.7830 - mae: 48.2795 Epoch 103/350 1/1 [==============================] - 0s 844us/step - loss: 47.8659 - mae: 48.3605 Epoch 104/350 1/1 [==============================] - 0s 1ms/step - loss: 48.0967 - mae: 48.5908 Epoch 105/350 1/1 [==============================] - 0s 832us/step - loss: 46.6607 - mae: 47.1546 ... ... ... Epoch 200/350 1/1 [==============================] - 0s 2ms/step - loss: 25.1572 - mae: 25.6457 Epoch 201/350 1/1 [==============================] - 0s 842us/step - loss: 25.0279 - mae: 25.5159 Epoch 202/350 1/1 [==============================] - 0s 1ms/step - loss: 25.0110 - mae: 25.4993 Epoch 203/350 1/1 [==============================] - 0s 889us/step - loss: 24.9164 - mae: 25.4044 Epoch 204/350 1/1 [==============================] - 0s 933us/step - loss: 24.9017 - mae: 25.3895 Epoch 205/350 1/1 [==============================] - 0s 1ms/step - loss: 24.7882 - mae: 25.2767 ... ... ... Epoch 300/350 1/1 [==============================] - 0s 1ms/step - loss: 22.3528 - mae: 22.8462 Epoch 301/350 1/1 [==============================] - 0s 844us/step - loss: 20.8384 - mae: 21.3259 Epoch 302/350 1/1 [==============================] - 0s 779us/step - loss: 19.6541 - mae: 20.1386 Epoch 303/350 1/1 [==============================] - 0s 820us/step - loss: 19.2455 - mae: 19.7316 Epoch 304/350 1/1 [==============================] - 0s 1ms/step - loss: 19.0995 - mae: 19.5838 Epoch 305/350 1/1 [==============================] - 0s 922us/step - loss: 19.0408 - mae: 19.5255 ... ... ... Epoch 345/350 1/1 [==============================] - 0s 914us/step - loss: 19.7900 - mae: 20.2793 Epoch 346/350 1/1 [==============================] - 0s 836us/step - loss: 18.8462 - mae: 19.3341 Epoch 347/350 1/1 [==============================] - 0s 963us/step - loss: 18.4283 - mae: 18.9135 Epoch 348/350 1/1 [==============================] - 0s 852us/step - loss: 18.3297 - mae: 18.8162 Epoch 349/350 1/1 [==============================] - 0s 963us/step - loss: 18.4796 - mae: 18.9658 Epoch 350/350 1/1 [==============================] - 0s 1ms/step - loss: 19.0140 - mae: 19.5016  The MAE (Mean Absolute Error) seems pretty low after running it for 350 epochs, which is an excellent sign. It started at ~330 and ended around ~19. Now we can build a forecast using the model_forecast function specified earlier:\nrnn_forecast = model_forecast(model, series[..., np.newaxis], window_size) rnn_forecast = rnn_forecast[split_time - window_size:-1,-1, 0]  There are mainly 2 metrics used for evaluating the accuracy of time series data, Mean Absolute Error and Root Mean Squared Error.\nn = tf.keras.metrics.MeanAbsoluteError() n.update_state(adj_valid, rnn_forecast) print('Mean Absolute Error: ', n.result().numpy()) m = tf.keras.metrics.RootMeanSquaredError() m.update_state(adj_valid, rnn_forecast) print('Root Mean Squared Error: ', m.result().numpy())  Mean Absolute Error: 152.27446 Root Mean Squared Error: 171.10997  Considering the fact that there was only 250 data points, and a pretty simple RNN built, a MAE of 150 and RMSE of 171 is extremely good. With more data, and a bigger model, it\u0026rsquo;s very possible to reduce these numbers.\n Plotting the predicted data against what actually happened:\nplt.figure(figsize=(20, 6)) plt.style.use('fivethirtyeight') plt.title(\u0026quot;Predictions vs Reality\u0026quot;, loc=\u0026quot;left\u0026quot;) plt.plot(dates_valid, adj_valid, label=\u0026quot;Actual\u0026quot;) plt.plot(dates_valid, rnn_forecast, label= \u0026quot;Prediction\u0026quot;) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7f12360679e8\u0026gt;    Plot of Predicted vs Actual   The model seemed to have gotten it pretty spot-on. It dips when the actual value dipped and seems to be on the rise, as is the case towards the end.\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"120c3dbcaa5ec46629587582e2fa61d0","permalink":"https://vishnubharadwaj00.github.io/project/tesla-stock-predictor/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/project/tesla-stock-predictor/","section":"project","summary":"Using Beautiful Soup, Selenium and Tensorflow to predict Tesla's Stock Prices","tags":["Deep Learning"],"title":"Tesla Stock Prediction using Web Scraping and Recurrent Neural Networks","type":"project"},{"authors":null,"categories":null,"content":"Full code can be accessed at the Github repository\nHealthcare is an extremely important part of the technological revolution, with deep learning techniques being applied to more and more medical problems.\nThis dataset for the detection of pneumonia, using chest x-ray images (https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) is used for binary classification (whether the person is normal or has pneumonia).\nThis notebook was executed in a Google Colab environment, and used transfer learning from a ResNet50 architecture, and after just 10 epochs of training, I was able to achieve \u0026gt;80% accuracy. Some minor tweaks and additional architectural changes can definitely increase the accuracy to close to 90% and maybe, even beyond that.\nUsing this very simple code, we can use the Kaggle API to download the dataset into our environment.\nimport os os.environ['KAGGLE_USERNAME'] = \u0026quot;##########\u0026quot; # username from the json file os.environ['KAGGLE_KEY'] = \u0026quot;###################\u0026quot; # key from the json file !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia  Downloading chest-xray-pneumonia.zip to /content 100% 2.29G/2.29G [00:30\u0026lt;00:00, 24.3MB/s] 100% 2.29G/2.29G [00:30\u0026lt;00:00, 80.1MB/s]  Next, the file has to be unzipped (it is in zip format) and the directories are specified for the train, test and validation sets, as well as the normal and pneumonia directories for the train and validation sets.\nimport zipfile local_zip = '/content/chest-xray-pneumonia.zip' zip_ref = zipfile.ZipFile(local_zip, 'r') zip_ref.extractall('/content') zip_ref.close()  base_dir = '/content/chest_xray' train_dir = os.path.join(base_dir, 'train') validation_dir = os.path.join(base_dir, 'val') test_dir = os.path.join(base_dir, 'test')  train_normal_dir = os.path.join(train_dir,'NORMAL') train_pneu_dir = os.path.join(train_dir,'PNEUMONIA') validation_normal_dir = os.path.join(validation_dir,'NORMAL') validation_pneu_dir = os.path.join(validation_dir,'PNEUMONIA')  print('total training normal images:', len(os.listdir(train_normal_dir))) print('total training pneu images:', len(os.listdir(train_pneu_dir)))  total training normal images: 1341 total training pneu images: 3875  There are 1341 normal images and 3875 pneumonia images in the training set, which seems to be more than sufficient.\nimport tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import Model  We specify a constant image size to make things uniform for the model to input. We then input the ResNet50 architecture, and ensure it cannot be trained, and that we retain all the weights.\nimg_size=[224,224] model=tf.keras.applications.resnet50.ResNet50(input_shape=img_size + [3], weights='imagenet', include_top=False) model.trainable = False  Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 94773248/94765736 [==============================] - 1s 0us/step  The last layer of the architecture is the \u0026lsquo;conv5_block3_out\u0026rsquo;, with a shape of (7,7,2048) which we will use later.\nlast_layer = model.get_layer('conv5_block3_out') print('last layer output shape: ', last_layer.output_shape) last_output = last_layer.output  last layer output shape: (None, 7, 7, 2048)  We then specify 2 last layers, a dense layer with 1024 nodes, with a 20% dropout rate to prevent overfitting, as well as a final dense layer with a single node and a sigmoid activation (which outputs 0 or 1 to classify)\nx = layers.Flatten()(last_output) x = layers.Dense(1024, activation='relu')(x) x = layers.Dropout(0.2)(x) x = layers.Dense (1, activation='sigmoid')(x)  amodel=Model(model.input,x)  The layers of our new model (amodel) and their shapes can be seen using the summary() function.\namodel.summary()  Model: \u0026quot;model_2\u0026quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_2 (InputLayer) [(None, 224, 224, 3) 0 __________________________________________________________________________________________________ conv1_pad (ZeroPadding2D) (None, 230, 230, 3) 0 input_2[0][0] __________________________________________________________________________________________________ conv1_conv (Conv2D) (None, 112, 112, 64) 9472 conv1_pad[0][0] __________________________________________________________________________________________________ conv1_bn (BatchNormalization) (None, 112, 112, 64) 256 conv1_conv[0][0] __________________________________________________________________________________________________ conv1_relu (Activation) (None, 112, 112, 64) 0 conv1_bn[0][0] __________________________________________________________________________________________________ pool1_pad (ZeroPadding2D) (None, 114, 114, 64) 0 conv1_relu[0][0] __________________________________________________________________________________________________ pool1_pool (MaxPooling2D) (None, 56, 56, 64) 0 pool1_pad[0][0] __________________________________________________________________________________________________ conv2_block1_1_conv (Conv2D) (None, 56, 56, 64) 4160 pool1_pool[0][0] __________________________________________________________________________________________________ conv2_block1_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block1_1_conv[0][0] __________________________________________________________________________________________________ conv2_block1_1_relu (Activation (None, 56, 56, 64) 0 conv2_block1_1_bn[0][0] __________________________________________________________________________________________________ conv2_block1_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block1_1_relu[0][0] __________________________________________________________________________________________________ conv2_block1_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block1_2_conv[0][0] __________________________________________________________________________________________________ conv2_block1_2_relu (Activation (None, 56, 56, 64) 0 conv2_block1_2_bn[0][0] __________________________________________________________________________________________________ conv2_block1_0_conv (Conv2D) (None, 56, 56, 256) 16640 pool1_pool[0][0] __________________________________________________________________________________________________ conv2_block1_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block1_2_relu[0][0] __________________________________________________________________________________________________ conv2_block1_0_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block1_0_conv[0][0] __________________________________________________________________________________________________ conv2_block1_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block1_3_conv[0][0] __________________________________________________________________________________________________ conv2_block1_add (Add) (None, 56, 56, 256) 0 conv2_block1_0_bn[0][0] conv2_block1_3_bn[0][0] __________________________________________________________________________________________________ conv2_block1_out (Activation) (None, 56, 56, 256) 0 conv2_block1_add[0][0] __________________________________________________________________________________________________ conv2_block2_1_conv (Conv2D) (None, 56, 56, 64) 16448 conv2_block1_out[0][0] __________________________________________________________________________________________________ conv2_block2_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block2_1_conv[0][0] __________________________________________________________________________________________________ conv2_block2_1_relu (Activation (None, 56, 56, 64) 0 conv2_block2_1_bn[0][0] __________________________________________________________________________________________________ conv2_block2_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block2_1_relu[0][0] __________________________________________________________________________________________________ conv2_block2_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block2_2_conv[0][0] __________________________________________________________________________________________________ conv2_block2_2_relu (Activation (None, 56, 56, 64) 0 conv2_block2_2_bn[0][0] __________________________________________________________________________________________________ conv2_block2_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block2_2_relu[0][0] __________________________________________________________________________________________________ conv2_block2_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block2_3_conv[0][0] __________________________________________________________________________________________________ conv2_block2_add (Add) (None, 56, 56, 256) 0 conv2_block1_out[0][0] conv2_block2_3_bn[0][0] __________________________________________________________________________________________________ conv2_block2_out (Activation) (None, 56, 56, 256) 0 conv2_block2_add[0][0] __________________________________________________________________________________________________ conv2_block3_1_conv (Conv2D) (None, 56, 56, 64) 16448 conv2_block2_out[0][0] __________________________________________________________________________________________________ conv2_block3_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block3_1_conv[0][0] __________________________________________________________________________________________________ conv2_block3_1_relu (Activation (None, 56, 56, 64) 0 conv2_block3_1_bn[0][0] __________________________________________________________________________________________________ conv2_block3_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block3_1_relu[0][0] __________________________________________________________________________________________________ conv2_block3_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block3_2_conv[0][0] __________________________________________________________________________________________________ conv2_block3_2_relu (Activation (None, 56, 56, 64) 0 conv2_block3_2_bn[0][0] __________________________________________________________________________________________________ conv2_block3_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block3_2_relu[0][0] __________________________________________________________________________________________________ conv2_block3_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block3_3_conv[0][0] __________________________________________________________________________________________________ conv2_block3_add (Add) (None, 56, 56, 256) 0 conv2_block2_out[0][0] conv2_block3_3_bn[0][0] __________________________________________________________________________________________________ conv2_block3_out (Activation) (None, 56, 56, 256) 0 conv2_block3_add[0][0] __________________________________________________________________________________________________ conv3_block1_1_conv (Conv2D) (None, 28, 28, 128) 32896 conv2_block3_out[0][0] __________________________________________________________________________________________________ conv3_block1_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block1_1_conv[0][0] __________________________________________________________________________________________________ conv3_block1_1_relu (Activation (None, 28, 28, 128) 0 conv3_block1_1_bn[0][0] __________________________________________________________________________________________________ conv3_block1_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block1_1_relu[0][0] __________________________________________________________________________________________________ conv3_block1_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block1_2_conv[0][0] __________________________________________________________________________________________________ conv3_block1_2_relu (Activation (None, 28, 28, 128) 0 conv3_block1_2_bn[0][0] __________________________________________________________________________________________________ conv3_block1_0_conv (Conv2D) (None, 28, 28, 512) 131584 conv2_block3_out[0][0] __________________________________________________________________________________________________ conv3_block1_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block1_2_relu[0][0] __________________________________________________________________________________________________ conv3_block1_0_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block1_0_conv[0][0] __________________________________________________________________________________________________ conv3_block1_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block1_3_conv[0][0] __________________________________________________________________________________________________ conv3_block1_add (Add) (None, 28, 28, 512) 0 conv3_block1_0_bn[0][0] conv3_block1_3_bn[0][0] __________________________________________________________________________________________________ conv3_block1_out (Activation) (None, 28, 28, 512) 0 conv3_block1_add[0][0] __________________________________________________________________________________________________ conv3_block2_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block1_out[0][0] __________________________________________________________________________________________________ conv3_block2_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block2_1_conv[0][0] __________________________________________________________________________________________________ conv3_block2_1_relu (Activation (None, 28, 28, 128) 0 conv3_block2_1_bn[0][0] __________________________________________________________________________________________________ conv3_block2_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block2_1_relu[0][0] __________________________________________________________________________________________________ conv3_block2_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block2_2_conv[0][0] __________________________________________________________________________________________________ conv3_block2_2_relu (Activation (None, 28, 28, 128) 0 conv3_block2_2_bn[0][0] __________________________________________________________________________________________________ conv3_block2_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block2_2_relu[0][0] __________________________________________________________________________________________________ conv3_block2_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block2_3_conv[0][0] __________________________________________________________________________________________________ conv3_block2_add (Add) (None, 28, 28, 512) 0 conv3_block1_out[0][0] conv3_block2_3_bn[0][0] __________________________________________________________________________________________________ conv3_block2_out (Activation) (None, 28, 28, 512) 0 conv3_block2_add[0][0] __________________________________________________________________________________________________ conv3_block3_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block2_out[0][0] __________________________________________________________________________________________________ conv3_block3_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block3_1_conv[0][0] __________________________________________________________________________________________________ conv3_block3_1_relu (Activation (None, 28, 28, 128) 0 conv3_block3_1_bn[0][0] __________________________________________________________________________________________________ conv3_block3_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block3_1_relu[0][0] __________________________________________________________________________________________________ conv3_block3_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block3_2_conv[0][0] __________________________________________________________________________________________________ conv3_block3_2_relu (Activation (None, 28, 28, 128) 0 conv3_block3_2_bn[0][0] __________________________________________________________________________________________________ conv3_block3_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block3_2_relu[0][0] __________________________________________________________________________________________________ conv3_block3_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block3_3_conv[0][0] __________________________________________________________________________________________________ conv3_block3_add (Add) (None, 28, 28, 512) 0 conv3_block2_out[0][0] conv3_block3_3_bn[0][0] __________________________________________________________________________________________________ conv3_block3_out (Activation) (None, 28, 28, 512) 0 conv3_block3_add[0][0] __________________________________________________________________________________________________ conv3_block4_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block3_out[0][0] __________________________________________________________________________________________________ conv3_block4_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block4_1_conv[0][0] __________________________________________________________________________________________________ conv3_block4_1_relu (Activation (None, 28, 28, 128) 0 conv3_block4_1_bn[0][0] __________________________________________________________________________________________________ conv3_block4_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block4_1_relu[0][0] __________________________________________________________________________________________________ conv3_block4_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block4_2_conv[0][0] __________________________________________________________________________________________________ conv3_block4_2_relu (Activation (None, 28, 28, 128) 0 conv3_block4_2_bn[0][0] __________________________________________________________________________________________________ conv3_block4_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block4_2_relu[0][0] __________________________________________________________________________________________________ conv3_block4_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block4_3_conv[0][0] __________________________________________________________________________________________________ conv3_block4_add (Add) (None, 28, 28, 512) 0 conv3_block3_out[0][0] conv3_block4_3_bn[0][0] __________________________________________________________________________________________________ conv3_block4_out (Activation) (None, 28, 28, 512) 0 conv3_block4_add[0][0] __________________________________________________________________________________________________ conv4_block1_1_conv (Conv2D) (None, 14, 14, 256) 131328 conv3_block4_out[0][0] __________________________________________________________________________________________________ conv4_block1_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block1_1_conv[0][0] __________________________________________________________________________________________________ conv4_block1_1_relu (Activation (None, 14, 14, 256) 0 conv4_block1_1_bn[0][0] __________________________________________________________________________________________________ conv4_block1_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block1_1_relu[0][0] __________________________________________________________________________________________________ conv4_block1_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block1_2_conv[0][0] __________________________________________________________________________________________________ conv4_block1_2_relu (Activation (None, 14, 14, 256) 0 conv4_block1_2_bn[0][0] __________________________________________________________________________________________________ conv4_block1_0_conv (Conv2D) (None, 14, 14, 1024) 525312 conv3_block4_out[0][0] __________________________________________________________________________________________________ conv4_block1_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block1_2_relu[0][0] __________________________________________________________________________________________________ conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block1_0_conv[0][0] __________________________________________________________________________________________________ conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block1_3_conv[0][0] __________________________________________________________________________________________________ conv4_block1_add (Add) (None, 14, 14, 1024) 0 conv4_block1_0_bn[0][0] conv4_block1_3_bn[0][0] __________________________________________________________________________________________________ conv4_block1_out (Activation) (None, 14, 14, 1024) 0 conv4_block1_add[0][0] __________________________________________________________________________________________________ conv4_block2_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block1_out[0][0] __________________________________________________________________________________________________ conv4_block2_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block2_1_conv[0][0] __________________________________________________________________________________________________ conv4_block2_1_relu (Activation (None, 14, 14, 256) 0 conv4_block2_1_bn[0][0] __________________________________________________________________________________________________ conv4_block2_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block2_1_relu[0][0] __________________________________________________________________________________________________ conv4_block2_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block2_2_conv[0][0] __________________________________________________________________________________________________ conv4_block2_2_relu (Activation (None, 14, 14, 256) 0 conv4_block2_2_bn[0][0] __________________________________________________________________________________________________ conv4_block2_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block2_2_relu[0][0] __________________________________________________________________________________________________ conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block2_3_conv[0][0] __________________________________________________________________________________________________ conv4_block2_add (Add) (None, 14, 14, 1024) 0 conv4_block1_out[0][0] conv4_block2_3_bn[0][0] __________________________________________________________________________________________________ conv4_block2_out (Activation) (None, 14, 14, 1024) 0 conv4_block2_add[0][0] __________________________________________________________________________________________________ conv4_block3_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block2_out[0][0] __________________________________________________________________________________________________ conv4_block3_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block3_1_conv[0][0] __________________________________________________________________________________________________ conv4_block3_1_relu (Activation (None, 14, 14, 256) 0 conv4_block3_1_bn[0][0] __________________________________________________________________________________________________ conv4_block3_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block3_1_relu[0][0] __________________________________________________________________________________________________ conv4_block3_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block3_2_conv[0][0] __________________________________________________________________________________________________ conv4_block3_2_relu (Activation (None, 14, 14, 256) 0 conv4_block3_2_bn[0][0] __________________________________________________________________________________________________ conv4_block3_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block3_2_relu[0][0] __________________________________________________________________________________________________ conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block3_3_conv[0][0] __________________________________________________________________________________________________ conv4_block3_add (Add) (None, 14, 14, 1024) 0 conv4_block2_out[0][0] conv4_block3_3_bn[0][0] __________________________________________________________________________________________________ conv4_block3_out (Activation) (None, 14, 14, 1024) 0 conv4_block3_add[0][0] __________________________________________________________________________________________________ conv4_block4_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block3_out[0][0] __________________________________________________________________________________________________ conv4_block4_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block4_1_conv[0][0] __________________________________________________________________________________________________ conv4_block4_1_relu (Activation (None, 14, 14, 256) 0 conv4_block4_1_bn[0][0] __________________________________________________________________________________________________ conv4_block4_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block4_1_relu[0][0] __________________________________________________________________________________________________ conv4_block4_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block4_2_conv[0][0] __________________________________________________________________________________________________ conv4_block4_2_relu (Activation (None, 14, 14, 256) 0 conv4_block4_2_bn[0][0] __________________________________________________________________________________________________ conv4_block4_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block4_2_relu[0][0] __________________________________________________________________________________________________ conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block4_3_conv[0][0] __________________________________________________________________________________________________ conv4_block4_add (Add) (None, 14, 14, 1024) 0 conv4_block3_out[0][0] conv4_block4_3_bn[0][0] __________________________________________________________________________________________________ conv4_block4_out (Activation) (None, 14, 14, 1024) 0 conv4_block4_add[0][0] __________________________________________________________________________________________________ conv4_block5_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block4_out[0][0] __________________________________________________________________________________________________ conv4_block5_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block5_1_conv[0][0] __________________________________________________________________________________________________ conv4_block5_1_relu (Activation (None, 14, 14, 256) 0 conv4_block5_1_bn[0][0] __________________________________________________________________________________________________ conv4_block5_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block5_1_relu[0][0] __________________________________________________________________________________________________ conv4_block5_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block5_2_conv[0][0] __________________________________________________________________________________________________ conv4_block5_2_relu (Activation (None, 14, 14, 256) 0 conv4_block5_2_bn[0][0] __________________________________________________________________________________________________ conv4_block5_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block5_2_relu[0][0] __________________________________________________________________________________________________ conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block5_3_conv[0][0] __________________________________________________________________________________________________ conv4_block5_add (Add) (None, 14, 14, 1024) 0 conv4_block4_out[0][0] conv4_block5_3_bn[0][0] __________________________________________________________________________________________________ conv4_block5_out (Activation) (None, 14, 14, 1024) 0 conv4_block5_add[0][0] __________________________________________________________________________________________________ conv4_block6_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block5_out[0][0] __________________________________________________________________________________________________ conv4_block6_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block6_1_conv[0][0] __________________________________________________________________________________________________ conv4_block6_1_relu (Activation (None, 14, 14, 256) 0 conv4_block6_1_bn[0][0] __________________________________________________________________________________________________ conv4_block6_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block6_1_relu[0][0] __________________________________________________________________________________________________ conv4_block6_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block6_2_conv[0][0] __________________________________________________________________________________________________ conv4_block6_2_relu (Activation (None, 14, 14, 256) 0 conv4_block6_2_bn[0][0] __________________________________________________________________________________________________ conv4_block6_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block6_2_relu[0][0] __________________________________________________________________________________________________ conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block6_3_conv[0][0] __________________________________________________________________________________________________ conv4_block6_add (Add) (None, 14, 14, 1024) 0 conv4_block5_out[0][0] conv4_block6_3_bn[0][0] __________________________________________________________________________________________________ conv4_block6_out (Activation) (None, 14, 14, 1024) 0 conv4_block6_add[0][0] __________________________________________________________________________________________________ conv5_block1_1_conv (Conv2D) (None, 7, 7, 512) 524800 conv4_block6_out[0][0] __________________________________________________________________________________________________ conv5_block1_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block1_1_conv[0][0] __________________________________________________________________________________________________ conv5_block1_1_relu (Activation (None, 7, 7, 512) 0 conv5_block1_1_bn[0][0] __________________________________________________________________________________________________ conv5_block1_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block1_1_relu[0][0] __________________________________________________________________________________________________ conv5_block1_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block1_2_conv[0][0] __________________________________________________________________________________________________ conv5_block1_2_relu (Activation (None, 7, 7, 512) 0 conv5_block1_2_bn[0][0] __________________________________________________________________________________________________ conv5_block1_0_conv (Conv2D) (None, 7, 7, 2048) 2099200 conv4_block6_out[0][0] __________________________________________________________________________________________________ conv5_block1_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block1_2_relu[0][0] __________________________________________________________________________________________________ conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block1_0_conv[0][0] __________________________________________________________________________________________________ conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block1_3_conv[0][0] __________________________________________________________________________________________________ conv5_block1_add (Add) (None, 7, 7, 2048) 0 conv5_block1_0_bn[0][0] conv5_block1_3_bn[0][0] __________________________________________________________________________________________________ conv5_block1_out (Activation) (None, 7, 7, 2048) 0 conv5_block1_add[0][0] __________________________________________________________________________________________________ conv5_block2_1_conv (Conv2D) (None, 7, 7, 512) 1049088 conv5_block1_out[0][0] __________________________________________________________________________________________________ conv5_block2_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block2_1_conv[0][0] __________________________________________________________________________________________________ conv5_block2_1_relu (Activation (None, 7, 7, 512) 0 conv5_block2_1_bn[0][0] __________________________________________________________________________________________________ conv5_block2_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block2_1_relu[0][0] __________________________________________________________________________________________________ conv5_block2_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block2_2_conv[0][0] __________________________________________________________________________________________________ conv5_block2_2_relu (Activation (None, 7, 7, 512) 0 conv5_block2_2_bn[0][0] __________________________________________________________________________________________________ conv5_block2_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block2_2_relu[0][0] __________________________________________________________________________________________________ conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block2_3_conv[0][0] __________________________________________________________________________________________________ conv5_block2_add (Add) (None, 7, 7, 2048) 0 conv5_block1_out[0][0] conv5_block2_3_bn[0][0] __________________________________________________________________________________________________ conv5_block2_out (Activation) (None, 7, 7, 2048) 0 conv5_block2_add[0][0] __________________________________________________________________________________________________ conv5_block3_1_conv (Conv2D) (None, 7, 7, 512) 1049088 conv5_block2_out[0][0] __________________________________________________________________________________________________ conv5_block3_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block3_1_conv[0][0] __________________________________________________________________________________________________ conv5_block3_1_relu (Activation (None, 7, 7, 512) 0 conv5_block3_1_bn[0][0] __________________________________________________________________________________________________ conv5_block3_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block3_1_relu[0][0] __________________________________________________________________________________________________ conv5_block3_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block3_2_conv[0][0] __________________________________________________________________________________________________ conv5_block3_2_relu (Activation (None, 7, 7, 512) 0 conv5_block3_2_bn[0][0] __________________________________________________________________________________________________ conv5_block3_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block3_2_relu[0][0] __________________________________________________________________________________________________ conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block3_3_conv[0][0] __________________________________________________________________________________________________ conv5_block3_add (Add) (None, 7, 7, 2048) 0 conv5_block2_out[0][0] conv5_block3_3_bn[0][0] __________________________________________________________________________________________________ conv5_block3_out (Activation) (None, 7, 7, 2048) 0 conv5_block3_add[0][0] __________________________________________________________________________________________________ flatten_1 (Flatten) (None, 100352) 0 conv5_block3_out[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 1024) 102761472 flatten_1[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 1024) 0 dense_2[0][0] __________________________________________________________________________________________________ dense_3 (Dense) (None, 1) 1025 dropout_1[0][0] ================================================================================================== Total params: 126,350,209 Trainable params: 102,762,497 Non-trainable params: 23,587,712 __________________________________________________________________________________________________  from tensorflow.keras.optimizers import RMSprop amodel.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])  Compiling the model using RMSProp as the optimizer and a binary_crossentropy loss because of the 2 classes.\n Next, we use the ImageDataGenerator function in keras to import the images and perform some augmentation on them, as well as to specify some arguments.\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True) test_datagen = ImageDataGenerator( rescale = 1.0/255. ) train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224)) validation_generator = test_datagen.flow_from_directory( validation_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))  Found 5216 images belonging to 2 classes. Found 16 images belonging to 2 classes.  There are 5216 training images and 16 validation images.\nAll that\u0026rsquo;s left is to run the model for just 10 epochs.\nhistory = amodel.fit( train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10, validation_steps = 50, verbose = 2)  Epoch 1/10 100/100 - 47s - loss: 0.8222 - accuracy: 0.6995 - val_loss: 0.6554 - val_accuracy: 0.5625 Epoch 2/10 100/100 - 45s - loss: 0.5184 - accuracy: 0.7490 - val_loss: 0.6276 - val_accuracy: 0.6250 Epoch 3/10 100/100 - 46s - loss: 0.4954 - accuracy: 0.7555 - val_loss: 0.7007 - val_accuracy: 0.5625 Epoch 4/10 100/100 - 46s - loss: 0.4552 - accuracy: 0.7710 - val_loss: 0.9272 - val_accuracy: 0.5625 Epoch 5/10 100/100 - 45s - loss: 0.4410 - accuracy: 0.7835 - val_loss: 0.9437 - val_accuracy: 0.5625 Epoch 6/10 100/100 - 45s - loss: 0.4338 - accuracy: 0.7871 - val_loss: 0.5694 - val_accuracy: 0.6250 Epoch 7/10 100/100 - 45s - loss: 0.4113 - accuracy: 0.7920 - val_loss: 0.9337 - val_accuracy: 0.5625 Epoch 8/10 100/100 - 45s - loss: 0.4150 - accuracy: 0.8001 - val_loss: 0.9175 - val_accuracy: 0.5625 Epoch 9/10 100/100 - 45s - loss: 0.4003 - accuracy: 0.8055 - val_loss: 0.7112 - val_accuracy: 0.6875 Epoch 10/10 100/100 - 46s - loss: 0.3825 - accuracy: 0.8155 - val_loss: 0.6212 - val_accuracy: 0.6250  We get 81% accuracy after 10 epochs. Validation accuracy is around 62%. Other architectures such as VGG16 or VGG19 can also be used, and may increase the accuracy.\n We can plot the training and validation accuracy and see how training occurred.\nimport matplotlib.pyplot as plt acc = history.history['accuracy'] val_acc = history.history['val_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(len(acc)) plt.plot(epochs, acc, 'r', label='Training accuracy') plt.plot(epochs, val_acc, 'b', label='Validation accuracy') plt.title('Training and validation accuracy') plt.legend(loc=0) plt.figure() plt.show()  \u0026lt;Figure size 432x288 with 0 Axes\u0026gt;  Training accuracy seems to be steadily increasing, but validation accuracy seems to have peaks and valleys, but overall it does increase. This might be a sign of some overfitting, but definitely not a large amount of overfitting.\n","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586304000,"objectID":"8d3615d2767b0df251e8442c80c71661","permalink":"https://vishnubharadwaj00.github.io/project/chestxray/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/project/chestxray/","section":"project","summary":"Using Tensorflow to detect Pneumonia in Chest X-Rays","tags":["Deep Learning"],"title":"Detecting Pneumonia with Chest X-Ray Images","type":"project"},{"authors":null,"categories":null,"content":"Full code can be accessed at the Github repository\nBird Sound Classifier fast.ai\u0026rsquo;s courses and software make it extremely easy to start working on difficult projects very quickly. This is just another example of that.\nThis is a Bird Sound Classifying Deep Learning model, which takes in bird sounds, converts them into images (spectograms), and then classifies those images based on what type of bird call it is.\nThe data is from: https://datadryad.org/resource/doi:10.5061/dryad.4g8b7/1\nThere are 6 types of bird calls: distance,hat,kackle,song,stack,tet.\nThis model gets around 80% accuracy, which is not bad at all for something that relies on so many different factors.\nLibraries\nThe necessary libraries and functions have to be imported:\nfrom fastai.vision import * %reload_ext autoreload %autoreload 2 %matplotlib inline from fastai import * import matplotlib.pyplot as plt from matplotlib.pyplot import specgram import librosa import numpy as np import librosa.display  Since this was done on Google\u0026rsquo;s Colab environment, it is necessary to link up Google Drive to the project, so that the data can be imported.\nfrom google.colab import drive drive.mount('/content/gdrive', force_remount=True) root_dir = \u0026quot;/content/gdrive/My Drive/\u0026quot; base_dir = root_dir + 'bird-recognition'  Mounted at /content/gdrive  path = Path(base_dir+'/wav_files_playback')  Creating spectograms:\nNext, a very simple function, create_fold_spectrograms, which takes in the folder name as input, and creates spectrograms in corresponding folders in a seperate path. This uses the librosa package. The code is similar to the one used in: https://github.com/etown/dl1/blob/master/UrbanSoundClassification.ipynb\ndef create_fold_spectrograms(folder): spectrogram_path = Path(base_dir+'/specto') audio_path = path os.makedirs(spectrogram_path/folder,exist_ok=True) for audio_file in list(Path(audio_path/f'{folder}').glob('*.wav')): samples, sample_rate = librosa.load(audio_file) fig = plt.figure(figsize=[0.72,0.72]) ax = fig.add_subplot(111) ax.axes.get_xaxis().set_visible(False) ax.axes.get_yaxis().set_visible(False) ax.set_frame_on(False) filename = spectrogram_path/folder/Path(audio_file).name.replace('.wav','.png') S = librosa.feature.melspectrogram(y=samples, sr=sample_rate) librosa.display.specshow(librosa.power_to_db(S, ref=np.max)) plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0) plt.close('all')  folds=['distance','hat','kackle','song','stack','tet']  for i in folds: create_fold_spectrograms(str(i))  Data Bunch\nOnce, the sound files are converted into image files, the data can be extracted from the folders and seperated into training and validation sets.\nnp.random.seed(42) spectrogram_path = Path(base_dir+'/specto') tfms = get_transforms(do_flip=False) # don't use any transformations because it doesn't make sense in the case of a spectrogram # i.e. flipping a spectrogram changes the meaning data = ImageDataBunch.from_folder(spectrogram_path, train=\u0026quot;.\u0026quot;, ds_tfms=tfms, valid_pct=0.2, size=224) data.normalize(imagenet_stats)  ImageDataBunch; Train: LabelList (152 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList distance,distance,distance,distance,distance Path: /content/gdrive/My Drive/bird-recognition/specto; Valid: LabelList (37 items) x: ImageList Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224) y: CategoryList tet,tet,distance,distance,hat Path: /content/gdrive/My Drive/bird-recognition/specto; Test: None  data.show_batch(rows=3,figsize=(7,7))    data.classes, data.c, len(data.train_ds), len(data.valid_ds)  (['distance', 'hat', 'kackle', 'song', 'stack', 'tet'], 6, 152, 37)  We can see that there are 6 different classes, and a good split between training and validation sets.\nTraining\nNext, cnn_learner can be used, with a resnet34 architecture, to train the model:\nlearn = cnn_learner(data, models.resnet34, metrics=[error_rate,accuracy])  Downloading: \u0026quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth\u0026quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth 100%|██████████| 87306240/87306240 [00:00\u0026lt;00:00, 101948611.46it/s]  learn.fit_one_cycle(6,max_lr=slice(3e-03))   epoch train_loss valid_loss error_rate accuracy time     0 0.438323 0.588238 0.189189 0.810811 00:03   1 0.361946 0.716108 0.324324 0.675676 00:03   2 0.333349 1.141138 0.297297 0.702703 00:03   3 0.290415 1.483750 0.297297 0.702703 00:03   4 0.271594 1.513314 0.324324 0.675676 00:03   5 0.240999 1.303614 0.297297 0.702703 00:03    There\u0026rsquo;s only about 70% accuracy, which can be made higher with the right learning rate:\nlearn.lr_find() learn.recorder.plot()  LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.    learn.unfreeze() learn.fit_one_cycle(6,max_lr=slice(3e-03))   epoch train_loss valid_loss error_rate accuracy time     0 0.089979 1.076153 0.324324 0.675676 00:03   1 0.085291 0.820889 0.243243 0.756757 00:03   2 0.080007 0.758169 0.189189 0.810811 00:03   3 0.099773 0.824883 0.216216 0.783784 00:03   4 0.106347 0.963399 0.243243 0.756757 00:03   5 0.101405 0.916323 0.216216 0.783784 00:03    78% accuracy is the final accuracy. With some tinkering, this can be increased to slightly above 80% as well.\nInterpreting Results:\nUsing the ClassificationInterpretation function, the results of the training model can be interepreted:\ninterp=ClassificationInterpretation.from_learner(learn)  interp.plot_confusion_matrix()    interp.most_confused(min_val=2)  [('tet', 'hat', 5), ('tet', 'stack', 2)]  From this, it is evident that tet is the one causing the most problem, with it being misclassified 7 times, and only correctly classified once. Otherwise, the model is almost fully accurate.\n  ","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"00c0250279105e3bb69085f095312c3c","permalink":"https://vishnubharadwaj00.github.io/project/birdsound/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/project/birdsound/","section":"project","summary":"Using fast.ai's library to classify Birds according to their sounds","tags":["Deep Learning"],"title":"Bird Sound Classifier","type":"project"},{"authors":null,"categories":null,"content":"Full code can be accessed at the Github repository\nWhat brand is this laptop? Based on Lesson 2 of fast.ai\u0026rsquo;s Deep Learning course, it is possible to scrape images of the internet (particularly Google Images) to build our own classifier, which is actually extremely useful and can be applied to any number of applications.\nHere, I chose a really simple problem, to classify laptops based on their brands using images of them. Although it may not seem so simple, since all laptops look similar to a certain extent, the highly efficient Deep Learning models will beg to differ.\nThis model gets around 83% accuracy, which is a very good result considering how similar laptops from different brands look.\nThis is the code used to carry out this task:\nfrom fastai.vision import *  After going on Google Images, and searching for whatever images we want (e.g Macbooks), we can insert a simple Javascript command into the browser:\nurls = Array.from(document.querySelectorAll(\u0026quot;.rg_di .rg_meta\u0026quot;)).map( (el) =\u0026gt; JSON.parse(el.textContent).ou ); window.open(\u0026quot;data:text/csv;charset=utf-8,\u0026quot; + escape(urls.join(\u0026quot;\\n\u0026quot;)));  Next, we create the necessary folder and file name for the data to be imported into.\nI am using Google\u0026rsquo;s Colab so all the images will be stored in Google Drive, from which the images are easily accesible.\nfrom google.colab import drive drive.mount('/content/gdrive', force_remount=True) root_dir = \u0026quot;/content/gdrive/My Drive/\u0026quot; base_dir = root_dir + 'fastai-v3'  Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com\u0026amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob\u0026amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly\u0026amp;response_type=code Enter your authorization code: ·········· Mounted at /content/gdrive  folder = 'macbook' file = 'macbook.txt'  folder = 'hp' file = 'hp.txt'  folder = 'lenovo' file = 'lenovo.txt'  Code has to be run once for every category.\npath = Path(base_dir+'/data/images') dest = path/folder dest.mkdir(parents=True, exist_ok=True)  path.ls()  [PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/macbook.txt'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/macbook'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/lenovo.txt'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/hp'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/hp.txt'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/lenovo'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/models'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/cleaned.csv'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/export.pkl'), PosixPath('/content/gdrive/My Drive/fastai-v3/data/images/mactest.jpg')]  Next, the files (txt files with urls of images) has to be uploaded into Drive.\nOnce that is done, the images can be downloaded into Drive, into the specified folders, from the urls using the download_images function.\ndownload_images(path/file, dest, max_pics=200)  classes = ['macbook','hp','lenovo']  We can remove any images that cannot be opened:\nfor c in classes: print(c) verify_images(path/c, delete=True, max_size=500)  Next, we can extract the images from the folders, and seperate them into training and validation sets, using the ImageDataBunch function.\nnp.random.seed(42) data = ImageDataBunch.from_folder(path, train=\u0026quot;.\u0026quot;, valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)  /usr/local/lib/python3.6/dist-packages/fastai/data_block.py:534: UserWarning: You are labelling your items with CategoryList. Your valid set contained the following unknown labels, the corresponding items have been discarded. images if getattr(ds, 'warn', False): warn(ds.warn)  Looking at some of the pictures:\ndata.show_batch(rows=3, figsize=(7,8))  data.classes, data.c, len(data.train_ds), len(data.valid_ds)  (['hp', 'lenovo', 'macbook'], 3, 306, 75)  Training the model, using the cnn_learner function:\nlearn = cnn_learner(data, models.resnet34, metrics=error_rate)  Downloading: \u0026quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth\u0026quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth 100%|██████████| 87306240/87306240 [00:00\u0026lt;00:00, 162957184.69it/s]  learn.fit_one_cycle(5)   epoch train_loss valid_loss error_rate time     0 1.305020 0.848843 0.346667 00:54   1 1.121091 0.731948 0.293333 00:06   2 0.956481 0.663035 0.293333 00:05   3 0.809013 0.651194 0.266667 00:05   4 0.718085 0.661706 0.240000 00:05    learn.lr_find(start_lr=1e-5, end_lr=1e-1)  LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.  Interpreting the results:\nlearn.recorder.plot()  learn.fit_one_cycle(2,max_lr=slice(1e-03,1e-02))   epoch train_loss valid_loss error_rate time     0 0.152411 0.790561 0.173333 00:05   1 0.102961 0.861176 0.186667 00:05    interp = ClassificationInterpretation.from_learner(learn)  interp.plot_confusion_matrix()  interp.most_confused(min_val=2)  [('lenovo', 'hp', 4), ('hp', 'macbook', 3), ('lenovo', 'macbook', 3), ('macbook', 'hp', 3)]  Lenovo\u0026rsquo;s are being mistaken for HP\u0026rsquo;s 4 times, but the reverse doesn\u0026rsquo;t seem to happen. Macbooks are the ones that are creating most of the error.\nUsing an unused picture, and checking if our model can predict what laptop brand it is:\nlearn.export()  defaults.device = torch.device('cpu')  img = open_image(path/'mactest.jpg') img  learn = load_learner(path)  pred_class,pred_idx,outputs = learn.predict(img) pred_class  Category macbook  img1 = open_image(path/'hptest.jpg') img1  pred_class,pred_idx,outputs = learn.predict(img1) pred_class  Category hp  The model is able to predict these new images perfectly as well.\nA very simple application to do something pretty complex.\n","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"233622995354249e5d848d511f9a6fd6","permalink":"https://vishnubharadwaj00.github.io/project/laptop/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/project/laptop/","section":"project","summary":"Classifying Laptop images according to brands using fast.ai's Deep Learning library","tags":["Deep Learning"],"title":"Laptop Brand Classifier","type":"project"},{"authors":null,"categories":null,"content":"Full code can be accessed at the Github repository\nHow to make a good movie Setup Load packages Let us load the 4 packages needed for this analysis. ggplot2 and gridExtra are required for the data visualizations, dplyr is needed for data manipulation and wrangling, statsr consists of all the statistical functions needed, BAS has the Bayesian functions and MASS contains the stepAIC function.\nlibrary(ggplot2) library(gridExtra) library(dplyr) library(statsr) library(BAS) library(MASS)  Load data The dataset can be loaded in two ways, either by using going to File-\u0026gt;Open File and clicking on the R Workspace file to load the data, or using the load() function. Here, we use the latter:\nload(\u0026quot;movies.Rdata\u0026quot;) dim(movies)  The dataset imported has 651 rows and 32 columns.\n Part 1: Data Rotten Tomatoes and the TomatometerT rating is the most trusted measurement of quality entertainment. As the leading online aggregator of movie and TV show reviews from professional critics, Rotten Tomatoes offers the most comprehensive guide to what\u0026rsquo;s fresh. The world famous TomatometerT rating represents the percentage of positive professional reviews for films and TV shows and is used by millions every day, to help with their entertainment viewing decisions. Rotten Tomatoes designates the best reviewed movies and TV shows as Certified Fresh. That accolade is awarded with Tomatometer ratings of 75% and higher, and a required minimum number of reviews. Weekly Rotten Tomatoes podcasts can be found on RottenTomatoes.com, iTunes, Soundcloud and Stitcher, and Rotten Tomatoes\u0026rsquo; entertainment experts make regular TV and radio appearances across the US.\nData Collection\nGeneralizability\nThe present data were derived from an observational study. The data set is comprised of 651 randomly sampled movies produced and released from 1970 to 2014. According to IMDb, there have 9,962 movies been release from 1972 to 2016 so that the 10% condition (9,962*0.01 = 996) is met. Since the sampling size is large enough and less than 10% of population, it can assume that the random sampling is conducted. Therefore we can conclude that the sample is indeed generalizable to the entire population.\nCausality The data cannot be used to establish a causal relation between the variables of interest as there was no random assignment to the explanatory and independent variables.\n Part 2: Data manipulation In the original dataset , not all of the required features have been provided, so we will perform some feature engineering to create the required features. For the analysis, new features as oscar_season, summer_season, mpaa_rating_R, drama and feature_film. All of them can be derived from existing variables in the dataset.\nmovies \u0026lt;-movies %\u0026gt;% mutate(feature_film = as.factor(ifelse(title_type == \u0026quot;Feature Film\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;))) movies \u0026lt;- movies %\u0026gt;% mutate(drama = as.factor(ifelse(genre == 'Drama', 'Yes', 'No'))) movies \u0026lt;- movies %\u0026gt;% mutate(mpaa_rating_R=as.factor(ifelse(mpaa_rating==\u0026quot;R\u0026quot;,\u0026quot;Yes\u0026quot;,\u0026quot;No\u0026quot;))) movies \u0026lt;- movies %\u0026gt;% mutate(oscar_season=as.factor(ifelse(thtr_rel_month %in% c('10','11','12'),\u0026quot;Yes\u0026quot;,\u0026quot;No\u0026quot;))) movies \u0026lt;- movies %\u0026gt;% mutate(summer_season=as.factor(ifelse(thtr_rel_month %in% c('6','7','8'),\u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;)))   Part 3: Exploratory data analysis Firstly, let us see the analyze the audience_score variable:\nAudience Score:\nsummary(movies$audience_score)  The lowest rating is 11 (Battlefield Earth) and the highest rating is 97.00 (The Godfather Part 2). The median score is 65.00, with a mean of 62.36.\nggplot(data=movies,aes(x=audience_score)) + geom_histogram(binwidth=5)  Using a binwidth=5, we get a readable display. It is evident that this data is left-skewed, with more values on the right side of the mean than the left.\nggplot(data=movies,aes(x=audience_score)) + geom_density()  This plot clearly shows the left skew of the audience_score variable.\nNow, using the variables created in the previous section, plots and summary statistics make it easier to understand the data we have created.\nFeature Film:\nThis variable contains a \u0026ldquo;Yes\u0026rdquo; value if it is a Feature Film and a \u0026ldquo;No\u0026rdquo; value if it is not.\nsummary(movies$feature_film)  This shows that there is mainly a huge majority of feature films. Actually, (591*100)/651= 90.738 % of the data are feature films.\nPlotting this data against audience_score and IMDB rating:\ng1=ggplot(data=movies,aes(x=feature_film,y=audience_score,fill=feature_film)) +geom_boxplot() g2=ggplot(data=movies,aes(x=feature_film,y=imdb_rating,fill=feature_film)) + geom_boxplot() grid.arrange(g1,g2)  Although there are fewer feature films, the distribution shows that feature films generally have a lower score than non-feature films. But this could also be attributed to the fewer number of feature films.\nDrama:\nThis variable contains a \u0026ldquo;Yes\u0026rdquo; value if it is a Drama movie and a \u0026ldquo;No\u0026rdquo; value if it is not.\nLet us check the summary statistics for the drama variable:\nsummary(movies$drama)  Here, the number of drama and non-drama movies are close in count, but there are slightly more non-drama movies.\nPlotting this variable against audience_score and IMDB rating:\ng3=ggplot(data=movies,aes(x=drama,y=audience_score,fill=drama)) +geom_boxplot() g4=ggplot(data=movies,aes(x=drama,y=imdb_rating,fill=drama)) + geom_boxplot() grid.arrange(g3,g4)  There isn\u0026rsquo;t a huge difference but the non-drama movies have slightly lower media score compared to the drama movies. The non-drama movies are also slightly more distributed, but not by a whole lot.\nMPAA Rating:\nThis variable contains a \u0026ldquo;Yes\u0026rdquo; value if the movies has an R MPAA Rating and a \u0026ldquo;No\u0026rdquo; value if it is not R-rated.\nThe summary statistics for the MPAA rating:\nsummary(movies$mpaa_rating_R)  The number of R-rated movies and movies with other ratings are very close.\nPlotting this variable:\ng5=ggplot(data=movies,aes(x=mpaa_rating_R,y=audience_score,fill=mpaa_rating_R)) +geom_boxplot() g6=ggplot(data=movies,aes(x=mpaa_rating_R,y=imdb_rating,fill=mpaa_rating_R)) + geom_boxplot() grid.arrange(g5,g6)  The ratings are very similar for R-rated and non R-rated movies.\nTheir distributions are also extremely similar, with not much to split the two variables.\nOscar Season:\nThis variable contains a \u0026ldquo;Yes\u0026rdquo; value if it was released in the Oscar season and a \u0026ldquo;No\u0026rdquo; value if it was not released in the Oscar season.\nThe summary of the Oscar variable:\nsummary(movies$oscar_season)  There are fewer movies that are released in the Oscar season and only about (191*100)/651=29.3394% are released in the Oscar season.\nLet us plot the variable:\ng7=ggplot(data=movies,aes(x=oscar_season,y=audience_score,fill=oscar_season)) +geom_boxplot() g8=ggplot(data=movies,aes(x=oscar_season,y=imdb_rating,fill=oscar_season)) + geom_boxplot() grid.arrange(g7,g8)  There are slightly higher scores for the movies released in the Oscar Season, but the distributions seem similar.\nSummer season:\nThis variable contains a \u0026ldquo;Yes\u0026rdquo; value if it was released in the Oscar season and a \u0026ldquo;No\u0026rdquo; value if it is not.\nThe summary of the summer variable:\nsummary(movies$summer_season)  Again, most movies are not released in the summer months. Only about (164*100)/651= 25.192% of the movies are released in the summer.\nPlotting this variable against ratings:\ng9=ggplot(data=movies,aes(x=summer_season,y=audience_score,fill=summer_season)) +geom_boxplot() g10=ggplot(data=movies,aes(x=summer_season,y=imdb_rating,fill=summer_season)) + geom_boxplot() grid.arrange(g9,g10)  The plots are almost identical, with very minute differences between them, if any. The movies not released in the summer season have very slightly higher scores, but the difference looks insignificant.\n Part 4: Modeling The best model is not always the most complicated. Sometimes including variables that are not evidently important, can actually reduce the accuracy of predictions. In practice, the model that includes all available explanatory variables is often referred to as the full model. The full model may not be the best model, and if it isn\u0026rsquo;t, we want to identify a smaller model that is preferable.\nFull model: audience_score ~ feature_film + drama + runtime + mpaa_rating_R + thtr_rel_year + oscar_season + summer_season + imdb_rating + imdb_num_votes + critics_score + best_pic_nom + best_pic_win + best_actor_win + best_actress_win + best_dir_win + top200_box\nBayesian Model Averaging (BMA): A comprehensive approach to address model uncertainty is Bayesian model averaging, which allows us to assess the robustness of results to alternative specifications by calculating posterior distributions over coefficients and models. Given the 17 features (n) there can be 2^n = 2^17 possible models. We will explore model uncertainty using posterior probabilities of models based on BIC. We will use BIC as a way to approximate the log of the marginal likelihood. The Bayesian information criterion (BIC) runs through several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2log-likelihood + nparlog(nobs), where npar represents the number of parameters and nobs the number of observations in the fitted model.\nSeperating the required features into a seperate dataframe:\nfeatures \u0026lt;- c('audience_score', 'feature_film', 'drama', 'runtime', 'mpaa_rating_R', 'thtr_rel_year', 'oscar_season', 'summer_season', 'imdb_rating', 'imdb_num_votes', 'critics_score', 'best_pic_nom', 'best_pic_win', 'best_actor_win', 'best_actress_win', 'best_dir_win','top200_box') moviesmodel=movies[ , features] summary(moviesmodel)  There seem to be no NA values, so we can proceed with the model selection process.\nBayesian Information Criterion:\nFirst, we create a multiple linear regression model, with all the factors included.\naudmodel=lm(audience_score~. , data=moviesmodel) summary(audmodel)  It is very evident that the number of factors that are not useful is very high. We can use the BIC (Bayesian Information Criterion) to eliminate the factors that are not significant in this model.\naudienceBIC=bas.lm(audience_score~ ., data=moviesmodel,prior=\u0026quot;BIC\u0026quot;,modelprior=uniform()) audienceBIC  These values denote the marginal posterior inclusion probabilities. We can actually see that IMDB rating and critics score do play a big role.\nFrom this object, we can get the top 5 most probable models:\nsummary(audienceBIC)  We can see that the most probable model contains only 3 variables, runtime, IMDB score and Critics score. The second most probable model contains 2 variables, IMDB score and Critics score.\nThe posterior probability for the top 2 most probably models are about 27%.\nCoeffecients:\nWe can extract the coeffecients from the Bayesian model into a seperate variable:\naudiencecoeff=coef(audienceBIC) #95% Credible Intervals for coeffecients: audinterval=confint(audiencecoeff) audinterval  Let us look at what the 3 most important variables mean.\nFor every 1 point increase in the runtime, the audience score is -2.5e-02 minutes lesser. Similarly, for every 1 point increase in the IMDB rating the audience score is 1.49e+01 points more. And for the Critics score there is an audience score increase of 6.33e-02 points.\nModel space:\nWe can visualize the model space using the image() function.\nimage(audienceBIC,rotate=FALSE)  Due to size constraints, all 17 variables are not shown in this picture. By opening this plot in a new window, all 17 variables are visible and it is evident that \u0026lsquo;runtime + imdb_rating + critics_score\u0026rsquo; is the best model. Also imdb rating and critics score are present in all the top models.\nZellner-Siow Cauchy:\nUsing Zellner-Siow Cauchy, with an MCMC method, we can get a different model:\naudiencezs=bas.lm(audience_score~ ., data=moviesmodel,prior=\u0026quot;ZS-null\u0026quot;,modelprior=uniform(),method=\u0026quot;MCMC\u0026quot;) summary(audiencezs)  Here, the most probable model, with a posterior probability of 14% has only IMDB rating and Critics score. And the 2nd most probable model, with a posterior probability of 12% has runtime, IMDB rating and Critics score. These results are very similar to the BIC method, with only a swap in the first two models.\nModel space:\nWe can visualize the models created:\nimage(audiencezs,rotate=FALSE)  Again, expanding the image, we can see that IMDB rating and Critics score are the major factors. Here, runtime doesn\u0026rsquo;t seem to be playing a huge role.\nSo we can clearly see that while BIC proposes 3 variables (runtime, imdb_rating, critics_score), the ZSC method only proposes 2 (imdb_rating, critics_score).\nAIC Model Selection:\nThe Aikake information criterion (AIC) is a measure of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Hence, AIC provides a means for model selection.\nWe can use backward elimination to find the best model:\nlmaic=lm(audience_score~ ., data=moviesmodel) audienceaic=stepAIC(lmaic,direction='backward',trace=FALSE) audienceaic$anova  We can see that there are a lot more variables in this method: summer_season, top200_box, best_dir_win, best_pic_win, oscar_season, feature_film, drama, imdb_num_votes.\naudienceaic$coefficients  Here, for example, for every 1 point increase in the IMDB rating, the Audience Score increases by 15 points, which is a lot.\nFinal Model:\nIn our final model, we are going to use IMDB ratings and Critics score as the two variables, with the Zellner-Siow Cauchy prior, and MCMC method, with 10^6 iterations of MCMC.\nfeat=c(\u0026quot;audience_score\u0026quot;,\u0026quot;imdb_rating\u0026quot;,\u0026quot;critics_score\u0026quot;) moviesfinal=movies[,feat] audiencezsfin=bas.lm(audience_score~.,data=moviesfinal,prior=\u0026quot;ZS-null\u0026quot;,modelprior=uniform(),method=\u0026quot;MCMC\u0026quot;,MCMC.iteration=10^6) summary(audiencezsfin)  We can see that in this, the first model, which has both variables, has 89% posterior probability.\nModel Diagnostics:\nWe will now look at the best model created:\ndiagnostics(audiencezs, type = \u0026quot;model\u0026quot;, pch = 16, cex = 1.5)  It shows about a straight line at the intercept, which shows a converged posterior probability.\nplot(audiencezs, which = 1, pch=16) abline(a = 0, b = 0, lwd = 2)  There seem to be few outliers, but the points are randomly scattered across the 0 line.\nplot(audiencezs, which=2,add.smooth = F)  The probability starts to straigthen around the 800th model appx, meaning all the models after that don\u0026rsquo;t make much of a difference.\nplot(audiencezs, which=3, ask=F)  These log marginal probabilites are pretty evenly distributed, somewhat favouring 4 or 5 factors.\nplot(audiencezs, which = 4, ask = F, col.in = \u0026quot;red\u0026quot;)  We can see again, that imdb_rating and critics_score matter the most.\n Part 5: Prediction The movie we are going to pick is Money Monster, a movie directed by Jodie Foster, starring George Clooney and Julia Roberts. (https://www.rottentomatoes.com/m/money_monster/) The audience score is 60%.\nWe create the BMA object first:\nBMA=predict(audiencezsfin,estimator=\u0026quot;BMA\u0026quot;, se.fit=TRUE)  We create a data frame with the values to be predicted:\npred=data.frame(imdb_rating=6.5,critics_score=58) aud=predict(audiencezsfin,newdata=pred,estimator=\u0026quot;BMA\u0026quot;,se.fit=TRUE) aud$fit  We get an estimated audience score of 62.49, which is a little higher than the actual score\nThe 95% credible interval is:\naudinterval=confint(aud,parm=\u0026quot;mean\u0026quot;) round(audinterval,3)  We get a confidence interval close to the 60% we were looking for.\n Part 6: Conclusion The predictive model presented here is used to predict the audience scores for a movie. Using Bayesian model averaging and many factors like BIC, ZSC, AIC, etc, many models can be constructed to perform better predictions.\nThe proposed linear model shows a \u0026lsquo;fairly good\u0026rsquo; prediction rate, but it should be noted that the model is based on a very small sample. The fact is that imdb_rating has the highest posterior probability, and that basically all of the newly created features were not that useful to support a better prediction. Creating a model, which has a high predictive power is not so easy to reach. Using Bayes for better prediction is only one part of the game. It might be beneficial to gather more data or try to extend the feature engineering part, which means to creating new meaningful features from existing or gather data for new features.\nPerhaps in a future project, for higher accuracy, we could have included all the remaining factors as well, which was done in the project for the 3rd course of this specialization, and then eliminated them one by one. Even though such models might be prone to overfitting or underfitting, these problems can certainly be mitigated using expert opinion on which factors are actually useful.\n ","date":1568851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568851200,"objectID":"d26cb02689cfec66449f4afbf352d132","permalink":"https://vishnubharadwaj00.github.io/project/bayes/","publishdate":"2019-09-19T00:00:00Z","relpermalink":"/project/bayes/","section":"project","summary":"Using Bayesian Statistical Methodologies to predict movie ratings in R","tags":["Statistics"],"title":"What makes a good movie, asked Bayes","type":"project"},{"authors":null,"categories":null,"content":"Full code can be accessed at the Github repository\nSetup Load packages For this project, we require ggplot2 and gridExtra package for the plots, dplyr for data manipulation and statsr for the statistical functions used in this course. These three packages contain a wide range of functions to be used, and should encompass all the functions we require to do this project:\nlibrary(ggplot2) library(gridExtra) library(dplyr) library(statsr)  Load data We can either go to File -\u0026gt; Open File and select our RData file or load the RData file as it is. This automatically imports our gss dataset into the workspace, as gss. The latter method is used here.\nload(\u0026quot;gss.Rdata\u0026quot;) dim(gss)  We have now loaded a dataset with 57061 rows and 114 columns.\n Part 1: Data According to the GSS website, the General Social Survey (GSS) has studied the growing complexity of American society. It is the only full-probability, personal-interview survey designed to monitor changes in both social characteristics and attitudes currently being conducted in the United States.\nThe GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.\nAs to how the data is collected, the GSS sample is drawn using an area probability design that randomly selects respondents in households across the United States to take part in the survey. The respondents are from a mix of urban, suburban and rural geographic areas. Therefore, random sampling does take place in this survey, meaning the results of the survey can to an extent, be generalized to the adult population of the United States.\nThere are a few things which might not make it completely accurate. Firstly, the GSS is strictly voluntary, meaning even when selected, participants may choose to not attend the survey. Secondly, up until a few years ago, only English speaking participants were chosen, and now Spanish speaking participants have been added. This raises another concern about other languages, as well.\nRandom assignment has not been used here to seperate the sampled population into further groups, which means that causality cannot be inferred from the results of this survey, since we cannot be sure that the only difference between different groups is what we are studying.\n Part 2: Research question With such a vast trove of data, it is possible to create many interesting and insightful research questions, each with their own meaningful conclusions.\nHere, I will focus on one particular area: education levels. We will analyzing 3 questions based on this area:\nQuestion 1: As of 2012 (latest year of the survey), is there a disparity in the education provided to males and females?\nIn other words, for the 2012 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 2012?\nQuestion 2: In 1972 (first year of the survey), was there a disparity in the education provided to males and females?\nIn other words, for the 1972 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 1972?\nQuestion 3: Is there a difference in the education levels of 1972 and 2012?\nTaking gender out of the equation, is there a difference in education levels of the years 1972 and 2012? Has the situation of education improved or declined in those 50 years?\nThis is becoming an all-important question in today\u0026rsquo;s world, with rising calls for equality between men and women. Education is an essential part of gaining knowledge, and an equal footing in education can open up equal opportunities, for men and women. Educational reforms have also been implemented over the years, to provide a higher level of education for all. But has education actually improved, overall?\n Part 3: Exploratory data analysis Question 1:\nFirst let us use only the section of the data that needs to be used, the 2012 data, by filtering it into another dataset.\ngss2012 = gss %\u0026gt;% filter(year==\u0026quot;2012\u0026quot;)  2012 is the latest year of the survey, providing us with the latest results to this research question.\nLet us see a summary of the education levels in the new subset of data we have:\nsummary(gss2012$educ)  We can see that the mean is 13.53, the maximum is 20, and the minimum is 0, with a median of 13. There are also 2 NA\u0026rsquo;s, out of 1974 entries, which is around 1% of the dataset, so we can remove those 2 rows to increase accuracy without affecting the accuracy of the results.\ngss2012= gss2012 %\u0026gt;% filter(educ!=\u0026quot;NA\u0026quot;) summary(gss2012$educ)  We can see that the NA values have been successfully removed.\nLet us look at the means and medians of education levels, grouped by gender:\ngss2012 %\u0026gt;% group_by(sex) %\u0026gt;% summarise(meaned=mean(educ),medec=median(educ))  The mean of female education level is slightly higher than males, and the median is the same for both.\nLet us create a simple boxplot to visualise these statistics, helping us get a better understanding:\nggplot(data=gss2012 ,aes(x=factor(sex),y=educ)) + geom_boxplot()  The boxplot does not seem to show much of a change between the two genders, with the two boxplots looking almost identical.\nA barplot to compare the counts of the education levels, with grouping done on gender might help us understand the data better:\nggplot(data=gss2012,aes(x=educ,fill=sex)) +geom_bar(position=\u0026quot;dodge\u0026quot;)  We can see that as the number of years of education is more than 10, females tend to get more education that men in that education level, except on one or two levels, but the difference is not a lot in most cases.\nLooking at the maximum and minimum, there are slightly more females than males who get 0 years of education, but at the maximum of 20 years, the difference is almost non-existent.\nQuestion 2:\nLet us now look at the data from 1972, by subsetting it into a seperate data frame, and doing a similar EDA on it:\ngss1972=gss %\u0026gt;% filter(year==\u0026quot;1972\u0026quot;) summary(gss1972$educ)  We get 5 NA values out of a total of 1613 values, which is around 3% of the dataset.\ngss1972 = gss1972 %\u0026gt;% filter(gss1972$educ!=\u0026quot;NA\u0026quot;) summary(gss1972$educ)  Already there is a clear difference in the median and mean, without looking at the gender difference.\ngss1972 %\u0026gt;% group_by(sex) %\u0026gt;% summarise(meaned=mean(educ),medianed=median(educ))  The mean of education levels of females was lower than the males in 1972, a sharp contrast to the results in 2012.\nLet us see a simple boxplot to visualise these statistics:\nggplot(data=gss1972,aes(x=sex,y=educ)) + geom_boxplot()  It is very evident from this that, although the male median education level and female median education level are almost the same, the average education level for males is definitely higher, and the 75th percentile level is much higher than that of females.\nCreating a plot to see each year of education:\nggplot(data=gss1972,aes(x=educ,fill=sex)) +geom_bar(position=\u0026quot;dodge\u0026quot;)  In 1972, it is evident that beyond 14 years of education, there are more men than women. These results are much different to the 2012 results.\nComparing the two datasets using scatterplots:\ng1=ggplot(data=gss1972,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(\u0026quot;1972\u0026quot;) g2=ggplot(data=gss2012,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(\u0026quot;2012\u0026quot;) grid.arrange(g1,g2,ncol=2)  The 2 scatter plots on the left are the 1972 data, arranged according to sex, and the 2 scatter plots on the right are the 2012 data, arranged according to sex. It is clear that in 2012, there is a higher concentration at a higher education level, whereas in 1972, that is only for males.\nQuestion 3:\nNow, to analyse the difference in education levels overall, between 1972 and 2012.\nFirst let us combine them into a single dataset to make the analysis easier:\ngssdata=rbind(gss1972,gss2012)  We already have the datasets as well as their summary statistics, so all that is left is to plot them:\nCreating a simple boxplot:\nggplot(data=gssdata,aes(x=factor(year),y=educ))+geom_boxplot()  Immediately, it is evident that there is a significant difference in the education levels between the two years. Although the median of 2012 is only slightly higher than 1972, the range and IQR of 2012 is much higher.\nLet us create a bar graph between the two years:\nggplot(data=gssdata,aes(x=educ,fill=factor(year)))+geom_bar(position=\u0026quot;dodge\u0026quot;)  Upto about 12 years of education, 1972 had a higher proportion, but after 12 years, 2012 had the higher proportion. This means that more people are getting more education in 2012, compared to 1972.\n Part 4: Inference First, let us check the conditions for doing a hypothesis test using the CLT method:\n1.Independence:\nWithin groups:\nRandom sampling was used.\n1972 observations is well below 10% of the population.\nBetween groups:\nThe two groups are independent of each other, as it is a representative of the population. Also the likelihood of dependence otherwise, is very small with this sample size.\n2.Sample Size/Skew:\nThe sample is slightly skewed, and n is well above 30.\nTherefore the conditions for CLT are satisfied and we can use the theoretical method.\nThe method to be used for all 3 questions is that to be used when comparing 2 independent means, as independence has already been established. The critical score will be the t-score corresponding to the degree of freedom. The degree of freedom is the $df =min(n_1 -1 ,n_2 - 1)$. Here, $n_1$=884 and $n_2$=1088, so the degree of freedom is 883.\n Question 1:\nThe null and alternative hypothesis are as follows:\n$H_{0}$ : There is no difference in the education levels of males and females in 2012. $\\mu_{male(2012)}-\\mu_{female(2012)}=0$\n$H_{a}$ There is a difference in the education levels of males and females in 2012. $\\mu_{male(2012)}-\\mu_{female(2012)} \\ne 0$\nOur parameters of interest are $\\mu_{male(2012)}$ and $\\mu_{female(2012)}$ but since we do not have access to that, we will use our point estimates $\\bar{x}_{male(2012)}$ and $\\bar{x}_{female(2012)}$.\nThe significance level $\\alpha=0.05$, which is the standard $\\alpha$.\nUsing the inference function to calculate the p-value for this hypothesis test:\ninference(y=educ,x=sex,data=gss2012,statistic=\u0026quot;mean\u0026quot;,type=\u0026quot;ht\u0026quot;,null=0,alternative=\u0026quot;twosided\u0026quot;,method=\u0026quot;theoretical\u0026quot;)  Results: Response variable: numerical Explanatory variable: categorical (2 levels) n_Male = 884, y_bar_Male = 13.5057, s_Male = 3.1721 n_Female = 1088, y_bar_Female = 13.546, s_Female = 3.0904 H0: mu_Male = mu_Female HA: mu_Male != mu_Female t = -0.2838, df = 883 p_value = 0.7766\nThe high p-value of 0.7766 which is much greater than 0.05, means we fail to reject the null hypothesis $H_0$. We might still run the risk of a type 2 error, but the big p-value offsets the effects of a larger significance level. What the p-value here indicates is the probability of observing extreme data given that the null hypothesis is true, is high.\nWe can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:\ninference(y=educ,x=sex,data=gss2012,statistic=\u0026quot;mean\u0026quot;,type=\u0026quot;ci\u0026quot;,method=\u0026quot;theoretical\u0026quot;,conf_level=0.95)  This 95% Confidence Interval means that we are 95% confident that the difference in the education levels of males and females is between -0.319 and 0.2384. The - sign shows that females getting more education that males.\nThis confidence interval method is well in agreement with the hypothesis test method as the 0 is in the confidence interval that has been produced. The differences shown by the Confidence Interval method are also not that significant, which are the same results that can be inferred from the hypothesis test method.\nIn conclusion, the results of both the confidence interval method and the hypothesis test method indicate that there is an insignificant difference in the education levels of males and females in 2012.\nQuestion 2:\nThe null and alternative hypothesis are as follows:\n$H_{0}$ : There is no difference in the education levels of males and females in 1972. $\\mu_{male(1972)}-\\mu_{female(1972)}=0$\n$H_{a}$ There is a difference in the education levels of males and females in 1972. $\\mu_{male(1972)}-\\mu_{female(1972)} \\ne 0$\nOur parameters of interest are $\\mu_{male(1972)}$ and $\\mu_{female(1972)}$ but since we do not have access to that, we will use our point estimates $\\bar{x}_{male(1972)}$ and $\\bar{x}_{female(1972)}$.\nThe significance level $\\alpha=0.05$, which is the standard $\\alpha$.\nUsing the inference function to calculate the p-value for this hypothesis test:\ninference(y=educ,x=sex,data=gss1972,statistic=\u0026quot;mean\u0026quot;,type=\u0026quot;ht\u0026quot;,null=0,alternative=\u0026quot;twosided\u0026quot;,method=\u0026quot;theoretical\u0026quot;)  From this inference, we get a p-value of 0.01, which is much lower than our significance level of 0.05, which means we reject the $H_0$ hypothesis. Again, we run the risk of a Type-1 error, but even a smaller significance level will not give a different result.\nWe can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:\ninference(y=educ,x=sex,data=gss1972,statistic=\u0026quot;mean\u0026quot;,type=\u0026quot;ci\u0026quot;,method=\u0026quot;theoretical\u0026quot;,conf_level=0.95)  The results show that males get about 0.07 to 0.75 years more education then females. Although this does not seem like a lot, since it is above the significance level of 0.05, it is a significant difference, as the difference when extrapolated to the entire population, becomes much bigger.\nThe conclusion from this test is that there was a significant difference in the education levels of males and females in 1972.\nQuestion 3:\nThe null and alternative hypothesis are as follows:\n$H_{0}$ : There is no difference in the education levels of males and females in 2012. $\\mu_{1972}-\\mu_{2012}=0$\n$H_{a}$ There is a difference in the education levels of males and females in 2012. $\\mu_{1972}-\\mu_{2012} \\ne 0$\nOur parameters of interest are $\\mu_{1972}$ and $\\mu_{2012}$ but since we do not have access to that, we will use our point estimates $\\bar{x}_{1972}$ and $\\bar{x}_{2012}$.\nThe significance level $\\alpha=0.05$, which is the standard $\\alpha$.\nUsing the inference function to calculate the p-value for this hypothesis test:\ninference(y=educ,x=factor(year),data=gssdata,statistic=\u0026quot;mean\u0026quot;,type=\u0026quot;ht\u0026quot;,null=0,alternative=\u0026quot;twosided\u0026quot;,method=\u0026quot;theoretical\u0026quot;)  The p-value obtained is extremely small, \u0026lt;0.0001, which is obviously lesser than the significance level of 0.05, hence we reject the $H_0$ hypothesis. The risk of a type 1 error is even smaller here, as it is such a small p-value.\nA confidence interval can also be created similar to the above questions:\ninference(y=educ,x=factor(year),data=gssdata,statistic=\u0026quot;mean\u0026quot;,type=\u0026quot;ci\u0026quot;,method=\u0026quot;theoretical\u0026quot;,conf_level=0.95)  The confidence interval indicates that there is a 95% chance that on average, people in 2012 had an 1.9825 to 2.4191 more years of education than people in 2012. This is in agreement with the hypothesis test done earlier.\nTherefore, it can be concluded that there is a significant difference in the education levels of 1972 and 2012.\n Part 5: Conclusion It can be concluded that: (1) There is an insignificant difference in the education levels of males and females in the year 2012, with a 95% confidence interval of (-0.319,0.2384).\n(2) There is a significant difference in the education levels of males and females in the year 1972, with a 95% confidence interval of (0.0783,0.7542).\n","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"3293433176d224b46ac0eef56379e7c4","permalink":"https://vishnubharadwaj00.github.io/project/litrates/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/project/litrates/","section":"project","summary":"Analyzing literacy rates using the General Social Survey Dataset in R","tags":["Statistics"],"title":"Literacy Rate Analysis using GSS Data","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://vishnubharadwaj00.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"This World Cup season, the whole world, football fans or not, are abuzz about the ongoing tournament being held in Russia. (Ah yes, that football.) So why not learn a bit more about ‘The Beautiful Game’ to try and show off to your die-hard football friends and family members, eh?\nLet’s start somewhere completely unrelated.\nBrad Pitt.\n  Do I have your attention now?\nNow, what does this annoyingly good-looking specimen have to do with football? I’ll get to that.\nYou may know him as the lead actor in some of those iconic movies you’ve seen, such as Fight Club, Inglourious Basterds, the Ocean’s trilogy, Se7en, Mr and Mrs. Smith, to name a few.\nOr you may know him as the person who has been in relationships with Hollywood’s most famous actresses like Jennifer Aniston and Angelina Jolie. Whatever the means, chances are you’ve heard of him.\nSpeaking of chances, Brad Pitt has also acted in a lesser known 2011 film called Moneyball.\n  Brad Pitt in Moneyball (2011)   He plays Billy Beane, the General Manager of the Oakland Athletics’, a baseball team in the Major League Baseball in the USA. The story revolves around his role in reinventing the team using an experimental technique based on statistics, with the help of a Yale economics graduate, to help them win the league with a significantly lesser amount of money. The movie, nominated for an Oscar, is actually based on a book, written about the events which actually transpired during the 2002 season, during which (spoiler alert) the Oakland A’s defied all odds to win the league with a record breaking 20 game winning streak.\n  The record breaking 2002 Oakland A’s   What did Billy Beane do? He threw the old methods of intuition and expertise in the trash and built a team from scratch purely using statistics for the 2002 season. He knew he didn’t have the budget that the bigger teams such as the Boston Red Sox ($108 million) and the New York Yankees ($125 million). In fact, they had the 3rd lowest budget for that season with just $39 million.\n  Oakland A’s had the 3rd lowest budget in the league   But they used that to their advantage and bought players that scouts had previously overlooked because of their physical quirks such as throwing the ball an unconventional way or having a weird batting stance, but in fact had excellent statistics. He trusted in the statistics despite opposition from almost everyone else managing the team.\nAnd mind you, this was in 2002, before computers and technology was as prevalent as it is now. But he preserved, and he built a dream team to win the league, with minimal financial aid.\nAnd this started a revolution in the sports world.\nSoon, teams from all different sports began to adopt this “Moneyball” technique. It became a commonly used term within sporting professionals, referring to using statistics to revamp a team. And with advancements in technology, this began to be more and more of a trend, popping up all over the world in all different sports.\nCurrently, the leading field in statistical analytics has to be machine learning. Machine learning is a prerequisite to Artificial Intelligence, a term you’re sure to have heard of, thanks to Siri, Google Assistant and Cortana. Machine learning is basically defined as the subfield in computer science to give computers the ability to learn without being programmed explicitly, which is basically what Artificial Intelligence deals with.\nSo what do Siri and Cortana have to do with sports?\nRight now, everything.\nTo give a few examples, the Kolkata Knight Riders, in the Indian Premier League which is a cricket tournament held in India annually, used the help of SAP Labs and an auction analytics tool to give insights on impact players to buy during the 2014 season, which they then went on to win.\nMost teams in the National Football League (NFL) in the USA use machine learning algorithms to pick players during the drafts.\nPredicting match outcomes using such algorithms is becoming a huge trend, with big players such as Microsoft getting into the act and becoming a frontrunner in predicting NBA and NFL matches.\nBut one sport almost seemingly untouched by this revolution is football (yes yes, soccer). The reason being that is a much more complicated sport with a larger number of variables, making it more and more difficult to implement the same tactics used in other sports.\nPredicting football games using the complex data and getting successful outcomes has become a trend, with many using it to bet on games based on the results of various simulations. UK sports betting company Stratagem is using artificial intelligence (deep neural networks, to be precise) to analyze patterns found in football matches, use them to predict outcomes of matches using the proprietary data, and possibly, earn a bit more than usual.\n  For such a money driven sport, it’s a surprise that statistical analysis hasn’t yet been implemented on a large scale.\nDespite all the negative criticism of using a statistical intervention in football clubs, it is evident that the sport will soon be turning towards that direction and there are clear signs showcasing exactly that. There have been many attempts in the past, but none effective enough to make a big enough impact.\nImmediately after the release of the movie, John W. Henry, who had implemented this “Moneyball” technique in the Boston Red Sox as its owner (the beginning of which are showcased at the end of the movie) and had a huge success, looked to do that with his investment in the English Premier League, Liverpool FC. As owner of Liverpool FC, he had hopes of making Liverpool the Boston Red Sox of the Premier League. He even tried to bring on the legendary Billy Beane himself as an advisor.\n  John W. Henry, owner of the Boston Red Sox and Liverpool FC   He brought in Sabermetrics analysts at Anfield, Liverpool’s home, scrutinising every aspect of the team’s game, from current player performances to tactical moves. Liverpool immediately signed on Jose Enrique, a left full-back, only for the reason that he had a high pass completion rate and that he was one of the best players in the league to break into the attacking third of the field. But this soon turned into a highly expensive campaign and after about 18 unsuccessful months, this system was called off\nEven smaller teams in the Premier League, like Stoke City FC, who had been trying to implement similar tactics, gave in and the method became a distant reality for the game of football and people soon forgot about such methods.\n  Jose Enrique had a disappointing spell at Liverpool   Enter FC Midtjylland.\nCan’t even pronounce it, can you? (It’s Mid-Jee-Lund)\nA small Danish club, on the brink of relegation to lower leagues, was bought and revamped by Matthew Benham. Matthew Benham is not your typical football club owner. He made himself a fortune by using mathematical models to bet on football matches, and eventually bought him childhood team, Brentford, a lowly League One team, languishing in the lower leagues of English Football. A strong believer in the “Moneyball” technique, he revamped the entire team based on his pure belief in statistics, very similar to Billy Beane. Brentford were promoted to the Championship that season and very nearly clinched promotion into the Premier League the very next season.\nMatthew Benham then went on to buy FC Midtjylland, a team barely even heard of. He used the same techniques there, and had an even greater success. In a fairytale story (very similar to the Oakland A’s of 2002), within a matter of a season, FC Midtjylland went on to win the Danish Superliga title, their very first major trophy. The match which sealed the deal, a 2–0 win against FC Copenhagen, is said to have been the one of the best matches ever played tactically and was a true testament to the statistical revolution they underwent. Their story went viral when they were matched up with Premier League team Southampton FC, who were strong contenders, in the Europa League of 2015, and beat them convincingly.\n  FC Midtjylland pulled off an impossible feat purely with statistics   At Brentford, Benham’s team identified underrated players and transformed them into superstars for the team. As an example, they made a staggering profit on striker Andre Grey, who was signed for just £500,000, and was eventually sold for a massive £9,000,000. Andre Grey was identified by Benham’s statistical team.\nRasmus Ankersen, who works alongside Benham at both clubs, said: “I met Matthew a few years ago. At the time, Brentford were third in League One and there were a couple of games to go. I said to him, ‘What are the chances of getting promoted?’ When you ask that question you expect an emotional ‘yes’ or ‘no’ from a football owner. But he just said rationally, ‘At the moment, there’s a 42.3 per cent chance we will get promoted’. I knew then he was a guy who was thinking very differently about football than I have ever ­experienced before.”\nMatthew Benham has been hailed as the Billy Beane of modern day football.\n  Matthew Benham(left) with Rasmus Ankersen   Next, a team you’ve probably heard of.\nLeicester City FC.\nI’m disappointed if you can’t pronounce this one.\nSurprise winners of the Premier League title in the 2015/16 season who didn’t use the exact same tactics as Matthew Benham but they did have possibly the greatest underdog story of all time. At 5000–1 odds of winning the Premier League at the start of the season, they pulled off one of the greatest upsets in football history.\nTheir best players were bought at terribly low prices and were previously unheard of. N’Golo Kante, who was playing for a team called Caen in France, was bought for a meagre £5.8 million. Later, he was sold to Chelsea FC for a whopping £30 million. The team’s top two scorers, Jaime Vardy and Riyad Mahrez were bought for £1 million from a non league team in 2012 and £400,000 from a French Ligue 2 team in 2014 respectively. Now, Vardy has a £20 million transfer value and Mahrez with £15 million.\n  The trio that won Leicester the Premier League(L to R: Kante, Vardy, Mahrez)   Even though it doesn’t follow the same statistical beliefs that Brentford and FC Midtjylland used, it is a very similar approach and eventually, had an even more successful result. These are all small-time examples of how statistics and analytics can influence the game of football. But it has not been implemented on a large scale yet.\nThen the question must arise; if small teams with a lesser amount of money can get significant gains, why don’t the bigger teams with larger amounts of money do the same and get an even bigger profit?\nBecause they have a lot more to lose. By injecting a lot of money, the owners of the big teams are afraid of using experimental methods which don’t have a guaranteed success rate.\nRecently, private equity firm Vista invested a huge amount into STATS, a leading sports data and technology company. STATS is at the forefront of football revolution, providing newer methods to efficiently and accurately track player performance as well as make predictions and suggest tactics which could help teams get an edge over one another. They recently used Spatiotemporal data to track teams in the Premier League, and published a paper on the same, which has reached critical acclaim in the footballing world.\n  Image of ball movement patterns in the 20 Premier League teams in the 2010–11 season   With more and more advancements in technology itself, it’s a given that there has been a major improvement in the field of football analytics and there have been substantial results to show for it. Perhaps it is time for the teams, big and small, to adapt with the times and give in to this wave of reform. It is without a doubt that the way football itself is viewed will undergo a drastic shift over the next decade.\nAnd who knows what the future holds? Maybe Brad Pitt will be playing the role of a young football manager on the big screen soon enough.\n  Or maybe not.\n","date":1530533402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530533402,"objectID":"0a7f51c9dd224a6cd8fc9395e3be21ba","permalink":"https://vishnubharadwaj00.github.io/post/footballrevolution/","publishdate":"2018-07-02T12:10:02.098Z","relpermalink":"/post/footballrevolution/","section":"post","summary":"Analyzing the analytical revolution in the beautiful game.","tags":["data science"],"title":"The Football Revolution and Why It’s Almost Here","type":"post"}]