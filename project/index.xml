<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Vishnu Bharadwaj</title>
    <link>https://vishnubharadwaj00.github.io/project/</link>
      <atom:link href="https://vishnubharadwaj00.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 03 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://vishnubharadwaj00.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://vishnubharadwaj00.github.io/project/</link>
    </image>
    
    <item>
      <title>Satellite Image Classifier</title>
      <link>https://vishnubharadwaj00.github.io/project/satelliteimage/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/satelliteimage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Science Salary Predictor</title>
      <link>https://vishnubharadwaj00.github.io/project/datasciencesalarypred/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/datasciencesalarypred/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tesla Stock Prediction using Web Scraping and Recurrent Neural Networks</title>
      <link>https://vishnubharadwaj00.github.io/project/tesla-stock-predictor/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/tesla-stock-predictor/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;Stock Prediction is a versatile and extensive field on its own. With increasingly sophisticated computational capabilities, Stock Prediction is becoming a more and more important application of fields like Machine Learning, Deep Learning and AI.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This project deals with web scraping Tesla stock prices from the Yahoo Finance page, using Beautiful Soup and Selenium, and using Recurrent Neural Networks (particularly LSTMs) to build a Deep Learning model to predict future stock prices.&lt;/p&gt;
&lt;p&gt;Importing the necessary libraries:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
import matplotlib
import pandas as pd
from datetime import datetime
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this is executed in Google Colab, the Google Drive containing the web-scraped data is contained here.&lt;/p&gt;
&lt;p&gt;The code for the webscraping, executed with BeautifulSoup and Selenium, can be found 
&lt;a href=&#34;https://github.com/rmacaraeg/yahoo_finance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;, force_remount=True)
root_dir = &amp;quot;/content/gdrive/My Drive/&amp;quot;
data_dir=os.path.join(root_dir,&amp;quot;tsla.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;amp;response_type=code&amp;amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly

Enter your authorization code:
··········
Mounted at /content/gdrive
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Creating a function to plot Time Series Data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def plot_series(time, series, format=&amp;quot;-&amp;quot;, start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel(&amp;quot;Time&amp;quot;)
    plt.ylabel(&amp;quot;Value&amp;quot;)
    plt.grid(True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reading in the data, using Pandas:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df=pd.read_csv(data_dir)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close*&lt;/th&gt;
      &lt;th&gt;Adj Close**&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Apr 29, 2020&lt;/td&gt;
      &lt;td&gt;790.17&lt;/td&gt;
      &lt;td&gt;803.20&lt;/td&gt;
      &lt;td&gt;783.16&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;15812100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Apr 28, 2020&lt;/td&gt;
      &lt;td&gt;795.64&lt;/td&gt;
      &lt;td&gt;805.00&lt;/td&gt;
      &lt;td&gt;756.69&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;15222000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Apr 27, 2020&lt;/td&gt;
      &lt;td&gt;737.61&lt;/td&gt;
      &lt;td&gt;799.49&lt;/td&gt;
      &lt;td&gt;735.00&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;20681400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Apr 24, 2020&lt;/td&gt;
      &lt;td&gt;710.81&lt;/td&gt;
      &lt;td&gt;730.73&lt;/td&gt;
      &lt;td&gt;698.18&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;13237600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Apr 23, 2020&lt;/td&gt;
      &lt;td&gt;727.60&lt;/td&gt;
      &lt;td&gt;734.00&lt;/td&gt;
      &lt;td&gt;703.13&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;13236700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;248&lt;/th&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;May 06, 2019&lt;/td&gt;
      &lt;td&gt;250.02&lt;/td&gt;
      &lt;td&gt;258.35&lt;/td&gt;
      &lt;td&gt;248.50&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;10833900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;249&lt;/th&gt;
      &lt;td&gt;249&lt;/td&gt;
      &lt;td&gt;May 03, 2019&lt;/td&gt;
      &lt;td&gt;243.86&lt;/td&gt;
      &lt;td&gt;256.61&lt;/td&gt;
      &lt;td&gt;243.49&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;23706800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;250&lt;/th&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;May 02, 2019&lt;/td&gt;
      &lt;td&gt;245.52&lt;/td&gt;
      &lt;td&gt;247.13&lt;/td&gt;
      &lt;td&gt;237.72&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;18159300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;251&lt;/th&gt;
      &lt;td&gt;251&lt;/td&gt;
      &lt;td&gt;May 01, 2019&lt;/td&gt;
      &lt;td&gt;238.85&lt;/td&gt;
      &lt;td&gt;240.00&lt;/td&gt;
      &lt;td&gt;231.50&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;10704400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;252&lt;/th&gt;
      &lt;td&gt;252&lt;/td&gt;
      &lt;td&gt;Apr 30, 2019&lt;/td&gt;
      &lt;td&gt;242.06&lt;/td&gt;
      &lt;td&gt;244.21&lt;/td&gt;
      &lt;td&gt;237.00&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;9464600&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;253 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a year&amp;rsquo;s worth of stock prices here, from April 30, 2019 to April 29, 2020, with a few days&amp;rsquo; data missing.&lt;/p&gt;
&lt;p&gt;But first, there is some Data Cleaning that needs to be done:&lt;/p&gt;
&lt;p&gt;The columns &amp;ldquo;Close&amp;rdquo; and &amp;ldquo;Adj Close&amp;rdquo; have additional * symbols which have to be removed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df.drop(labels=&amp;quot;Unnamed: 0&amp;quot;, axis=1, inplace=True)
df.rename(columns={&amp;quot;Close*&amp;quot;: &amp;quot;Close&amp;quot;, &amp;quot;Adj Close**&amp;quot;: &amp;quot;Adj Close&amp;quot;},inplace=True)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close&lt;/th&gt;
      &lt;th&gt;Adj Close&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Apr 29, 2020&lt;/td&gt;
      &lt;td&gt;790.17&lt;/td&gt;
      &lt;td&gt;803.20&lt;/td&gt;
      &lt;td&gt;783.16&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;15812100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Apr 28, 2020&lt;/td&gt;
      &lt;td&gt;795.64&lt;/td&gt;
      &lt;td&gt;805.00&lt;/td&gt;
      &lt;td&gt;756.69&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;15222000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Apr 27, 2020&lt;/td&gt;
      &lt;td&gt;737.61&lt;/td&gt;
      &lt;td&gt;799.49&lt;/td&gt;
      &lt;td&gt;735.00&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;20681400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Apr 24, 2020&lt;/td&gt;
      &lt;td&gt;710.81&lt;/td&gt;
      &lt;td&gt;730.73&lt;/td&gt;
      &lt;td&gt;698.18&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;13237600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Apr 23, 2020&lt;/td&gt;
      &lt;td&gt;727.60&lt;/td&gt;
      &lt;td&gt;734.00&lt;/td&gt;
      &lt;td&gt;703.13&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;13236700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;248&lt;/th&gt;
      &lt;td&gt;May 06, 2019&lt;/td&gt;
      &lt;td&gt;250.02&lt;/td&gt;
      &lt;td&gt;258.35&lt;/td&gt;
      &lt;td&gt;248.50&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;10833900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;249&lt;/th&gt;
      &lt;td&gt;May 03, 2019&lt;/td&gt;
      &lt;td&gt;243.86&lt;/td&gt;
      &lt;td&gt;256.61&lt;/td&gt;
      &lt;td&gt;243.49&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;23706800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;250&lt;/th&gt;
      &lt;td&gt;May 02, 2019&lt;/td&gt;
      &lt;td&gt;245.52&lt;/td&gt;
      &lt;td&gt;247.13&lt;/td&gt;
      &lt;td&gt;237.72&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;18159300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;251&lt;/th&gt;
      &lt;td&gt;May 01, 2019&lt;/td&gt;
      &lt;td&gt;238.85&lt;/td&gt;
      &lt;td&gt;240.00&lt;/td&gt;
      &lt;td&gt;231.50&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;10704400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;252&lt;/th&gt;
      &lt;td&gt;Apr 30, 2019&lt;/td&gt;
      &lt;td&gt;242.06&lt;/td&gt;
      &lt;td&gt;244.21&lt;/td&gt;
      &lt;td&gt;237.00&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;9464600&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;253 rows × 7 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Next, it would be much easier if the order of the data was reversed, since the data was originally arranged as latest to earliest.&lt;/p&gt;
&lt;p&gt;This would make it easier to use the earlier data for training and the later data for validation/testing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df=pd.DataFrame(df.values[::-1], df.index, df.columns)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close&lt;/th&gt;
      &lt;th&gt;Adj Close&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Apr 30, 2019&lt;/td&gt;
      &lt;td&gt;242.06&lt;/td&gt;
      &lt;td&gt;244.21&lt;/td&gt;
      &lt;td&gt;237&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;9464600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;May 01, 2019&lt;/td&gt;
      &lt;td&gt;238.85&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;231.5&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;10704400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;May 02, 2019&lt;/td&gt;
      &lt;td&gt;245.52&lt;/td&gt;
      &lt;td&gt;247.13&lt;/td&gt;
      &lt;td&gt;237.72&lt;/td&gt;
      &lt;td&gt;244.1&lt;/td&gt;
      &lt;td&gt;244.1&lt;/td&gt;
      &lt;td&gt;18159300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;May 03, 2019&lt;/td&gt;
      &lt;td&gt;243.86&lt;/td&gt;
      &lt;td&gt;256.61&lt;/td&gt;
      &lt;td&gt;243.49&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;23706800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;May 06, 2019&lt;/td&gt;
      &lt;td&gt;250.02&lt;/td&gt;
      &lt;td&gt;258.35&lt;/td&gt;
      &lt;td&gt;248.5&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;10833900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;248&lt;/th&gt;
      &lt;td&gt;Apr 23, 2020&lt;/td&gt;
      &lt;td&gt;727.6&lt;/td&gt;
      &lt;td&gt;734&lt;/td&gt;
      &lt;td&gt;703.13&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;13236700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;249&lt;/th&gt;
      &lt;td&gt;Apr 24, 2020&lt;/td&gt;
      &lt;td&gt;710.81&lt;/td&gt;
      &lt;td&gt;730.73&lt;/td&gt;
      &lt;td&gt;698.18&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;13237600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;250&lt;/th&gt;
      &lt;td&gt;Apr 27, 2020&lt;/td&gt;
      &lt;td&gt;737.61&lt;/td&gt;
      &lt;td&gt;799.49&lt;/td&gt;
      &lt;td&gt;735&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;20681400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;251&lt;/th&gt;
      &lt;td&gt;Apr 28, 2020&lt;/td&gt;
      &lt;td&gt;795.64&lt;/td&gt;
      &lt;td&gt;805&lt;/td&gt;
      &lt;td&gt;756.69&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;15222000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;252&lt;/th&gt;
      &lt;td&gt;Apr 29, 2020&lt;/td&gt;
      &lt;td&gt;790.17&lt;/td&gt;
      &lt;td&gt;803.2&lt;/td&gt;
      &lt;td&gt;783.16&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;15812100&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;253 rows × 7 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Creating a seperate Data Frame to visualize the stock prices better:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;adj_close=df[&#39;Adj Close&#39;]
adj_close.index = df[&#39;Date&#39;]

adj_close.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Date
Apr 30, 2019    238.69
May 01, 2019    234.01
May 02, 2019     244.1
May 03, 2019    255.03
May 06, 2019    255.34
Name: Adj Close, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;type(df[&#39;Date&#39;][0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;str
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &amp;ldquo;Date&amp;rdquo; column in the original Data Frame is of type &amp;ldquo;String&amp;rdquo;, but it has to be of &amp;ldquo;Datetime&amp;rdquo; format, to make it easier to plot it in the x-axis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dates=df[&#39;Date&#39;]
dates1=[]
for date in dates:
    dates1.append(datetime.strptime(date, &#39;%b %d, %Y&#39;))
dates=pd.core.series.Series(dates1)

type(dates[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;pandas._libs.tslibs.timestamps.Timestamp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting the Adjusted Close Stock Prices over the last year:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.figure(figsize=(13,8))
plt.style.use(&#39;fivethirtyeight&#39;)
plt.plot(dates1,adj_close)
plt.title(&amp;quot;Tesla Adjusted Close Prices&amp;quot;,loc=&#39;left&#39;)
plt.rcParams.update({&#39;font.size&#39;: 14})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-plot-of-adjusted-close-stock-prices&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://vishnubharadwaj00.github.io/img/TeslaStockPredictor_20_0.png&#34; data-caption=&#34;Plot of Adjusted Close Stock Prices&#34;&gt;


  &lt;img src=&#34;https://vishnubharadwaj00.github.io/img/TeslaStockPredictor_20_0.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Plot of Adjusted Close Stock Prices
  &lt;/figcaption&gt;


&lt;/figure&gt;

There&amp;rsquo;s a huge up-tick in the price sometime after January 2020, which was when Tesla announced strong fourth-quarter financials, which exceeded all expectations.&lt;/p&gt;
&lt;p&gt;The dip in the price in March corresponds to the COVID-19 pandemic and the financial crisis that it brought with it.&lt;/p&gt;
&lt;p&gt;It does seem to be on the rise now, so it will be interesting to see if the model can predict all these ups and downs.&lt;/p&gt;
&lt;p&gt;Creating Series and Time arrays, and converting them to the right type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;series = np.array(adj_close.values)
time = np.array(dates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;series=pd.to_numeric(series,errors=&#39;coerce&#39;,downcast=&#39;float&#39;)
series
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([238.69, 234.01, 244.1 , 255.03, 255.34, 247.06, 244.84, 241.98,
       239.52, 227.01, 232.31, 231.95, 228.33, 211.03, 205.36, 205.08,
       192.73, 195.49, 190.63, 188.7 , 189.86, 188.22, 185.16, 178.97,
       193.6 , 196.59, 205.95, 204.5 , 212.88, 217.1 , 209.26, 213.91,
       214.92, 225.03, 224.74, 226.43, 219.62, 221.86, 223.64, 219.76,
       219.27, 222.84, 223.46, 227.17, 224.55, 234.9 , 233.1 , 230.34,
       230.06, 238.92, 238.6 , 245.08, 253.5 , 252.38, 254.86, 253.54,
       258.18, 255.68, 260.17, 264.88, 228.82, 228.04, 235.77, 242.26,
       241.61, 233.85, 234.34, 228.32, 230.75, 233.42, 238.3 , 235.01,
       229.01, 235.  , 219.62, 215.64, 219.94, 226.83, 225.86, 220.83,
       222.15, 211.4 , 215.  , 214.08, 215.59, 221.71, 225.61, 225.01,
       220.68, 229.58, 227.45, 231.79, 235.54, 247.1 , 245.87, 245.2 ,
       242.81, 244.79, 243.49, 246.6 , 240.62, 241.23, 223.21, 228.7 ,
       242.56, 242.13, 240.87, 244.69, 243.13, 233.03, 231.43, 237.72,
       240.05, 244.53, 244.74, 247.89, 256.96, 257.89, 259.75, 261.97,
       256.95, 253.5 , 255.58, 254.68, 299.68, 328.13, 327.71, 316.22,
       315.01, 314.92, 313.31, 317.47, 317.22, 326.58, 335.54, 337.14,
       345.09, 349.93, 346.11, 349.35, 352.17, 349.99, 359.52, 352.22,
       354.83, 333.04, 336.34, 328.92, 331.29, 329.94, 334.87, 336.2 ,
       333.03, 330.37, 335.89, 339.53, 348.84, 352.7 , 359.68, 358.39,
       381.5 , 378.99, 393.15, 404.04, 405.59, 419.22, 425.25, 430.94,
       430.38, 414.7 , 418.33, 430.26, 443.01, 451.54, 469.06, 492.14,
       481.34, 478.15, 524.86, 537.92, 518.5 , 513.49, 510.5 , 547.2 ,
       569.56, 572.2 , 564.82, 558.02, 566.9 , 580.99, 640.81, 650.57,
       780.  , 887.06, 734.7 , 748.96, 748.07, 771.28, 774.38, 767.29,
       804.  , 800.03, 858.4 , 917.42, 899.41, 901.  , 833.79, 799.91,
       778.8 , 679.  , 667.99, 743.62, 745.51, 749.5 , 724.54, 703.48,
       608.  , 645.33, 634.23, 560.55, 546.62, 445.07, 430.2 , 361.22,
       427.64, 427.53, 434.29, 505.  , 539.25, 528.16, 514.36, 502.13,
       524.  , 481.56, 454.47, 480.01, 516.24, 545.45, 548.84, 573.  ,
       650.95, 709.89, 729.83, 745.21, 753.89, 746.36, 686.72, 732.11,
       705.63, 725.15, 798.75, 769.12, 800.51], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;series.dtype
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dtype(&#39;float32&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are about 250 tuples, so a 80-20 split is somewhere around 210 for training set and 40 for testing set. The window size (for creating a windowed_dataset) and batch size can also be specified here.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;split_time = 210
adj_train = series[:split_time]
adj_valid = series[split_time:]
dates_train=dates[:split_time]
dates_valid=dates[split_time:]

window_size = 16
batch_size = 32
shuffle_buffer_size = 50
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A function for creating a windowed dataset can be specified here. This is particularly helpful because it helps to create specific sized data slices, to train on them, make predictions, and subsequently learn from those predictions as well.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A function for forecasting data, given the series and the model can be specified here:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def model_forecast(model, series, window_size):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size))
    ds = ds.batch(32).prefetch(1)
    forecast = model.predict(ds)
    return forecast
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Creating our windowed training set:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
window_size = 64
batch_size = 256
train_set = windowed_dataset(adj_train, window_size, batch_size, shuffle_buffer_size)
print(train_set)
print(adj_train.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float32, tf.float32)&amp;gt;
(210,)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifying the layers of our Keras model.&lt;/p&gt;
&lt;p&gt;It has 1 Convolutional Layer, which complements the windowing of the dataset. Simply through extensive trial and error, the configuration of 2 LSTMs (64 and 32 nodes), and 3 Dense Layers (24, 12 and 1 nodes) was selected here.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=60, kernel_size=5,
                      strides=1, padding=&amp;quot;causal&amp;quot;,
                      activation=&amp;quot;relu&amp;quot;,
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(32, return_sequences=True),
  tf.keras.layers.Dense(24, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(12, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400)
])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before actually running our model in its entirity, it might help to specify a learning rate, so the model can be run for a specified number of epochs, and seeing how the model does, a optimized learning rate can be found. This hyperparameter tuning will prove to be very useful later.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))
optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=[&amp;quot;mae&amp;quot;])
history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/100
1/1 [==============================] - 0s 40ms/step - loss: 269.6010 - mae: 270.1010 - lr: 1.0000e-08
Epoch 2/100
1/1 [==============================] - 0s 2ms/step - loss: 269.5738 - mae: 270.0738 - lr: 1.1220e-08
Epoch 3/100
1/1 [==============================] - 0s 2ms/step - loss: 269.5186 - mae: 270.0186 - lr: 1.2589e-08
Epoch 4/100
1/1 [==============================] - 0s 2ms/step - loss: 269.4341 - mae: 269.9341 - lr: 1.4125e-08
Epoch 5/100
1/1 [==============================] - 0s 2ms/step - loss: 269.3180 - mae: 269.8180 - lr: 1.5849e-08
....
....
....
Epoch 96/100
1/1 [==============================] - 0s 2ms/step - loss: 72.6451 - mae: 73.1451 - lr: 5.6234e-04
Epoch 97/100
1/1 [==============================] - 0s 2ms/step - loss: 117.5780 - mae: 118.0779 - lr: 6.3096e-04
Epoch 98/100
1/1 [==============================] - 0s 2ms/step - loss: 81.4628 - mae: 81.9628 - lr: 7.0795e-04
Epoch 99/100
1/1 [==============================] - 0s 2ms/step - loss: 105.4503 - mae: 105.9502 - lr: 7.9433e-04
Epoch 100/100
1/1 [==============================] - 0s 2ms/step - loss: 81.7803 - mae: 82.2790 - lr: 8.9125e-04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting the results of the limited run of the model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.title(&amp;quot;Loss vs Learning Rate&amp;quot;)
plt.xlabel(&amp;quot;Loss&amp;quot;)
plt.ylabel(&amp;quot;Learning Rate&amp;quot;)
plt.semilogx(history.history[&amp;quot;lr&amp;quot;], history.history[&amp;quot;loss&amp;quot;])
#plt.axis([1e-8, 1e-3,135,250])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D at 0x7f3e82620048&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-plot-of-loss-vs-learning-rate&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://vishnubharadwaj00.github.io/img/TeslaStockPredictor_39_1.png&#34; data-caption=&#34;Plot of Loss vs Learning Rate&#34;&gt;


  &lt;img src=&#34;https://vishnubharadwaj00.github.io/img/TeslaStockPredictor_39_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Plot of Loss vs Learning Rate
  &lt;/figcaption&gt;


&lt;/figure&gt;

There&amp;rsquo;s a huge dip in the learning rate somewhere between $10^-7$ and $10^-6$, which helps the gradient descent process in training, helping the model to learn quickly and more effectively, so that can be fixed as the learning rate, and the model is run completely this time:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=60, kernel_size=5,
                      strides=1, padding=&amp;quot;causal&amp;quot;,
                      activation=&amp;quot;relu&amp;quot;,
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(256, return_sequences=True),
  tf.keras.layers.LSTM(128, return_sequences=True),
  tf.keras.layers.Dense(128, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(64, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 500)
])



optimizer = tf.keras.optimizers.SGD(lr=3e-7, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=[&amp;quot;mae&amp;quot;])
history = model.fit(train_set,epochs=350)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/350
1/1 [==============================] - 0s 7ms/step - loss: 330.9964 - mae: 331.4964
Epoch 2/350
1/1 [==============================] - 0s 901us/step - loss: 314.5646 - mae: 315.0646
Epoch 3/350
1/1 [==============================] - 0s 849us/step - loss: 295.7275 - mae: 296.2275
Epoch 4/350
1/1 [==============================] - 0s 986us/step - loss: 277.2317 - mae: 277.7317
Epoch 5/350
1/1 [==============================] - 0s 835us/step - loss: 244.2985 - mae: 244.7985
Epoch 6/350
1/1 [==============================] - 0s 860us/step - loss: 225.8815 - mae: 226.3815
Epoch 7/350
1/1 [==============================] - 0s 836us/step - loss: 208.5272 - mae: 209.0272
Epoch 8/350
1/1 [==============================] - 0s 2ms/step - loss: 184.0172 - mae: 184.5172
Epoch 9/350
1/1 [==============================] - 0s 895us/step - loss: 158.5788 - mae: 159.0788
Epoch 10/350
1/1 [==============================] - 0s 988us/step - loss: 140.6364 - mae: 141.1363
...
...
...
Epoch 101/350
1/1 [==============================] - 0s 927us/step - loss: 48.9045 - mae: 49.4025
Epoch 102/350
1/1 [==============================] - 0s 833us/step - loss: 47.7830 - mae: 48.2795
Epoch 103/350
1/1 [==============================] - 0s 844us/step - loss: 47.8659 - mae: 48.3605
Epoch 104/350
1/1 [==============================] - 0s 1ms/step - loss: 48.0967 - mae: 48.5908
Epoch 105/350
1/1 [==============================] - 0s 832us/step - loss: 46.6607 - mae: 47.1546
...
...
...
Epoch 200/350
1/1 [==============================] - 0s 2ms/step - loss: 25.1572 - mae: 25.6457
Epoch 201/350
1/1 [==============================] - 0s 842us/step - loss: 25.0279 - mae: 25.5159
Epoch 202/350
1/1 [==============================] - 0s 1ms/step - loss: 25.0110 - mae: 25.4993
Epoch 203/350
1/1 [==============================] - 0s 889us/step - loss: 24.9164 - mae: 25.4044
Epoch 204/350
1/1 [==============================] - 0s 933us/step - loss: 24.9017 - mae: 25.3895
Epoch 205/350
1/1 [==============================] - 0s 1ms/step - loss: 24.7882 - mae: 25.2767
...
...
...
Epoch 300/350
1/1 [==============================] - 0s 1ms/step - loss: 22.3528 - mae: 22.8462
Epoch 301/350
1/1 [==============================] - 0s 844us/step - loss: 20.8384 - mae: 21.3259
Epoch 302/350
1/1 [==============================] - 0s 779us/step - loss: 19.6541 - mae: 20.1386
Epoch 303/350
1/1 [==============================] - 0s 820us/step - loss: 19.2455 - mae: 19.7316
Epoch 304/350
1/1 [==============================] - 0s 1ms/step - loss: 19.0995 - mae: 19.5838
Epoch 305/350
1/1 [==============================] - 0s 922us/step - loss: 19.0408 - mae: 19.5255
...
...
...
Epoch 345/350
1/1 [==============================] - 0s 914us/step - loss: 19.7900 - mae: 20.2793
Epoch 346/350
1/1 [==============================] - 0s 836us/step - loss: 18.8462 - mae: 19.3341
Epoch 347/350
1/1 [==============================] - 0s 963us/step - loss: 18.4283 - mae: 18.9135
Epoch 348/350
1/1 [==============================] - 0s 852us/step - loss: 18.3297 - mae: 18.8162
Epoch 349/350
1/1 [==============================] - 0s 963us/step - loss: 18.4796 - mae: 18.9658
Epoch 350/350
1/1 [==============================] - 0s 1ms/step - loss: 19.0140 - mae: 19.5016
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The MAE (Mean Absolute Error) seems pretty low after running it for 350 epochs, which is an excellent sign. It started at ~330 and ended around ~19. Now we can build a forecast using the model_forecast function specified earlier:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)
rnn_forecast = rnn_forecast[split_time - window_size:-1,-1, 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are mainly 2 metrics used for evaluating the accuracy of time series data, Mean Absolute Error and Root Mean Squared Error.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;n = tf.keras.metrics.MeanAbsoluteError()
n.update_state(adj_valid, rnn_forecast)
print(&#39;Mean Absolute Error: &#39;, n.result().numpy())
m = tf.keras.metrics.RootMeanSquaredError()
m.update_state(adj_valid, rnn_forecast)
print(&#39;Root Mean Squared Error: &#39;, m.result().numpy())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean Absolute Error:  152.27446
Root Mean Squared Error:  171.10997
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Considering the fact that there was only 250 data points, and a pretty simple RNN built, a MAE of 150 and RMSE of 171 is extremely good. With more data, and a bigger model, it&amp;rsquo;s very possible to reduce these numbers.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Plotting the predicted data against what actually happened:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.figure(figsize=(20, 6))
plt.style.use(&#39;fivethirtyeight&#39;)
plt.title(&amp;quot;Predictions vs Reality&amp;quot;, loc=&amp;quot;left&amp;quot;)
plt.plot(dates_valid, adj_valid, label=&amp;quot;Actual&amp;quot;)
plt.plot(dates_valid, rnn_forecast, label= &amp;quot;Prediction&amp;quot;)
plt.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend at 0x7f12360679e8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-plot-of-predicted-vs-actual&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://vishnubharadwaj00.github.io/img/TeslaStockPredictor_47_1.png&#34; data-caption=&#34;Plot of Predicted vs Actual&#34;&gt;


  &lt;img src=&#34;https://vishnubharadwaj00.github.io/img/TeslaStockPredictor_47_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Plot of Predicted vs Actual
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model seemed to have gotten it pretty spot-on. It dips when the actual value dipped and seems to be on the rise, as is the case towards the end.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Pneumonia with Chest X-Ray Images</title>
      <link>https://vishnubharadwaj00.github.io/project/chestxray/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/chestxray/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/PneumoniaDetection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Healthcare is an extremely important part of the technological revolution, with deep learning techniques being applied to more and more medical problems.&lt;/p&gt;
&lt;p&gt;This dataset for the detection of pneumonia, using chest x-ray images (&lt;a href=&#34;https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia&#34;&gt;https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia&lt;/a&gt;) is used for binary classification (whether the person is normal or has pneumonia).&lt;/p&gt;
&lt;p&gt;This notebook was executed in a Google Colab environment, and used transfer learning from a ResNet50 architecture, and after just 10 epochs of training, I was able to achieve &lt;strong&gt;&amp;gt;80% accuracy&lt;/strong&gt;. Some minor tweaks and additional architectural changes can definitely increase the accuracy to close to 90% and maybe, even beyond that.&lt;/p&gt;
&lt;p&gt;Using this very simple code, we can use the Kaggle API to download the dataset into our environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
os.environ[&#39;KAGGLE_USERNAME&#39;] = &amp;quot;##########&amp;quot; # username from the json file
os.environ[&#39;KAGGLE_KEY&#39;] = &amp;quot;###################&amp;quot; # key from the json file
!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading chest-xray-pneumonia.zip to /content
100% 2.29G/2.29G [00:30&amp;lt;00:00, 24.3MB/s]
100% 2.29G/2.29G [00:30&amp;lt;00:00, 80.1MB/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, the file has to be unzipped (it is in zip format) and the directories are specified for the train, test and validation sets, as well as the normal and pneumonia directories for the train and validation sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import zipfile
local_zip = &#39;/content/chest-xray-pneumonia.zip&#39;
zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;)
zip_ref.extractall(&#39;/content&#39;)
zip_ref.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;base_dir = &#39;/content/chest_xray&#39;
train_dir = os.path.join(base_dir, &#39;train&#39;)
validation_dir = os.path.join(base_dir, &#39;val&#39;)
test_dir = os.path.join(base_dir, &#39;test&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;train_normal_dir = os.path.join(train_dir,&#39;NORMAL&#39;)
train_pneu_dir = os.path.join(train_dir,&#39;PNEUMONIA&#39;)
validation_normal_dir = os.path.join(validation_dir,&#39;NORMAL&#39;)
validation_pneu_dir = os.path.join(validation_dir,&#39;PNEUMONIA&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;print(&#39;total training normal images:&#39;, len(os.listdir(train_normal_dir)))
print(&#39;total training pneu images:&#39;, len(os.listdir(train_pneu_dir)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;total training normal images: 1341
total training pneu images: 3875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 1341 normal images and 3875 pneumonia images in the training set, which seems to be more than sufficient.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import Model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We specify a constant image size to make things uniform for the model to input. We then input the ResNet50 architecture, and ensure it cannot be trained, and that we retain all the weights.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;img_size=[224,224]
model=tf.keras.applications.resnet50.ResNet50(input_shape=img_size + [3], weights=&#39;imagenet&#39;, include_top=False)
model.trainable = False
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
94773248/94765736 [==============================] - 1s 0us/step
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last layer of the architecture is the &amp;lsquo;conv5_block3_out&amp;rsquo;, with a shape of (7,7,2048) which we will use later.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;last_layer = model.get_layer(&#39;conv5_block3_out&#39;)
print(&#39;last layer output shape: &#39;, last_layer.output_shape)
last_output = last_layer.output
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;last layer output shape:  (None, 7, 7, 2048)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then specify 2 last layers, a dense layer with 1024 nodes, with a 20% dropout rate to prevent overfitting, as well as a final dense layer with a single node and a sigmoid activation (which outputs 0 or 1 to classify)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = layers.Flatten()(last_output)
x = layers.Dense(1024, activation=&#39;relu&#39;)(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense  (1, activation=&#39;sigmoid&#39;)(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;amodel=Model(model.input,x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The layers of our new model (amodel) and their shapes can be seen using the summary() function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;amodel.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model_2&amp;quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_2 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_2[0][0]
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]
                                                                 conv2_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]
                                                                 conv2_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]
                                                                 conv2_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]
                                                                 conv3_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]
                                                                 conv3_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]
                                                                 conv3_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]
                                                                 conv3_block4_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]
                                                                 conv4_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]
                                                                 conv4_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]
                                                                 conv4_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]
                                                                 conv4_block4_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]
                                                                 conv4_block5_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]
                                                                 conv4_block6_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]
                                                                 conv5_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]
                                                                 conv5_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]
                                                                 conv5_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 100352)       0           conv5_block3_out[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         102761472   flatten_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 1024)         0           dense_2[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            1025        dropout_1[0][0]
==================================================================================================
Total params: 126,350,209
Trainable params: 102,762,497
Non-trainable params: 23,587,712
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;from tensorflow.keras.optimizers import RMSprop
amodel.compile(optimizer = RMSprop(lr=0.0001),
              loss = &#39;binary_crossentropy&#39;,
              metrics = [&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compiling the model using RMSProp as the optimizer and a binary_crossentropy loss because of the 2 classes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Next, we use the ImageDataGenerator function in keras to import the images and perform some augmentation on them, as well as to specify some arguments.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255.,
                                   rotation_range = 40,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator( rescale = 1.0/255. )

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    batch_size = 20,
                                                    class_mode = &#39;binary&#39;,
                                                    target_size = (224, 224))

validation_generator =  test_datagen.flow_from_directory( validation_dir,
                                                          batch_size  = 20,
                                                          class_mode  = &#39;binary&#39;,
                                                          target_size = (224, 224))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Found 5216 images belonging to 2 classes.
Found 16 images belonging to 2 classes.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 5216 training images and 16 validation images.&lt;/p&gt;
&lt;p&gt;All that&amp;rsquo;s left is to run the model for just 10 epochs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;history = amodel.fit(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = 100,
            epochs = 10,
            validation_steps = 50,
            verbose = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
100/100 - 47s - loss: 0.8222 - accuracy: 0.6995 - val_loss: 0.6554 - val_accuracy: 0.5625
Epoch 2/10
100/100 - 45s - loss: 0.5184 - accuracy: 0.7490 - val_loss: 0.6276 - val_accuracy: 0.6250
Epoch 3/10
100/100 - 46s - loss: 0.4954 - accuracy: 0.7555 - val_loss: 0.7007 - val_accuracy: 0.5625
Epoch 4/10
100/100 - 46s - loss: 0.4552 - accuracy: 0.7710 - val_loss: 0.9272 - val_accuracy: 0.5625
Epoch 5/10
100/100 - 45s - loss: 0.4410 - accuracy: 0.7835 - val_loss: 0.9437 - val_accuracy: 0.5625
Epoch 6/10
100/100 - 45s - loss: 0.4338 - accuracy: 0.7871 - val_loss: 0.5694 - val_accuracy: 0.6250
Epoch 7/10
100/100 - 45s - loss: 0.4113 - accuracy: 0.7920 - val_loss: 0.9337 - val_accuracy: 0.5625
Epoch 8/10
100/100 - 45s - loss: 0.4150 - accuracy: 0.8001 - val_loss: 0.9175 - val_accuracy: 0.5625
Epoch 9/10
100/100 - 45s - loss: 0.4003 - accuracy: 0.8055 - val_loss: 0.7112 - val_accuracy: 0.6875
Epoch 10/10
100/100 - 46s - loss: 0.3825 - accuracy: 0.8155 - val_loss: 0.6212 - val_accuracy: 0.6250
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get 81% accuracy after 10 epochs. Validation accuracy is around 62%. Other architectures such as VGG16 or VGG19 can also be used, and may increase the accuracy.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We can plot the training and validation accuracy and see how training occurred.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import matplotlib.pyplot as plt
acc = history.history[&#39;accuracy&#39;]
val_acc = history.history[&#39;val_accuracy&#39;]
loss = history.history[&#39;loss&#39;]
val_loss = history.history[&#39;val_loss&#39;]

epochs = range(len(acc))

plt.plot(epochs, acc, &#39;r&#39;, label=&#39;Training accuracy&#39;)
plt.plot(epochs, val_acc, &#39;b&#39;, label=&#39;Validation accuracy&#39;)
plt.title(&#39;Training and validation accuracy&#39;)
plt.legend(loc=0)
plt.figure()


plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vishnubharadwaj00.github.io/projectimages/ChestXRayResNet50_files/ChestXRayResNet50_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 432x288 with 0 Axes&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training accuracy seems to be steadily increasing, but validation accuracy seems to have peaks and valleys, but overall it does increase. This might be a sign of some overfitting, but definitely not a large amount of overfitting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bird Sound Classifier</title>
      <link>https://vishnubharadwaj00.github.io/project/birdsound/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/birdsound/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/BirdSoundClassifier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;bird-sound-classifier&#34;&gt;&lt;strong&gt;Bird Sound Classifier&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;fast.ai&amp;rsquo;s courses and software make it extremely easy to start working on difficult projects very quickly. This is just another example of that.&lt;/p&gt;
&lt;p&gt;This is a Bird Sound Classifying Deep Learning model, which takes in bird sounds, converts them into images (spectograms), and then classifies those images based on what type of bird call it is.&lt;/p&gt;
&lt;p&gt;The data is from: &lt;a href=&#34;https://datadryad.org/resource/doi:10.5061/dryad.4g8b7/1&#34;&gt;https://datadryad.org/resource/doi:10.5061/dryad.4g8b7/1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are 6 types of bird calls: distance,hat,kackle,song,stack,tet.&lt;/p&gt;
&lt;p&gt;This model gets around 80% accuracy, which is not bad at all for something that relies on so many different factors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Libraries&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The necessary libraries and functions have to be imported:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from fastai.vision import *
%reload_ext autoreload
%autoreload 2
%matplotlib inline
from fastai import *
import matplotlib.pyplot as plt
from matplotlib.pyplot import specgram
import librosa
import numpy as np
import librosa.display
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this was done on Google&amp;rsquo;s Colab environment, it is necessary to link up Google Drive to the project, so that the data can be imported.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;, force_remount=True)
root_dir = &amp;quot;/content/gdrive/My Drive/&amp;quot;
base_dir = root_dir + &#39;bird-recognition&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mounted at /content/gdrive
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;path = Path(base_dir+&#39;/wav_files_playback&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Creating spectograms:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next, a very simple function, create_fold_spectrograms, which takes in the folder name as input, and creates spectrograms in corresponding folders in a seperate path. This uses the librosa package. The code is similar to the one used in: &lt;a href=&#34;https://github.com/etown/dl1/blob/master/UrbanSoundClassification.ipynb&#34;&gt;https://github.com/etown/dl1/blob/master/UrbanSoundClassification.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def create_fold_spectrograms(folder):
    spectrogram_path = Path(base_dir+&#39;/specto&#39;)
    audio_path = path
    os.makedirs(spectrogram_path/folder,exist_ok=True)
    for audio_file in list(Path(audio_path/f&#39;{folder}&#39;).glob(&#39;*.wav&#39;)):
        samples, sample_rate = librosa.load(audio_file)
        fig = plt.figure(figsize=[0.72,0.72])
        ax = fig.add_subplot(111)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        ax.set_frame_on(False)
        filename  = spectrogram_path/folder/Path(audio_file).name.replace(&#39;.wav&#39;,&#39;.png&#39;)
        S = librosa.feature.melspectrogram(y=samples, sr=sample_rate)
        librosa.display.specshow(librosa.power_to_db(S, ref=np.max))
        plt.savefig(filename, dpi=400, bbox_inches=&#39;tight&#39;,pad_inches=0)
        plt.close(&#39;all&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folds=[&#39;distance&#39;,&#39;hat&#39;,&#39;kackle&#39;,&#39;song&#39;,&#39;stack&#39;,&#39;tet&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;for i in folds:
  create_fold_spectrograms(str(i))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Data Bunch&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once, the sound files are converted into image files, the data can be extracted from the folders and seperated into training and validation sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;np.random.seed(42)
spectrogram_path = Path(base_dir+&#39;/specto&#39;)
tfms = get_transforms(do_flip=False)
# don&#39;t use any transformations because it doesn&#39;t make sense in the case of a spectrogram
# i.e. flipping a spectrogram changes the meaning
data = ImageDataBunch.from_folder(spectrogram_path, train=&amp;quot;.&amp;quot;, ds_tfms=tfms, valid_pct=0.2, size=224)
data.normalize(imagenet_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ImageDataBunch;

Train: LabelList (152 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
distance,distance,distance,distance,distance
Path: /content/gdrive/My Drive/bird-recognition/specto;

Valid: LabelList (37 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
tet,tet,distance,distance,hat
Path: /content/gdrive/My Drive/bird-recognition/specto;

Test: None
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;data.show_batch(rows=3,figsize=(7,7))
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://vishnubharadwaj00.github.io/img/BirdSoundClassifier_12_0.png&#34; &gt;


  &lt;img src=&#34;https://vishnubharadwaj00.github.io/img/BirdSoundClassifier_12_0.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;pre&gt;&lt;code&gt;data.classes, data.c, len(data.train_ds), len(data.valid_ds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;([&#39;distance&#39;, &#39;hat&#39;, &#39;kackle&#39;, &#39;song&#39;, &#39;stack&#39;, &#39;tet&#39;], 6, 152, 37)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that there are 6 different classes, and a good split between training and validation sets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next, cnn_learner can be used, with a resnet34 architecture, to train the model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn = cnn_learner(data, models.resnet34, metrics=[error_rate,accuracy])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading: &amp;quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&amp;quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth
100%|██████████| 87306240/87306240 [00:00&amp;lt;00:00, 101948611.46it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;learn.fit_one_cycle(6,max_lr=slice(3e-03))
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;accuracy&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.438323&lt;/td&gt;
      &lt;td&gt;0.588238&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.810811&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.361946&lt;/td&gt;
      &lt;td&gt;0.716108&lt;/td&gt;
      &lt;td&gt;0.324324&lt;/td&gt;
      &lt;td&gt;0.675676&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.333349&lt;/td&gt;
      &lt;td&gt;1.141138&lt;/td&gt;
      &lt;td&gt;0.297297&lt;/td&gt;
      &lt;td&gt;0.702703&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.290415&lt;/td&gt;
      &lt;td&gt;1.483750&lt;/td&gt;
      &lt;td&gt;0.297297&lt;/td&gt;
      &lt;td&gt;0.702703&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.271594&lt;/td&gt;
      &lt;td&gt;1.513314&lt;/td&gt;
      &lt;td&gt;0.324324&lt;/td&gt;
      &lt;td&gt;0.675676&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.240999&lt;/td&gt;
      &lt;td&gt;1.303614&lt;/td&gt;
      &lt;td&gt;0.297297&lt;/td&gt;
      &lt;td&gt;0.702703&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There&amp;rsquo;s only about 70% accuracy, which can be made higher with the right learning rate:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.lr_find()
learn.recorder.plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://vishnubharadwaj00.github.io/img/BirdSoundClassifier_19_2.png&#34; &gt;


  &lt;img src=&#34;https://vishnubharadwaj00.github.io/img/BirdSoundClassifier_19_2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;pre&gt;&lt;code&gt;learn.unfreeze()
learn.fit_one_cycle(6,max_lr=slice(3e-03))
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;accuracy&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.089979&lt;/td&gt;
      &lt;td&gt;1.076153&lt;/td&gt;
      &lt;td&gt;0.324324&lt;/td&gt;
      &lt;td&gt;0.675676&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.085291&lt;/td&gt;
      &lt;td&gt;0.820889&lt;/td&gt;
      &lt;td&gt;0.243243&lt;/td&gt;
      &lt;td&gt;0.756757&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.080007&lt;/td&gt;
      &lt;td&gt;0.758169&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.810811&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.099773&lt;/td&gt;
      &lt;td&gt;0.824883&lt;/td&gt;
      &lt;td&gt;0.216216&lt;/td&gt;
      &lt;td&gt;0.783784&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.106347&lt;/td&gt;
      &lt;td&gt;0.963399&lt;/td&gt;
      &lt;td&gt;0.243243&lt;/td&gt;
      &lt;td&gt;0.756757&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.101405&lt;/td&gt;
      &lt;td&gt;0.916323&lt;/td&gt;
      &lt;td&gt;0.216216&lt;/td&gt;
      &lt;td&gt;0.783784&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;78% accuracy is the final accuracy. With some tinkering, this can be increased to slightly above 80% as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting Results:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the ClassificationInterpretation function, the results of the training model can be interepreted:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;interp=ClassificationInterpretation.from_learner(learn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;interp.plot_confusion_matrix()
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://vishnubharadwaj00.github.io/img/BirdSoundClassifier_24_0.png&#34; &gt;


  &lt;img src=&#34;https://vishnubharadwaj00.github.io/img/BirdSoundClassifier_24_0.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;pre&gt;&lt;code&gt;interp.most_confused(min_val=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(&#39;tet&#39;, &#39;hat&#39;, 5), (&#39;tet&#39;, &#39;stack&#39;, 2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this, it is evident that tet is the one causing the most problem, with it being misclassified 7 times, and only correctly classified once. Otherwise, the model is almost fully accurate.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Laptop Brand Classifier</title>
      <link>https://vishnubharadwaj00.github.io/project/laptop/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/laptop/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/LaptopBrandRecognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;what-brand-is-this-laptop&#34;&gt;&lt;strong&gt;What brand is this laptop?&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;Based on Lesson 2 of fast.ai&amp;rsquo;s Deep Learning course, it is possible to scrape images of the internet (particularly Google Images) to build our own classifier, which is actually extremely useful and can be applied to any number of applications.&lt;/p&gt;
&lt;p&gt;Here, I chose a really simple problem, to classify laptops based on their brands using images of them. Although it may not seem so simple, since all laptops look similar to a certain extent, the highly efficient Deep Learning models will beg to differ.&lt;/p&gt;
&lt;p&gt;This model gets around &lt;strong&gt;83% accuracy&lt;/strong&gt;, which is a very good result considering how similar laptops from different brands look.&lt;/p&gt;
&lt;p&gt;This is the code used to carry out this task:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from fastai.vision import *
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After going on Google Images, and searching for whatever images we want (e.g Macbooks), we can insert a simple Javascript command into the browser:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;urls = Array.from(document.querySelectorAll(&amp;quot;.rg_di .rg_meta&amp;quot;)).map(
  (el) =&amp;gt; JSON.parse(el.textContent).ou
);
window.open(&amp;quot;data:text/csv;charset=utf-8,&amp;quot; + escape(urls.join(&amp;quot;\n&amp;quot;)));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create the necessary folder and file name for the data to be imported into.&lt;/p&gt;
&lt;p&gt;I am using Google&amp;rsquo;s Colab so all the images will be stored in Google Drive, from which the images are easily accesible.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;, force_remount=True)
root_dir = &amp;quot;/content/gdrive/My Drive/&amp;quot;
base_dir = root_dir + &#39;fastai-v3&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;amp;response_type=code

Enter your authorization code:
··········
Mounted at /content/gdrive
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folder = &#39;macbook&#39;
file = &#39;macbook.txt&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folder = &#39;hp&#39;
file = &#39;hp.txt&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folder = &#39;lenovo&#39;
file = &#39;lenovo.txt&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Code has to be run once for every category.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;path = Path(base_dir+&#39;/data/images&#39;)
dest = path/folder
dest.mkdir(parents=True, exist_ok=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;path.ls()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/macbook.txt&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/macbook&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/lenovo.txt&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/hp&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/hp.txt&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/lenovo&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/models&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/cleaned.csv&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/export.pkl&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/mactest.jpg&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, the files (txt files with urls of images) has to be uploaded into Drive.&lt;/p&gt;
&lt;p&gt;Once that is done, the images can be downloaded into Drive, into the specified folders, from the urls using the download_images function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;download_images(path/file, dest, max_pics=200)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;classes = [&#39;macbook&#39;,&#39;hp&#39;,&#39;lenovo&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can remove any images that cannot be opened:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for c in classes:
    print(c)
    verify_images(path/c, delete=True, max_size=500)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we can extract the images from the folders, and seperate them into training and validation sets, using the ImageDataBunch function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;np.random.seed(42)
data = ImageDataBunch.from_folder(path, train=&amp;quot;.&amp;quot;, valid_pct=0.2,
        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/fastai/data_block.py:534: UserWarning: You are labelling your items with CategoryList.
Your valid set contained the following unknown labels, the corresponding items have been discarded.
images
  if getattr(ds, &#39;warn&#39;, False): warn(ds.warn)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at some of the pictures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data.show_batch(rows=3, figsize=(7,8))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vishnubharadwaj00.github.io/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data.classes, data.c, len(data.train_ds), len(data.valid_ds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;([&#39;hp&#39;, &#39;lenovo&#39;, &#39;macbook&#39;], 3, 306, 75)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training the model, using the cnn_learner function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn = cnn_learner(data, models.resnet34, metrics=error_rate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading: &amp;quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&amp;quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth
100%|██████████| 87306240/87306240 [00:00&amp;lt;00:00, 162957184.69it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;learn.fit_one_cycle(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.305020&lt;/td&gt;
      &lt;td&gt;0.848843&lt;/td&gt;
      &lt;td&gt;0.346667&lt;/td&gt;
      &lt;td&gt;00:54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.121091&lt;/td&gt;
      &lt;td&gt;0.731948&lt;/td&gt;
      &lt;td&gt;0.293333&lt;/td&gt;
      &lt;td&gt;00:06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.956481&lt;/td&gt;
      &lt;td&gt;0.663035&lt;/td&gt;
      &lt;td&gt;0.293333&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.809013&lt;/td&gt;
      &lt;td&gt;0.651194&lt;/td&gt;
      &lt;td&gt;0.266667&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.718085&lt;/td&gt;
      &lt;td&gt;0.661706&lt;/td&gt;
      &lt;td&gt;0.240000&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;learn.lr_find(start_lr=1e-5, end_lr=1e-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interpreting the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.recorder.plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vishnubharadwaj00.github.io/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.fit_one_cycle(2,max_lr=slice(1e-03,1e-02))
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.152411&lt;/td&gt;
      &lt;td&gt;0.790561&lt;/td&gt;
      &lt;td&gt;0.173333&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.102961&lt;/td&gt;
      &lt;td&gt;0.861176&lt;/td&gt;
      &lt;td&gt;0.186667&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;interp = ClassificationInterpretation.from_learner(learn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;interp.plot_confusion_matrix()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vishnubharadwaj00.github.io/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;interp.most_confused(min_val=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(&#39;lenovo&#39;, &#39;hp&#39;, 4),
 (&#39;hp&#39;, &#39;macbook&#39;, 3),
 (&#39;lenovo&#39;, &#39;macbook&#39;, 3),
 (&#39;macbook&#39;, &#39;hp&#39;, 3)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lenovo&amp;rsquo;s are being mistaken for HP&amp;rsquo;s 4 times, but the reverse doesn&amp;rsquo;t seem to happen. Macbooks are the ones that are creating most of the error.&lt;/p&gt;
&lt;p&gt;Using an unused picture, and checking if our model can predict what laptop brand it is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.export()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;defaults.device = torch.device(&#39;cpu&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;img = open_image(path/&#39;mactest.jpg&#39;)
img
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vishnubharadwaj00.github.io/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn = load_learner(path)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;pred_class,pred_idx,outputs = learn.predict(img)
pred_class
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Category macbook
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;img1  = open_image(path/&#39;hptest.jpg&#39;)
img1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vishnubharadwaj00.github.io/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pred_class,pred_idx,outputs = learn.predict(img1)
pred_class
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Category hp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model is able to predict these new images perfectly as well.&lt;/p&gt;
&lt;p&gt;A very simple application to do something pretty complex.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What makes a good movie, asked Bayes</title>
      <link>https://vishnubharadwaj00.github.io/project/bayes/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/bayes/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/What-makes-a-good-movie-asked-Bayes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;how-to-make-a-good-movie&#34;&gt;How to make a good movie&lt;/h1&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;load-packages&#34;&gt;Load packages&lt;/h3&gt;
&lt;p&gt;Let us load the 4 packages needed for this analysis. ggplot2 and gridExtra are required for the data visualizations, dplyr is needed for data manipulation and wrangling, statsr consists of all the statistical functions needed, BAS has the Bayesian functions and MASS contains the stepAIC function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;library(ggplot2)
library(gridExtra)
library(dplyr)
library(statsr)
library(BAS)
library(MASS)

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;load-data&#34;&gt;Load data&lt;/h3&gt;
&lt;p&gt;The dataset can be loaded in two ways, either by using going to File-&amp;gt;Open File and clicking on the R Workspace file to load the data, or using the load() function. Here, we use the latter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;load(&amp;quot;movies.Rdata&amp;quot;)
dim(movies)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset imported has 651 rows and 32 columns.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-1-data&#34;&gt;Part 1: Data&lt;/h2&gt;
&lt;p&gt;Rotten Tomatoes and the TomatometerT rating is the most trusted measurement of quality entertainment. As the leading online aggregator of movie and TV show reviews from professional critics, Rotten Tomatoes offers the most comprehensive guide to what&amp;rsquo;s fresh. The world famous TomatometerT rating represents the percentage of positive professional reviews for films and TV shows and is used by millions every day, to help with their entertainment viewing decisions. Rotten Tomatoes designates the best reviewed movies and TV shows as Certified Fresh. That accolade is awarded with Tomatometer ratings of 75% and higher, and a required minimum number of reviews. Weekly Rotten Tomatoes podcasts can be found on RottenTomatoes.com, iTunes, Soundcloud and Stitcher, and Rotten Tomatoes&amp;rsquo; entertainment experts make regular TV and radio appearances across the US.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data Collection&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalizability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The present data were derived from an observational study. The data set is comprised of 651 randomly sampled movies produced and released from 1970 to 2014. According to IMDb, there have 9,962 movies been release from 1972 to 2016 so that the 10% condition (9,962*0.01 = 996) is met. Since the sampling size is large enough and less than 10% of population, it can assume that the random sampling is conducted. Therefore we can conclude that the sample is indeed generalizable to the entire population.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Causality&lt;/strong&gt;
The data cannot be used to establish a causal relation between the variables of interest as there was no random assignment to the explanatory and independent variables.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-2-data-manipulation&#34;&gt;Part 2: Data manipulation&lt;/h2&gt;
&lt;p&gt;In the original dataset , not all of the required features have been provided, so we will perform some feature engineering to create the required features. For the analysis, new features as oscar_season, summer_season, mpaa_rating_R, drama and feature_film. All of them can be derived from existing variables in the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;movies &amp;lt;-movies %&amp;gt;% mutate(feature_film = as.factor(ifelse(title_type == &amp;quot;Feature Film&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(drama = as.factor(ifelse(genre == &#39;Drama&#39;, &#39;Yes&#39;, &#39;No&#39;)))
movies &amp;lt;- movies %&amp;gt;% mutate(mpaa_rating_R=as.factor(ifelse(mpaa_rating==&amp;quot;R&amp;quot;,&amp;quot;Yes&amp;quot;,&amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(oscar_season=as.factor(ifelse(thtr_rel_month %in% c(&#39;10&#39;,&#39;11&#39;,&#39;12&#39;),&amp;quot;Yes&amp;quot;,&amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(summer_season=as.factor(ifelse(thtr_rel_month %in% c(&#39;6&#39;,&#39;7&#39;,&#39;8&#39;),&amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-3-exploratory-data-analysis&#34;&gt;Part 3: Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;Firstly, let us see the analyze the audience_score variable:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Audience Score:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$audience_score)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The lowest rating is 11 (Battlefield Earth) and the highest rating is 97.00 (The Godfather Part 2).
The median score is 65.00, with a mean of 62.36.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=movies,aes(x=audience_score)) + geom_histogram(binwidth=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a binwidth=5, we get a readable display. It is evident that this data is left-skewed, with more values on the right side of the mean than the left.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=movies,aes(x=audience_score)) + geom_density()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot clearly shows the left skew of the audience_score variable.&lt;/p&gt;
&lt;p&gt;Now, using the variables created in the previous section, plots and summary statistics make it easier to understand the data we have created.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Feature Film:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it is a Feature Film and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$feature_film)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows that there is mainly a huge majority of feature films. Actually, (591*100)/651= 90.738 % of the data are feature films.&lt;/p&gt;
&lt;p&gt;Plotting this data against audience_score and IMDB rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g1=ggplot(data=movies,aes(x=feature_film,y=audience_score,fill=feature_film)) +geom_boxplot()
g2=ggplot(data=movies,aes(x=feature_film,y=imdb_rating,fill=feature_film)) + geom_boxplot()
grid.arrange(g1,g2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there are fewer feature films, the distribution shows that feature films generally have a lower score than non-feature films. But this could also be attributed to the fewer number of feature films.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Drama:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it is a Drama movie and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;p&gt;Let us check the summary statistics for the drama variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$drama)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the number of drama and non-drama movies are close in count, but there are slightly more non-drama movies.&lt;/p&gt;
&lt;p&gt;Plotting this variable against audience_score and IMDB rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g3=ggplot(data=movies,aes(x=drama,y=audience_score,fill=drama)) +geom_boxplot()
g4=ggplot(data=movies,aes(x=drama,y=imdb_rating,fill=drama)) + geom_boxplot()
grid.arrange(g3,g4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There isn&amp;rsquo;t a huge difference but the non-drama movies have slightly lower media score compared to the drama movies. The non-drama movies are also slightly more distributed, but not by a whole lot.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MPAA Rating:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if the movies has an R MPAA Rating and a &amp;ldquo;No&amp;rdquo; value if it is not R-rated.&lt;/p&gt;
&lt;p&gt;The summary statistics for the MPAA rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$mpaa_rating_R)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The number of R-rated movies and movies with other ratings are very close.&lt;/p&gt;
&lt;p&gt;Plotting this variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g5=ggplot(data=movies,aes(x=mpaa_rating_R,y=audience_score,fill=mpaa_rating_R)) +geom_boxplot()
g6=ggplot(data=movies,aes(x=mpaa_rating_R,y=imdb_rating,fill=mpaa_rating_R)) + geom_boxplot()
grid.arrange(g5,g6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ratings are very similar for R-rated and non R-rated movies.&lt;/p&gt;
&lt;p&gt;Their distributions are also extremely similar, with not much to split the two variables.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Oscar Season:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it was released in the Oscar season and a &amp;ldquo;No&amp;rdquo; value if it was not released in the Oscar season.&lt;/p&gt;
&lt;p&gt;The summary of the Oscar variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$oscar_season)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are fewer movies that are released in the Oscar season and only about (191*100)/651=29.3394% are released in the Oscar season.&lt;/p&gt;
&lt;p&gt;Let us plot the variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g7=ggplot(data=movies,aes(x=oscar_season,y=audience_score,fill=oscar_season)) +geom_boxplot()
g8=ggplot(data=movies,aes(x=oscar_season,y=imdb_rating,fill=oscar_season)) + geom_boxplot()
grid.arrange(g7,g8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are slightly higher scores for the movies released in the Oscar Season, but the distributions seem similar.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Summer season:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it was released in the Oscar season and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;p&gt;The summary of the summer variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$summer_season)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, most movies are not released in the summer months. Only about (164*100)/651= 25.192% of the movies are released in the summer.&lt;/p&gt;
&lt;p&gt;Plotting this variable against ratings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g9=ggplot(data=movies,aes(x=summer_season,y=audience_score,fill=summer_season)) +geom_boxplot()
g10=ggplot(data=movies,aes(x=summer_season,y=imdb_rating,fill=summer_season)) + geom_boxplot()
grid.arrange(g9,g10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plots are almost identical, with very minute differences between them, if any. The movies not released in the summer season have very slightly higher scores, but the difference looks insignificant.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-4-modeling&#34;&gt;Part 4: Modeling&lt;/h2&gt;
&lt;p&gt;The best model is not always the most complicated. Sometimes including variables that are not evidently important, can actually reduce the accuracy of predictions. In practice, the model that includes all available explanatory variables is often referred to as the full model. The full model may not be the best model, and if it isn&amp;rsquo;t, we want to identify a smaller model that is preferable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Full model:&lt;/em&gt;
audience_score ~ feature_film + drama + runtime + mpaa_rating_R + thtr_rel_year + oscar_season + summer_season + imdb_rating + imdb_num_votes + critics_score + best_pic_nom + best_pic_win + best_actor_win + best_actress_win + best_dir_win + top200_box&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayesian Model Averaging (BMA):&lt;/em&gt;
A comprehensive approach to address model uncertainty is Bayesian model averaging, which allows us to assess the robustness of results to alternative specifications by calculating posterior distributions over coefficients and models. Given the 17 features (n) there can be 2^n = 2^17 possible models. We will explore model uncertainty using posterior probabilities of models based on BIC.
We will use BIC as a way to approximate the log of the marginal likelihood. The Bayesian information criterion (BIC) runs through several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2log-likelihood + nparlog(nobs), where npar represents the number of parameters and nobs the number of observations in the fitted model.&lt;/p&gt;
&lt;p&gt;Seperating the required features into a seperate dataframe:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;features &amp;lt;- c(&#39;audience_score&#39;, &#39;feature_film&#39;, &#39;drama&#39;, &#39;runtime&#39;, &#39;mpaa_rating_R&#39;, &#39;thtr_rel_year&#39;, &#39;oscar_season&#39;, &#39;summer_season&#39;, &#39;imdb_rating&#39;, &#39;imdb_num_votes&#39;, &#39;critics_score&#39;, &#39;best_pic_nom&#39;, &#39;best_pic_win&#39;, &#39;best_actor_win&#39;, &#39;best_actress_win&#39;, &#39;best_dir_win&#39;,&#39;top200_box&#39;)
moviesmodel=movies[ , features]
summary(moviesmodel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seem to be no NA values, so we can proceed with the model selection process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian Information Criterion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we create a multiple linear regression model, with all the factors included.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audmodel=lm(audience_score~. , data=moviesmodel)
summary(audmodel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very evident that the number of factors that are not useful is very high. We can use the BIC (Bayesian Information Criterion) to eliminate the factors that are not significant in this model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audienceBIC=bas.lm(audience_score~ ., data=moviesmodel,prior=&amp;quot;BIC&amp;quot;,modelprior=uniform())
audienceBIC
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These values denote the marginal posterior inclusion probabilities. We can actually see that IMDB rating and critics score do play a big role.&lt;/p&gt;
&lt;p&gt;From this object, we can get the top 5 most probable models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(audienceBIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the most probable model contains only 3 variables, runtime, IMDB score and Critics score. The second most probable model contains 2 variables, IMDB score and Critics score.&lt;/p&gt;
&lt;p&gt;The posterior probability for the top 2 most probably models are about 27%.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Coeffecients:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can extract the coeffecients from the Bayesian model into a seperate variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audiencecoeff=coef(audienceBIC)
#95% Credible Intervals for coeffecients:
audinterval=confint(audiencecoeff)
audinterval
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us look at what the 3 most important variables mean.&lt;/p&gt;
&lt;p&gt;For every 1 point increase in the runtime, the audience score is -2.5e-02 minutes lesser. Similarly, for every 1 point increase in the IMDB rating the audience score is 1.49e+01 points more. And for the Critics score there is an audience score increase of 6.33e-02 points.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Model space:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can visualize the model space using the image() function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;image(audienceBIC,rotate=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to size constraints, all 17 variables are not shown in this picture. By opening this plot in a new window, all 17 variables are visible and it is evident that &amp;lsquo;runtime + imdb_rating + critics_score&amp;rsquo; is the best model. Also imdb rating and critics score are present in all the top models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zellner-Siow Cauchy:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using Zellner-Siow Cauchy, with an MCMC method, we can get a different model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audiencezs=bas.lm(audience_score~ .,  data=moviesmodel,prior=&amp;quot;ZS-null&amp;quot;,modelprior=uniform(),method=&amp;quot;MCMC&amp;quot;)
summary(audiencezs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the most probable model, with a posterior probability of 14% has only IMDB rating and Critics score. And the 2nd most probable model, with a posterior probability of 12% has runtime, IMDB rating and Critics score. These results are very similar to the BIC method, with only a swap in the first two models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Model space:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can visualize the models created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;image(audiencezs,rotate=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, expanding the image, we can see that IMDB rating and Critics score are the major factors. Here, runtime doesn&amp;rsquo;t seem to be playing a huge role.&lt;/p&gt;
&lt;p&gt;So we can clearly see that while BIC proposes 3 variables (runtime, imdb_rating, critics_score), the ZSC method only proposes 2 (imdb_rating, critics_score).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AIC Model Selection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Aikake information criterion (AIC) is a measure of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Hence, AIC provides a means for model selection.&lt;/p&gt;
&lt;p&gt;We can use backward elimination to find the best model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;lmaic=lm(audience_score~ ., data=moviesmodel)
audienceaic=stepAIC(lmaic,direction=&#39;backward&#39;,trace=FALSE)
audienceaic$anova
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that there are a lot more variables in this method: summer_season, top200_box, best_dir_win, best_pic_win, oscar_season, feature_film, drama, imdb_num_votes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audienceaic$coefficients
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, for example, for every 1 point increase in the IMDB rating, the Audience Score increases by 15 points, which is a lot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Final Model:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our final model, we are going to use IMDB ratings and Critics score as the two variables, with the Zellner-Siow Cauchy prior, and MCMC method, with 10^6 iterations of MCMC.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;feat=c(&amp;quot;audience_score&amp;quot;,&amp;quot;imdb_rating&amp;quot;,&amp;quot;critics_score&amp;quot;)
moviesfinal=movies[,feat]
audiencezsfin=bas.lm(audience_score~.,data=moviesfinal,prior=&amp;quot;ZS-null&amp;quot;,modelprior=uniform(),method=&amp;quot;MCMC&amp;quot;,MCMC.iteration=10^6)
summary(audiencezsfin)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that in this, the first model, which has both variables, has 89% posterior probability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model Diagnostics:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will now look at the best model created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;diagnostics(audiencezs, type = &amp;quot;model&amp;quot;, pch = 16, cex = 1.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It shows about a straight line at the intercept, which shows a converged posterior probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which = 1, pch=16)
abline(a = 0, b = 0, lwd = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seem to be few outliers, but the points are randomly scattered across the 0 line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which=2,add.smooth = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability starts to straigthen around the 800th model appx, meaning all the models after that don&amp;rsquo;t make much of a difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which=3, ask=F)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These log marginal probabilites are pretty evenly distributed, somewhat favouring 4 or 5 factors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which = 4, ask = F, col.in = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see again, that imdb_rating and critics_score matter the most.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-5-prediction&#34;&gt;Part 5: Prediction&lt;/h2&gt;
&lt;p&gt;The movie we are going to pick is Money Monster, a movie directed by Jodie Foster, starring George Clooney and Julia Roberts. (&lt;a href=&#34;https://www.rottentomatoes.com/m/money_monster/&#34;&gt;https://www.rottentomatoes.com/m/money_monster/&lt;/a&gt;) The audience score is 60%.&lt;/p&gt;
&lt;p&gt;We create the BMA object first:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;BMA=predict(audiencezsfin,estimator=&amp;quot;BMA&amp;quot;, se.fit=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a data frame with the values to be predicted:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;pred=data.frame(imdb_rating=6.5,critics_score=58)
aud=predict(audiencezsfin,newdata=pred,estimator=&amp;quot;BMA&amp;quot;,se.fit=TRUE)
aud$fit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get an estimated audience score of 62.49, which is a little higher than the actual score&lt;/p&gt;
&lt;p&gt;The 95% credible interval is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audinterval=confint(aud,parm=&amp;quot;mean&amp;quot;)
round(audinterval,3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a confidence interval close to the 60% we were looking for.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-6-conclusion&#34;&gt;Part 6: Conclusion&lt;/h2&gt;
&lt;p&gt;The predictive model presented here is used to predict the audience scores for a movie. Using Bayesian model averaging and many factors like BIC, ZSC, AIC, etc, many models can be constructed to perform better predictions.&lt;/p&gt;
&lt;p&gt;The proposed linear model shows a &amp;lsquo;fairly good&amp;rsquo; prediction rate, but it should be noted that the model is based on a very small sample. The fact is that imdb_rating has the highest posterior probability, and that basically all of the newly created features were not that useful to support a better prediction. Creating a model, which has a high predictive power is not so easy to reach. Using Bayes for better prediction is only one part of the game. It might be beneficial to gather more data or try to extend the feature engineering part, which means to creating new meaningful features from existing or gather data for new features.&lt;/p&gt;
&lt;p&gt;Perhaps in a future project, for higher accuracy, we could have included all the remaining factors as well, which was done in the project for the 3rd course of this specialization, and then eliminated them one by one. Even though such models might be prone to overfitting or underfitting, these problems can certainly be mitigated using expert opinion on which factors are actually useful.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Literacy Rate Analysis using GSS Data</title>
      <link>https://vishnubharadwaj00.github.io/project/litrates/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/litrates/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/Literacy-Rate-Analysis-Using-GSS-Dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;load-packages&#34;&gt;Load packages&lt;/h3&gt;
&lt;p&gt;For this project, we require ggplot2 and gridExtra package for the plots, dplyr for data manipulation and statsr for the statistical functions used in this course. These three packages contain a wide range of functions to be used, and should encompass all the functions we require to do this project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;library(ggplot2)
library(gridExtra)
library(dplyr)
library(statsr)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;load-data&#34;&gt;Load data&lt;/h3&gt;
&lt;p&gt;We can either go to File -&amp;gt; Open File and select our RData file or load the RData file as it is. This automatically imports our gss dataset into the workspace, as gss. The latter method is used here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;load(&amp;quot;gss.Rdata&amp;quot;)
dim(gss)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now loaded a dataset with 57061 rows and 114 columns.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-1-data&#34;&gt;Part 1: Data&lt;/h2&gt;
&lt;p&gt;According to the GSS website, the General Social Survey (GSS) has studied the growing complexity of American society. It is the only full-probability, personal-interview survey designed to monitor changes in both social characteristics and attitudes currently being conducted in the United States.&lt;/p&gt;
&lt;p&gt;The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.&lt;/p&gt;
&lt;p&gt;As to how the data is collected, the GSS sample is drawn using an area probability design that randomly selects respondents in households across the United States to take part in the survey. The respondents are from a mix of urban, suburban and rural geographic areas.
Therefore, random sampling does take place in this survey, meaning the results of the survey can to an extent, be generalized to the adult population of the United States.&lt;/p&gt;
&lt;p&gt;There are a few things which might not make it completely accurate. Firstly, the GSS is strictly voluntary, meaning even when selected, participants may choose to not attend the survey. Secondly, up until a few years ago, only English speaking participants were chosen, and now Spanish speaking participants have been added. This raises another concern about other languages, as well.&lt;/p&gt;
&lt;p&gt;Random assignment has not been used here to seperate the sampled population into further groups, which means that causality cannot be inferred from the results of this survey, since we cannot be sure that the only difference between different groups is what we are studying.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-2-research-question&#34;&gt;Part 2: Research question&lt;/h2&gt;
&lt;p&gt;With such a vast trove of data, it is possible to create many interesting and insightful research questions, each with their own meaningful conclusions.&lt;/p&gt;
&lt;p&gt;Here, I will focus on one particular area: education levels. We will analyzing 3 questions based on this area:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 1: As of 2012 (latest year of the survey), is there a disparity in the education provided to males and females?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, for the 2012 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 2012?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 2: In 1972 (first year of the survey), was there a disparity in the education provided to males and females?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, for the 1972 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 1972?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 3: Is there a difference in the education levels of 1972 and 2012?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taking gender out of the equation, is there a difference in education levels of the years 1972 and 2012? Has the situation of education improved or declined in those 50 years?&lt;/p&gt;
&lt;p&gt;This is becoming an all-important question in today&amp;rsquo;s world, with rising calls for equality between men and women. Education is an essential part of gaining knowledge, and an equal footing in education can open up equal opportunities, for men and women. Educational reforms have also been implemented over the years, to provide a higher level of education for all. But has education actually improved, overall?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-3-exploratory-data-analysis&#34;&gt;Part 3: Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Question 1:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First let us use only the section of the data that needs to be used, the 2012 data, by filtering it into another dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012 = gss %&amp;gt;%
  filter(year==&amp;quot;2012&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2012 is the latest year of the survey, providing us with the latest results to this research question.&lt;/p&gt;
&lt;p&gt;Let us see a summary of the education levels in the new subset of data we have:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(gss2012$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the mean is 13.53, the maximum is 20, and the minimum is 0, with a median of 13.
There are also 2 NA&amp;rsquo;s, out of 1974 entries, which is around 1% of the dataset, so we can remove those 2 rows to increase accuracy without affecting the accuracy of the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012= gss2012 %&amp;gt;%
  filter(educ!=&amp;quot;NA&amp;quot;)
summary(gss2012$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the NA values have been successfully removed.&lt;/p&gt;
&lt;p&gt;Let us look at the means and medians of education levels, grouped by gender:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012 %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarise(meaned=mean(educ),medec=median(educ))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of female education level is slightly higher than males, and the median is the same for both.&lt;/p&gt;
&lt;p&gt;Let us create a simple boxplot to visualise these statistics, helping us get a better understanding:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss2012 ,aes(x=factor(sex),y=educ)) + geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The boxplot does not seem to show much of a change between the two genders, with the two boxplots looking almost identical.&lt;/p&gt;
&lt;p&gt;A barplot to compare the counts of the education levels, with grouping done on gender might help us understand the data better:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss2012,aes(x=educ,fill=sex)) +geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that as the number of years of education is more than 10, females tend to get more education that men in that education level, except on one or two levels, but the difference is not a lot in most cases.&lt;/p&gt;
&lt;p&gt;Looking at the maximum and minimum, there are slightly more females than males who get 0 years of education, but at the maximum of 20 years, the difference is almost non-existent.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 2:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let us now look at the data from 1972, by subsetting it into a seperate data frame, and doing a similar EDA on it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972=gss %&amp;gt;%
  filter(year==&amp;quot;1972&amp;quot;)
summary(gss1972$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get 5 NA values out of a total of 1613 values, which is around 3% of the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972 = gss1972 %&amp;gt;%
  filter(gss1972$educ!=&amp;quot;NA&amp;quot;)
summary(gss1972$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Already there is a clear difference in the median and mean, without looking at the gender difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972 %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarise(meaned=mean(educ),medianed=median(educ))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of education levels of females was lower than the males in 1972, a sharp contrast to the results in 2012.&lt;/p&gt;
&lt;p&gt;Let us see a simple boxplot to visualise these statistics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss1972,aes(x=sex,y=educ)) + geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very evident from this that, although the male median education level and female median education level are almost the same, the average education level for males is definitely higher, and the 75th percentile level is much higher than that of females.&lt;/p&gt;
&lt;p&gt;Creating a plot to see each year of education:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss1972,aes(x=educ,fill=sex)) +geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In 1972, it is evident that beyond 14 years of education, there are more men than women. These results are much different to the 2012 results.&lt;/p&gt;
&lt;p&gt;Comparing the two datasets using scatterplots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g1=ggplot(data=gss1972,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(&amp;quot;1972&amp;quot;)
g2=ggplot(data=gss2012,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(&amp;quot;2012&amp;quot;)
grid.arrange(g1,g2,ncol=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 2 scatter plots on the left are the 1972 data, arranged according to sex, and the 2 scatter plots on the right are the 2012 data, arranged according to sex. It is clear that in 2012, there is a higher concentration at a higher education level, whereas in 1972, that is only for males.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 3:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, to analyse the difference in education levels overall, between 1972 and 2012.&lt;/p&gt;
&lt;p&gt;First let us combine them into a single dataset to make the analysis easier:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gssdata=rbind(gss1972,gss2012)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already have the datasets as well as their summary statistics, so all that is left is to plot them:&lt;/p&gt;
&lt;p&gt;Creating a simple boxplot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gssdata,aes(x=factor(year),y=educ))+geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Immediately, it is evident that there is a significant difference in the education levels between the two years. Although the median of 2012 is only slightly higher than 1972, the range and IQR of 2012 is much higher.&lt;/p&gt;
&lt;p&gt;Let us create a bar graph between the two years:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gssdata,aes(x=educ,fill=factor(year)))+geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upto about 12 years of education, 1972 had a higher proportion, but after 12 years, 2012 had the higher proportion. This means that more people are getting more education in 2012, compared to 1972.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-4-inference&#34;&gt;Part 4: Inference&lt;/h2&gt;
&lt;p&gt;First, let us check the conditions for doing a hypothesis test using the CLT method:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.Independence:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Within groups:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Random sampling was used.&lt;/p&gt;
&lt;p&gt;1972 observations is well below 10% of the population.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Between groups:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The two groups are independent of each other, as it is a representative of the population. Also the likelihood of dependence otherwise, is very small with this sample size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.Sample Size/Skew:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sample is slightly skewed, and n is well above 30.&lt;/p&gt;
&lt;p&gt;Therefore the conditions for CLT are satisfied and we can use the theoretical method.&lt;/p&gt;
&lt;p&gt;The method to be used for all 3 questions is that to be used when comparing 2 independent means, as independence has already been established. The critical score will be the t-score corresponding to the degree of freedom. The degree of freedom is the $df =min(n_1 -1 ,n_2 - 1)$. Here, $n_1$=884 and $n_2$=1088, so the degree of freedom is 883.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Question 1:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 2012.
$\mu_{male(2012)}-\mu_{female(2012)}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 2012.
$\mu_{male(2012)}-\mu_{female(2012)} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{male(2012)}$ and $\mu_{female(2012)}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{male(2012)}$ and $\bar{x}_{female(2012)}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss2012,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results:
Response variable: numerical
Explanatory variable: categorical (2 levels)
n_Male = 884, y_bar_Male = 13.5057, s_Male = 3.1721
n_Female = 1088, y_bar_Female = 13.546, s_Female = 3.0904
H0: mu_Male = mu_Female
HA: mu_Male != mu_Female
t = -0.2838, df = 883
p_value = 0.7766&lt;/p&gt;
&lt;p&gt;The high p-value of 0.7766 which is much greater than 0.05, means we &lt;strong&gt;fail to reject the null hypothesis $H_0$&lt;/strong&gt;. We might still run the risk of a type 2 error, but the big p-value offsets the effects of a larger significance level. What the p-value here indicates is the probability of observing extreme data given that the null hypothesis is true, is high.&lt;/p&gt;
&lt;p&gt;We can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss2012,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This 95% Confidence Interval means that we are 95% confident that the difference in the education levels of males and females is between -0.319 and 0.2384. The - sign shows that females getting more education that males.&lt;/p&gt;
&lt;p&gt;This confidence interval method is well in agreement with the hypothesis test method as the 0 is in the confidence interval that has been produced. The differences shown by the Confidence Interval method are also not that significant, which are the same results that can be inferred from the hypothesis test method.&lt;/p&gt;
&lt;p&gt;In conclusion, the results of both the confidence interval method and the hypothesis test method indicate that &lt;strong&gt;there is an insignificant difference in the education levels of males and females in 2012.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 2:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 1972.
$\mu_{male(1972)}-\mu_{female(1972)}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 1972.
$\mu_{male(1972)}-\mu_{female(1972)} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{male(1972)}$ and $\mu_{female(1972)}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{male(1972)}$ and $\bar{x}_{female(1972)}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss1972,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this inference, we get a p-value of 0.01, which is much lower than our significance level of 0.05, which means we &lt;strong&gt;reject the $H_0$ hypothesis&lt;/strong&gt;. Again, we run the risk of a Type-1 error, but even a smaller significance level will not give a different result.&lt;/p&gt;
&lt;p&gt;We can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss1972,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results show that males get about 0.07 to 0.75 years more education then females. Although this does not seem like a lot, since it is above the significance level of 0.05, it is a significant difference, as the difference when extrapolated to the entire population, becomes much bigger.&lt;/p&gt;
&lt;p&gt;The conclusion from this test is that &lt;strong&gt;there was a significant difference in the education levels of males and females in 1972&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 3:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 2012.
$\mu_{1972}-\mu_{2012}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 2012.
$\mu_{1972}-\mu_{2012} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{1972}$ and $\mu_{2012}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{1972}$ and $\bar{x}_{2012}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=factor(year),data=gssdata,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value obtained is extremely small, &amp;lt;0.0001, which is obviously lesser than the significance level of 0.05, hence we &lt;strong&gt;reject the $H_0$ hypothesis&lt;/strong&gt;. The risk of a type 1 error is even smaller here, as it is such a small p-value.&lt;/p&gt;
&lt;p&gt;A confidence interval can also be created similar to the above questions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=factor(year),data=gssdata,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confidence interval indicates that there is a 95% chance that on average, people in 2012 had an 1.9825 to 2.4191 more years of education than people in 2012. This is in agreement with the hypothesis test done earlier.&lt;/p&gt;
&lt;p&gt;Therefore, it can be concluded that &lt;strong&gt;there is a significant difference in the education levels of 1972 and 2012&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-5-conclusion&#34;&gt;Part 5: Conclusion&lt;/h2&gt;
&lt;p&gt;It can be concluded that:
(1) There is an insignificant difference in the education levels of males and females in the year 2012, with a 95% confidence interval of (-0.319,0.2384).&lt;/p&gt;
&lt;p&gt;(2) There is a significant difference in the education levels of males and females in the year 1972, with a 95% confidence interval of (0.0783,0.7542).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
