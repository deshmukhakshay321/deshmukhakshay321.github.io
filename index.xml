<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Akshay Deshmukh</title>
    <link>https://deshmukhakshay321.github.io/cloneakshay123/</link>
      <atom:link href="https://deshmukhakshay321.github.io/cloneakshay123/index.xml" rel="self" type="application/rss+xml" />
    <description>Akshay Deshmukh</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://deshmukhakshay321.github.io/cloneakshay123/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Akshay Deshmukh</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Satellite  Classifier</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/satelliteimage/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/satelliteimage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How NOT to Build a Data Science Project</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/post/datascienceprojectmistakes/</link>
      <pubDate>Tue, 28 Jul 2020 13:49:43 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/post/datascienceprojectmistakes/</guid>
      <description>&lt;p&gt;(Originally published on 
&lt;a href=&#34;https://towardsdatascience.com/how-not-to-build-a-data-science-project-baa494d98da4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Data Science&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;I knew I was doing something wrong.&lt;/p&gt;
&lt;p&gt;I had built up a repertoire of interesting, practical projects. I had a couple of online courses to showcase what I had learnt. I even built a portfolio website to showcase all my projects and articles (which you can access 
&lt;a href=&#34;https://vishnubharadwaj00.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But, I still felt a gaping hole in the knowledge I had. A huge, gaping, chasm-sized hole. I felt like something major was missing from the equation.&lt;/p&gt;
&lt;p&gt;That’s when I came across some absolutely amazing playlists on YouTube on building an End-to-End Data Science project from scratch. Some great examples are 
&lt;a href=&#34;https://www.youtube.com/watch?v=MpF9HENQjDo&amp;amp;list=PL2zq7klxX5ASFejJj80ob9ZAnBHdz5O1t&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt; by Ken Jee, 
&lt;a href=&#34;https://www.youtube.com/watch?v=C_lIenSJb3c&amp;amp;list=PL6vjgQ2-qJFeMrZ0sBjmnUBZNX9xaqKuM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt; by Daniel Bourke and 
&lt;a href=&#34;https://www.youtube.com/watch?v=ZZ4B0QUHuNc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt; by Data Professor. (Shout out to all of them for some absolutely brilliant content!)&lt;/p&gt;
&lt;p&gt;I realized I needed to get started on an End-to-End Data Science project, stat (pun very much intended). Getting hands-on and diving right in, though uncomfortable at first, is the best way to learn something new.&lt;/p&gt;
&lt;p&gt;And so I dove straight in.&lt;/p&gt;
&lt;p&gt;Until I got stuck. Again.&lt;/p&gt;
&lt;p&gt;And again.&lt;/p&gt;
&lt;p&gt;And again.&lt;/p&gt;
&lt;p&gt;But finally, after hours of toiling, a couple dozen Stack Overflow searches (thank the heavens for Stack Overflow) and a lot of banging my head against the wall out of frustration after running into a wall of errors (this didn’t help as much), I was done.&lt;/p&gt;
&lt;p&gt;You can check out my final project 
&lt;a href=&#34;http://ds-salary-predictor.herokuapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. It’s a Data Science Salary Predictor (inspired and guided by Ken Jee’s 
&lt;a href=&#34;https://www.youtube.com/watch?v=MpF9HENQjDo&amp;amp;list=PL2zq7klxX5ASFejJj80ob9ZAnBHdz5O1t&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;playlist&lt;/a&gt;). I’m also working on a couple more of my own, this time with no reference or guidance, to really push myself beyond the perceived limits of my understanding.&lt;/p&gt;
&lt;p&gt;During the process of building these projects, I learnt a whole lot, not only from a Data Science point of view, but also about how to tackle the problem of structuring such a project in the first place; by making a lot of mistakes and learning from them, again and again.&lt;/p&gt;
&lt;p&gt;Here are some of the mistakes I had made before, and what you can do to avoid these mistakes in your next project:&lt;/p&gt;
&lt;h3 id=&#34;mistake-1-not-making-my-projects-end-to-end&#34;&gt;Mistake #1: Not making my projects End-to-End.&lt;/h3&gt;
&lt;p&gt;Typically, most of the time spent on real-life applications of Data Science is on Data Cleaning and Preparation. In fact, it’s widely been acknowledged that &lt;strong&gt;about 80% of the time&lt;/strong&gt; spent is on cleaning the data and transforming into a form suitable for further analysis.&lt;/p&gt;
&lt;p&gt;As annoying as it may be, Data Cleaning is an essential step in the Data Science life cycle. Not all the datasets you might work with in a real job or internship will be as cleaned and ready to use as a Kaggle dataset.&lt;/p&gt;
&lt;p&gt;It might be very messy, and it might possibly be your job to clean it up and make it ready-to-use.&lt;/p&gt;
&lt;p&gt;And an equally important part of the project is productionizing and deploying projects. There’s a quote I’m seeing more and more of these days:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Your project shouldn’t end its life in a Jupyter Notebook.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Building projects and products that can be &lt;strong&gt;accessed by other people&lt;/strong&gt; is one of the main uses of Data Science.&lt;/p&gt;
&lt;p&gt;A bunch of code in a Jupyter Notebook is typically of no practical use, but building a simple Web Application using a powerful and easy-to-use tool like 
&lt;a href=&#34;https://www.streamlit.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit&lt;/a&gt; or a slightly more complex tool like 
&lt;a href=&#34;https://flask.palletsprojects.com/en/1.1.x/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flask&lt;/a&gt;, to showcase what you built, just that little bit of extra effort, makes it a lot easier for people to see what you’ve built.&lt;/p&gt;
&lt;p&gt;It’s very important to start with strong foundations and end with a tangible, practical product.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pro Tip: Get your own data (web scrape if you have to). Clean it, preprocess it, do some feature engineering. And don’t forget to productionize your project with Streamlit or Flask.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;mistake-2-not-spending-time-on-the-things-that-mattermost&#34;&gt;Mistake #2: Not spending time on the things that matter most.&lt;/h3&gt;
&lt;p&gt;Pareto’s principle (also known as the &lt;strong&gt;80–20 rule&lt;/strong&gt;) states that, for many events, around 80% of the effects come from 20% of the causes. Similarly, in a project, around 80% of the value comes from just 20% of the things you do. Equivalently, what this also means is that around 80% of the things you might do don’t really add a whole lot of value (around 20% only).&lt;/p&gt;
&lt;p&gt;I used to spend a lot of my time thinking and analyzing about the different options or different paths I could take with my project.&lt;/p&gt;
&lt;p&gt;I shouldn’t have.&lt;/p&gt;
&lt;p&gt;I should have just gotten started with something. I could have dealt with the after-effect of having chosen the wrong option, later.&lt;/p&gt;
&lt;p&gt;A simple example of where I’ve wasted a lot of time: &lt;strong&gt;Cloud providers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you’re with me so far, productionizing a project using Streamlit or Flask is very important. But also, deploying that project so that it can be accessed by everyone is also equally important.&lt;/p&gt;
&lt;p&gt;And there are a lot of options out there to do this.&lt;/p&gt;
&lt;p&gt;I started obsessively analyzing and researching all the different options to help deploy my project. Looking up all the differences between AWS and GCP, the pros and cons for each, what instances they provide, what costs I might have to incur, and so on.&lt;/p&gt;
&lt;p&gt;Until I realized the mistake I was making. I just had to deploy it, period. It didn’t matter where.&lt;/p&gt;
&lt;p&gt;So, I deployed it on 
&lt;a href=&#34;https://www.heroku.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Heroku&lt;/a&gt; for free. (They allow one free deployment, which I graciously used).&lt;/p&gt;
&lt;p&gt;Next time, I’ll deploy it on whatever I find to be the easiest. No second thoughts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pro Tip: Stop thinking, start doing. Use the 80–20 rule to your advantage.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;mistake-3-not-planning-what-the-end-result-will-looklike&#34;&gt;Mistake #3: Not planning what the end result will look like.&lt;/h3&gt;
&lt;p&gt;If I’ve managed to convince you by now that productionization and deployment are invaluable steps in your project life cycle and about the power of the 80–20 rule, then another equally important and frequently overlooked step is planning what the final result is going to look like.&lt;/p&gt;
&lt;p&gt;Okay, let’s assume you’ve agreed with my logic so far and you’ve decided you want to build a web application showcasing your project.&lt;/p&gt;
&lt;p&gt;But what is that application going to do?&lt;/p&gt;
&lt;p&gt;Ask yourself questions about the UI, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the &lt;strong&gt;inputs&lt;/strong&gt; it takes from the user?&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;what form&lt;/strong&gt; are you going to take in the inputs?&lt;/li&gt;
&lt;li&gt;What’s the &lt;strong&gt;end result&lt;/strong&gt; your model is gonna spew out?&lt;/li&gt;
&lt;li&gt;What form is the &lt;strong&gt;output&lt;/strong&gt; going to be in?&lt;/li&gt;
&lt;li&gt;Are there any &lt;strong&gt;additional features&lt;/strong&gt; that I could add that will make the user experience better?&lt;/li&gt;
&lt;li&gt;Are there some things I should &lt;strong&gt;avoid&lt;/strong&gt; at all costs?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During my Data Science Salary Predictor project, I made the grave mistake of not thinking about the final interface and its layout. When I finally got to that stage, I saw that the user’s categorical inputs would make no sense to my model, so I had to go back and change certain categorical variables into ordinal variables, wasting some valuable time in the process.&lt;/p&gt;
&lt;p&gt;Answering these questions at the beginning will save a whole lot of time later on, and it will avoid you having to go back and forth between the different steps &lt;strong&gt;unnecessarily&lt;/strong&gt;. I use the word ‘unnecessarily’ here for a reason, which brings me to Mistake #4.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pro Tip: Have a rough idea of the end-result when you start, in whatever form it may be. Sketch it out somewhere if you have to.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;mistake-4-not-circling-back-to-previousstages&#34;&gt;Mistake #4: Not circling back to previous stages.&lt;/h3&gt;





  











&lt;figure id=&#34;figure-the-data-science-life-cycle-credits-chanin-nantasenamatdata-professor&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/0__SgzHN6H7R46HAtPa.jpeg&#34; data-caption=&#34;The Data Science Life Cycle (Credits: Chanin Nantasenamat/Data Professor)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/0__SgzHN6H7R46HAtPa.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Data Science Life Cycle (Credits: Chanin Nantasenamat/Data Professor)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Typically, any project can be categorized into these 5 major stages: Data Collection, Data Cleaning, Exploratory Data Analysis, Model Building and Model Deployment.&lt;/p&gt;
&lt;p&gt;But a major mistake I made was thinking that the process was a linear series of steps, when in fact, it’s an iterative and often, cyclical process.&lt;/p&gt;
&lt;p&gt;As an example, during my Data Science Salary Predictor project, when I was at the Model Building stage, I thought of a couple more interesting features that I could have added, to give as inputs to my model.&lt;/p&gt;
&lt;p&gt;So what did I do?&lt;/p&gt;
&lt;p&gt;I went all the way back to the Data Cleaning and Preprocessing step, had to do some additional feature engineering, and had to perform some Exploratory Data Analysis on the new data, and build another model with the additional features.&lt;/p&gt;
&lt;p&gt;Sometimes, these circling backs might be completely unnecessary, such as in Mistake #3.&lt;/p&gt;
&lt;p&gt;But sometimes, you might have a new insight after having worked with the data for a while and in that case, circling back is the right thing to do.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pro Tip: Plan as much as necessary before hand, but circle back if needed. That little effort might make your project a lot better, for all you know.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;mistake-5-not-introspecting-and-evaluating-after-the-project-isdeployed&#34;&gt;Mistake #5: Not introspecting and evaluating after the project is deployed&lt;/h3&gt;
&lt;p&gt;A step I neglected completely, and what might actually be the most important step of them all, is evaluating your project after you’re done.&lt;/p&gt;
&lt;p&gt;Seeing what went right, what went wrong, and how to improve next time.&lt;/p&gt;
&lt;p&gt;I learnt this about this very important step from Daniel Bourke’s 
&lt;a href=&#34;https://www.youtube.com/watch?v=aBkUE5OJ8u8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt; after the Airbnb project he had embarked upon.&lt;/p&gt;
&lt;p&gt;Take some time, analyze all the steps of your project, introspect, learn from them and move on.&lt;/p&gt;
&lt;p&gt;This article wouldn’t exist without this step. The mistakes listed here are those that I learnt from my projects, and I’m almost sure that anyone embarking on a project of their own will commit certain mistakes along the way. Learn from them, make sure you don’t repeat them next time, move on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pro Tip: Go back and see what went right and wrong, and learn from them.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“It is okay to mistakes, as long as you learn from them.” — Anonymous&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And most importantly: don’t forget to have fun along the way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Feel free to reach out to me on&lt;/em&gt; 
&lt;a href=&#34;https://www.linkedin.com/in/vishnu-bharadwaj-796877175/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;LinkedIn&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, check out my&lt;/em&gt; 
&lt;a href=&#34;https://github.com/vishnubharadwaj00&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;GitHub&lt;/em&gt;&lt;/a&gt; &lt;em&gt;for the projects I’ve done, or my personal&lt;/em&gt; 
&lt;a href=&#34;https://vishnubharadwaj00.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;website&lt;/em&gt;&lt;/a&gt; &lt;em&gt;for all my work.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Cal Newport Saved my CS Career</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/post/calnewport/</link>
      <pubDate>Thu, 16 Jul 2020 12:35:17 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/post/calnewport/</guid>
      <description>&lt;p&gt;(Originally published on 
&lt;a href=&#34;https://blog.usejournal.com/how-cal-newport-saved-my-cs-career-141028f4cd34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Noteworthy&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;I was struggling.&lt;/p&gt;
&lt;p&gt;I was halfway through the second year of my undergraduate CS degree. And I wanted to quit.&lt;/p&gt;
&lt;p&gt;There were a lot of personal reasons I wanted to quit, but one of the main reasons was that I felt stuck and unable to attain my full potential. Growing up, I had always been told, “Hey, you’re a smart kid. You have a lot of potential.” I had always had a loosely structured work ethic; I worked twice as hard as others to get the same result as them. But this never really sat right with me.&lt;/p&gt;
&lt;p&gt;During my degree, I felt like a hamster in a wheel, running in place with no real destination. Always working hard, only to get mediocre results. I was getting really tired, really fast, and I had the “rational” thought of “Hey, why don’t I just quit?”&lt;/p&gt;
&lt;p&gt;Luckily for me, somewhere around the same time I was thinking of quitting, I went to a bookstore and came across a book. And immediately, the title of this book really struck a chord with me:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So Good They Can’t Ignore You.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I thought to myself, “Hey, that’s what I want to be: really good at whatever it is I do.” So I picked it up and the book right next to it, “&lt;strong&gt;Deep Work&lt;/strong&gt;”, also by Cal Newport, and it’s safe to say my life’s never been the same since. (Read till the end if you’d like to know how I’m doing now.)&lt;/p&gt;
&lt;p&gt;I have since purchased two of his other books “&lt;strong&gt;How to be a Straight-A Student&lt;/strong&gt;” and “&lt;strong&gt;Digital Minimalism&lt;/strong&gt;”, which also had an equally significant impact on me.&lt;/p&gt;
&lt;p&gt;Here are some of the things I’ve learnt from these books and how they might help you as well:&lt;/p&gt;
&lt;h3 id=&#34;work-accomplished--time-spent-x-intensity-offocus&#34;&gt;Work accomplished = Time Spent x Intensity of Focus&lt;/h3&gt;
&lt;h4 id=&#34;from-how-to-be-a-straight-a-student&#34;&gt;(From How to be a Straight-A Student)&lt;/h4&gt;
&lt;p&gt;This was one of the foundations for the book “How to be a Straight-A Student”, outlining the unconventional strategies that college students used to study a lot less but still ace their college life. The key part of this formula is &lt;strong&gt;Intensity of Focus&lt;/strong&gt;, which basically refers to how much concentration and focus you’re putting into a particular task.&lt;/p&gt;
&lt;p&gt;Haphazardly flitting across different tasks or multitasking are all ways to diminish focus, and is basically the equivalent of completing your assignment with both your hands tied behind your back. Instead, putting in many short intervals of high focus to complete a task is much easier and takes a lot less time in the long run.&lt;/p&gt;
&lt;p&gt;Ever since reading about this, I have used this technique for every test or exam I’ve prepared for. I adopted the vastly popular 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pomodoro_Technique&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pomodoro technique&lt;/a&gt;, where I study for 25 minutes with high concentration levels and take a 5 minute break after that to recharge. 4–5 of these sessions and I’m done for the day, having gotten a lot more done than if I had just continuously studied for hours together.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: Use the Pomodoro Technique. Now.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;active-recall-is-the-only-studying-thatmatters&#34;&gt;Active Recall is the only studying that matters.&lt;/h3&gt;
&lt;h4 id=&#34;from-how-to-be-a-straight-a-student-1&#34;&gt;(From How to be a Straight-A Student)&lt;/h4&gt;
&lt;p&gt;Active recall, or Quiz-and-Recall as Cal calls it, is the polar opposite of rote memorization or flipping through a textbook a couple of times. Active recall consists of &lt;strong&gt;understanding the concept at hand and then explaining the concept out loud, without any help or referral to your textbook or notes&lt;/strong&gt;. It’s a variation of the 
&lt;a href=&#34;https://collegeinfogeek.com/feynman-technique/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feynman technique&lt;/a&gt;, named after legendary physicist Richard Feynman.&lt;/p&gt;
&lt;p&gt;It’s hard. It’s uncomfortable. The act of trying to force information out of your brain is painful. But over time, you get used to it. And guess what? If you can explain it well without any help, you’re done. That’s it. It is brutally time-effective.&lt;/p&gt;
&lt;p&gt;During the 25-minute Pomodoro sessions, I try and do as much active recall as possible. Although initially uncomfortable, I was able to save a ton of time with this method. In fact, thanks mainly to this method, I was typically done studying the day before an exam or test, and so I’ve never had to pull an all-nighter ever. I’d usually spend the evening and night before relaxing by watching TV or reading.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: Explain things out loud when studying. Nothing else matters.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;follow-your-passion-is-terribleadvice&#34;&gt;“Follow your passion” is terrible advice.&lt;/h3&gt;
&lt;h4 id=&#34;from-so-good-they-cant-ignoreyou&#34;&gt;(From So Good They Can’t Ignore You)&lt;/h4&gt;
&lt;p&gt;It’s very likely that you’ve heard the phrase “Follow your passion”. But in the book “So Good They Can’t Ignore You”, Cal makes the argument that skills matter way more than passion in finding a career that is meaningful and satisfying. Putting in the time to build important, valuable skills should be the mainstream way to build a career that you end up loving.&lt;/p&gt;
&lt;p&gt;This particular piece of writing really resonated with me. One of the reasons I wanted to quit my CS degree was because I didn’t feel “passionate” about the subjects I was learning.&lt;/p&gt;
&lt;p&gt;I was almost certain that there was a different career trajectory out there for me and this was not it. But after I read this, I realized that if you don’t have a pre-existing passion (I didn’t), then &lt;strong&gt;passion is simply a byproduct of building important, necessary skills.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It’s not the destination, it’s the journey.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: Don’t worry about passion. Passion doesn’t matter. Only skills do.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-idea-of-careercapital&#34;&gt;The Idea of Career Capital.&lt;/h3&gt;
&lt;h4 id=&#34;from-so-good-they-cant-ignoreyou-1&#34;&gt;(From So Good They Can’t Ignore You)&lt;/h4&gt;
&lt;p&gt;According to Cal, there are typically 3 traits that make a career compelling and enriching: Creativity, Impact and Control. Being stimulated creatively, having a significant impact and having a degree of control over your own career are some of the things that people ideally want in a career.&lt;/p&gt;
&lt;p&gt;And to gain these things, Cal introduces the concept of &lt;strong&gt;Career Capital, or rare and valuable skills, that you can exchange for these traits&lt;/strong&gt;. Like a commodity like oil or gold, these skills can be leveraged to build a sustainable career. Without the necessary skills, asking for these factors in a career is close to impossible.&lt;/p&gt;
&lt;p&gt;How do you gain these skills? 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Practice_%28learning_method%29#Deliberate_practice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deliberate practice&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Deliberate practice consists of a well-defined stretch goal, a little beyond your current capabilities, and full concentration and effort practicing that skill repeatedly to attain that goal, while getting constant feedback. Active Recall, as discussed above, is one important form of deliberate practice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: Gain rare and valuable skills using Deliberate Practice.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;embracing-boredom&#34;&gt;Embracing Boredom.&lt;/h3&gt;
&lt;h4 id=&#34;from-deepwork&#34;&gt;(From Deep Work)&lt;/h4&gt;
&lt;p&gt;Deep Work was arguably one of Cal’s more popular books, and created the largest ripple effect, making the discussion and criticism of current technological use more mainstream. According to Cal, &lt;strong&gt;Deep Work is the ability to perform activities in a state of distraction-free concentration that push your cognitive capabilities to their limit.&lt;/strong&gt; Deep Work is becoming increasingly rare, and yet is increasingly valuable, a winning combination for those who can actually practice Deep Work.&lt;/p&gt;
&lt;p&gt;In one part of the book, he talks about actually embracing boredom and making that a more significant part of our lives. According to him, Deep Work is not an overnight decision, but rather like a muscle that has to be trained. So if Deep Work is equivalent to working out at the gym, boredom would be adopting a healthy diet and staying away from junk food. They need to coexist to have any kind of effect.&lt;/p&gt;
&lt;p&gt;Only by removing the constant distractions around us can we be preparing our brains for the type of hard thinking that Deep Work needs. As Cal says, &lt;strong&gt;“Take breaks from focus, not distraction.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: Be bored, for a fixed period everyday. Let your mind wander, without any distractions. Be intrigued by the thoughts you come across.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;schedule-every-minute-of-yourday&#34;&gt;Schedule Every Minute of your Day.&lt;/h3&gt;
&lt;h4 id=&#34;from-deepwork-1&#34;&gt;(From Deep Work)&lt;/h4&gt;
&lt;p&gt;Planning is a crucial part of productivity. Without proper planning, it could seem like you’re ‘busy’, but without any major impact. We sometimes run on autopilot, not really worrying about what we do with our time during the day. But over a longer period of time, this turns out to have a really negative impact.&lt;/p&gt;
&lt;p&gt;To sidestep this major problem, Cal suggests 
&lt;a href=&#34;https://www.calnewport.com/blog/2013/12/21/deep-habits-the-importance-of-planning-every-minute-of-your-work-day/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;time-blocking,&lt;/a&gt; a planning method in which you plan the entire day using blocks or chunks (Time chunking doesn’t have the same ring to it though). It’s a simple method but it is highly effective.&lt;/p&gt;
&lt;p&gt;I adopted this and struggled with it initially, because I always underestimated the time for a given task, and had to switch up my entire schedule sometimes. On some days, interruptions I hadn’t planned for might pop up, which requires some planning around.&lt;/p&gt;
&lt;p&gt;But again, over time, I started to get the hang of this technique, and it’s allowed me to really make the most of my days.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: Plan your day by Time Blocking.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;quit-socialmedia&#34;&gt;Quit Social Media.&lt;/h3&gt;
&lt;h4 id=&#34;from-deep-workdigital-minimalism&#34;&gt;(From Deep Work/Digital Minimalism)&lt;/h4&gt;
&lt;p&gt;Probably the most controversial statement of Cal’s has been ‘Quit Social Media’. He is a huge critic of social media and their highly engineered manner to grab as much of our attention as possible. Having never had a social media account, Cal had a very unique insight into how these apps and devices were wreaking havoc on our brains. He’s even given a 
&lt;a href=&#34;https://www.youtube.com/watch?v=3E7hkPZ-HTk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TED Talk&lt;/a&gt; on this topic.&lt;/p&gt;
&lt;p&gt;This piece of advice is the one I have prioritized over any other, simply because I was also getting bogged down by social media. Seeing everyone else’s happy lives made me question the happiness of my own, when the truth is that that was not the full picture that I was seeing.&lt;/p&gt;
&lt;p&gt;So, &lt;strong&gt;I quit&lt;/strong&gt;. I quit everything. The first week or two were really difficult. I kept picking up my phone, only to realize I’d uninstalled all the apps. Keeping in touch with friends became a little bit more difficult. I felt like I was missing out on something.&lt;/p&gt;
&lt;p&gt;But after the first week or two, &lt;strong&gt;I felt free&lt;/strong&gt;. I had so much more time on my hands now, to do the things that were really valuable to me. As a millennial not on social media, being the only one among my peers not in this hyper-connected state, I had a unique edge over my peers. More importantly, I started enjoying the little things in life. The things people usually missed while looking into their screens.&lt;/p&gt;
&lt;p&gt;Now, it’s been more than 2 years since I quit. I have only a select few technologies that I use because they’re valuable to me such as Whatsapp for my college groups and LinkedIn for professional purposes like sharing my projects. &lt;strong&gt;Now, I’m really starting to wonder how I ever was on social media.&lt;/strong&gt; There have been times where I reinstalled some of the apps, only to get disillusioned after a few hours, and uninstalling them again. I’m happier without it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: Quit social media, either for a fixed period of time, or indefinitely. Push through the initial pain, and see remarkable changes immediately.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;so-hows-my-lifenow&#34;&gt;So, how’s my life now?&lt;/h3&gt;
&lt;p&gt;A million times better. I decided to push through and continue with CS, and graduated last month, with a pretty good GPA. I was also accepted into a highly ranked university for my graduate studies (hint: Go Boilermakers!). I’ve picked up quite and mastered quite a few important skills in Data Science, one of my main interests (and dare I say, passion?)&lt;/p&gt;
&lt;p&gt;I’m happier, way more productive, not on social media and enjoying the important things in life.&lt;/p&gt;
&lt;p&gt;Thank you, Cal Newport, for saving my career before it even started.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Check out Cal Newport’s blog,&lt;/em&gt; 
&lt;a href=&#34;https://www.calnewport.com/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Study Hacks&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, or any of his books, for some great content. Feel free to reach out to me on&lt;/em&gt; 
&lt;a href=&#34;https://www.linkedin.com/in/vishnu-bharadwaj-796877175/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;LinkedIn,&lt;/em&gt;&lt;/a&gt; &lt;em&gt;or check out some of the Data Science projects I’ve done on my&lt;/em&gt; 
&lt;a href=&#34;https://github.com/vishnubharadwaj00&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;GitHub&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science Salary Predictor</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/datasciencesalarypred/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/datasciencesalarypred/</guid>
      <description></description>
    </item>
    
    <item>
      <title>10 Unique Visualizations of the NBA</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/post/nbaviz/</link>
      <pubDate>Sat, 06 Jun 2020 09:07:47 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/post/nbaviz/</guid>
      <description>&lt;p&gt;(Originally published on 
&lt;a href=&#34;https://towardsdatascience.com/10-unique-visualizations-of-the-nba-b981cfdb78bf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Data Science&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;i-miss-thenba&#34;&gt;I miss the NBA.&lt;/h3&gt;
&lt;p&gt;The suspension of the NBA due to the COVID-19 pandemic is just one of the small disruptions in the world today. But like many other sports enthusiasts, I miss the thrill of watching live sports.&lt;/p&gt;
&lt;p&gt;So I dealt with it, the same way any Data-loving, basketball enthusiast would: &lt;strong&gt;I dove into the stats, and built some graphs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I used &lt;strong&gt;Tableau&lt;/strong&gt; (something I only recently learnt how to use, but absolutely love already) to build these plots, and a 
&lt;a href=&#34;https://www.kaggle.com/justinas/nba-players-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset&lt;/a&gt; I found on Kaggle, which contains information about all players who’ve played in the NBA from the 1996–97 season to 2019–20 season.&lt;/p&gt;
&lt;p&gt;Before I even started, without creating a single graph, I immediately stumbled upon my first interesting insight:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;There have been only 2235 unique players in the NBA since 1996.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That’s an insanely low number, for 24 years of basketball. And that’s why we keep hearing experts say that getting into the NBA is like “winning the lottery”.&lt;/p&gt;
&lt;h3 id=&#34;before-thenba&#34;&gt;Before the NBA&lt;/h3&gt;
&lt;p&gt;College basketball is the typical recruitment point for the NBA, with most players turning pro after a successful college basketball career. Tournaments like NCAA March Madness are extremely popular, and players who shine in these tournaments catch the eye of NBA recruiters.&lt;/p&gt;
&lt;h4 id=&#34;1-what-are-the-most-successful-colleges-in-terms-of-getting-players-to-thenba&#34;&gt;1. What are the most successful colleges, in terms of getting players to the NBA?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-number-of-players-from-each-college-whove-played-in-thenba&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__jbDeKg469x6AO8FAjmE9CQ.png&#34; data-caption=&#34;Number of Players from each College who’ve played in the NBA&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__jbDeKg469x6AO8FAjmE9CQ.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Number of Players from each College who’ve played in the NBA
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;These are the top 20 colleges in the US in terms of players who have gone on to play in the NBA.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The top 3 colleges are &lt;strong&gt;The University of Kentucky, Duke University and the UCLA&lt;/strong&gt; are the most successful colleges with 168 players going on to play in the NBA.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Players are typically drafted from colleges in different draft rounds. Being a first-round draft pick, means you’re the best of the best.&lt;/p&gt;
&lt;h4 id=&#34;2-but-does-the-draft-round-affect-how-many-games-the-player-ends-up-playing-perseason&#34;&gt;&lt;strong&gt;2. But does the draft round affect how many games the player ends up playing per season?&lt;/strong&gt;&lt;/h4&gt;





  











&lt;figure id=&#34;figure-draft-round-vs-average-games-played-per-season&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__pKRHxrVqgwC5g6ulRoNEvw.png&#34; data-caption=&#34;Draft Round vs Average Games Played per Season&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__pKRHxrVqgwC5g6ulRoNEvw.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Draft Round vs Average Games Played per Season
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Not necessarily.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1st round picks only play the 3rd most number of games, while 7th and 3rd round picks play more games on average per season.&lt;/p&gt;
&lt;p&gt;But in terms of average points per season (the colors in the plot), the 1st round picks contribute the most points. Could this be because 1st round picks are rested a bit more to score more efficiently in crucial and important games for the team?&lt;/p&gt;
&lt;p&gt;Another interesting insight, &lt;strong&gt;being undrafted helps you&lt;/strong&gt; play more games than being a 4th, 6th or 8th round pick. Surprising.&lt;/p&gt;
&lt;h3 id=&#34;the-biggest-of-thebiggest&#34;&gt;The Biggest of the Biggest&lt;/h3&gt;
&lt;p&gt;So the player has been drafted. Chances are he’s really tall and really strong. The NBA’s known for having the tallest and strongest players, real athletic specimens.&lt;/p&gt;
&lt;p&gt;In fact, the average height and weight of the players since 1996 are 200.8 cm (6&#39;5&amp;rsquo;’) and 100.6 kg (221.7 lbs).&lt;/p&gt;
&lt;h4 id=&#34;3-who-were-the-biggest-players-out-of-all-ofthem&#34;&gt;3. Who were the biggest players out of all of them?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-nbas-biggest-players-in-terms-of-height-cm-and-weight-kg&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__5L9t9imjrLqwCuwJXdWHvw.png&#34; data-caption=&#34;NBA’s biggest players, in terms of Height (cm) and Weight (kg)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__5L9t9imjrLqwCuwJXdWHvw.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    NBA’s biggest players, in terms of Height (cm) and Weight (kg)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As most of the players in the NBA do come from the USA, it’s definitely no surprise that some of the biggest players are from the USA as well.&lt;/p&gt;
&lt;p&gt;But it looks like one of the biggest ever was 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Sim_Bhullar&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Sim Bhullar&lt;/strong&gt;&lt;/a&gt; from Canada (who was actually the first Indian-origin player in the NBA), who was 2.26m (7’5’’) and 163 kg (359 lbs).&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Shaquille_O%27Neal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Shaquille O’Neal&lt;/strong&gt;&lt;/a&gt;, known as one of the best “big guys” in the NBA, was 2.15m (7’1’’) and 145 kg (319 lbs). 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Yao_Ming&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Yao Ming&lt;/strong&gt;&lt;/a&gt; (the unlabeled orange icon in the graph) was also one of the best “big guys”, and was 2.29m (7&#39;6&amp;rsquo;’) and 141 kg (310 lbs)&lt;/p&gt;
&lt;h3 id=&#34;the-most-important-team-statistic&#34;&gt;The Most Important Team Statistic?&lt;/h3&gt;
&lt;p&gt;Teams can be measured in many different ways, each bringing out different insights. But arguably, one of the most important is “Net Rating”. Net rating refers to the team’s point differential per 100 possessions.&lt;/p&gt;
&lt;h4 id=&#34;4-which-team-has-the-best-and-worst-netrating&#34;&gt;4. Which team has the best and worst Net Rating?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-average-net-rating-for-all-teams-since-1996&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__2esyA2thxLOcaMY__rhSbFw.png&#34; data-caption=&#34;Average Net Rating for all teams since 1996&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__2esyA2thxLOcaMY__rhSbFw.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Average Net Rating for all teams since 1996
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Turns out, the &lt;strong&gt;San Antonio Spurs have the best Net Rating on average, since 1996, with +3.24&lt;/strong&gt;, meaning on average, for every 100 possessions, they’re ahead by 3 points. Only 2 other teams have positive Net Ratings, the Miami Heat and Oklahoma City Thunder. The &lt;strong&gt;worst team is the now defunct Vancouver Grizzlies, with -8.39&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Another surprising insight is that the &lt;strong&gt;San Antonio Spurs were also the oldest team on average&lt;/strong&gt;, over all the seasons. Does experience, in the form of having an older team, actually work? That’s probably the basis for another analysis.&lt;/p&gt;
&lt;h3 id=&#34;the-best-offensive-players&#34;&gt;The Best Offensive Players&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(Disclaimer: These statistics are from 1996 onwards, so with only about half of Michael Jordan’s career recorded here, he’s not on&lt;/em&gt; &lt;strong&gt;&lt;em&gt;these&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;lists, but with a bigger dataset, you can be sure he’d be right up there.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are so many metrics that individual players can be measured by. For an offensive player, the player in charge of trying to create as many baskets as possible, the most used and analyzed metrics are Points and Assists.&lt;/p&gt;
&lt;h4 id=&#34;5-which-players-have-the-most-points-andassists&#34;&gt;5. Which players have the most Points and Assists?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-total-points-vs-total-assists-color-indicates-total-gamesplayed&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__H2__qI8wfkXE7AYxBsSyd8w.png&#34; data-caption=&#34;Total Points vs Total Assists (Color indicates Total Games Played)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__H2__qI8wfkXE7AYxBsSyd8w.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Total Points vs Total Assists (Color indicates Total Games Played)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;LeBron James&lt;/strong&gt;, in a complete league of his own, a complete outlier, well above the rest of the pack with a whopping 34,027 points and 9,280 assists. &lt;strong&gt;Kobe Bryant&lt;/strong&gt; (RIP Mamba) is the only one who can come close to LBJ with 33,633 points and 6,319 assists. (I Googled Michael Jordan’s stats for completeness and he would come somewhere around third, with 32,292 points and 5,633 assists)&lt;/p&gt;
&lt;p&gt;Other notable players on the list are &lt;strong&gt;Dirk Nowitzki&lt;/strong&gt;, with 31,561 points, and 3,667 assists, and &lt;strong&gt;Allen Iverson&lt;/strong&gt;, who’s played the least number of games out of everyone on this graph, with 24,380 points and 5,622 assists in just 914 games. (In comparison, LeBron’s played 1,256 games, Kobe’s played 1,346 games and Dirk’s played 1,522 games).&lt;/p&gt;
&lt;p&gt;— — — — — — — — — — — — — — — — — — — — — — — — — —&lt;/p&gt;
&lt;p&gt;Using points and assists in a different way can bring out more insights. Using total shooting efficiency of a player and the percentage of assists by the player while on the floor, a completely different set of players show up.&lt;/p&gt;
&lt;h4 id=&#34;6-which-players-had-the-highest-shooting-efficiency-and-assist-percentage&#34;&gt;6. Which players had the highest Shooting Efficiency and Assist Percentage?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-scoring-efficiency--vs--of-assists-contributed-color-indicates-average-games-played-perseason&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__LoH8Mml8qN6j97fDkeS__AA.png&#34; data-caption=&#34;Scoring Efficiency % vs % of Assists Contributed (Color indicates Average games played per Season)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__LoH8Mml8qN6j97fDkeS__AA.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Scoring Efficiency % vs % of Assists Contributed (Color indicates Average games played per Season)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;John Stockton&lt;/strong&gt; seems almost like an outlier, with a high shooting efficiency and high average assist percentage. But the repetition of certain players in this graph, such as &lt;strong&gt;Steph Curry, James Harden, LeBron James and Chris Paul&lt;/strong&gt;, showcases their superior shooting ability. Also younger, current players like &lt;strong&gt;Ben Simmons&lt;/strong&gt; and &lt;strong&gt;Nikola Jokic&lt;/strong&gt;, are definitely on the rise and will soon dominate these statistics with consistent perfomances.&lt;/p&gt;
&lt;p&gt;— — — — — — — — — — — — — — — — — — — — — — — — — —&lt;/p&gt;
&lt;p&gt;Play usage percentage is an estimate of the percentage of team plays used by a player while he was on the floor. Typically, when the play usage percentage increases, the scoring efficiency decreases for a player. The real superstar players are those who can keep both these numbers up, turning into very efficient shooters.&lt;/p&gt;
&lt;h4 id=&#34;7-who-were-the-most-effective-shooters&#34;&gt;&lt;strong&gt;7. Who were the most effective shooters?&lt;/strong&gt;&lt;/h4&gt;





  











&lt;figure id=&#34;figure-scoring-efficiency--vs-play-usage--color-indicates-average-games-played-perseason&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__L8m__h4qAeyf2L1hX9uq6mQ.png&#34; data-caption=&#34;Scoring Efficiency % vs Play Usage % (Color indicates Average games played per Season)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__L8m__h4qAeyf2L1hX9uq6mQ.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Scoring Efficiency % vs Play Usage % (Color indicates Average games played per Season)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Joel Embiid&lt;/strong&gt; has the highest average play usage percentage, while &lt;strong&gt;Boban Marjanovic&lt;/strong&gt; has the highest average shooting percentage.&lt;/p&gt;
&lt;p&gt;But the most effective superstar shooters seem to be &lt;strong&gt;Kevin Durant, James Harden and LeBron James,&lt;/strong&gt; due to their all-round ability in using plays and having a high shooting efficiency.&lt;/p&gt;
&lt;p&gt;Closely following them are &lt;strong&gt;Anthony Davis, Shaquille O’Neal and Steph Curry.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-best-defensive-players&#34;&gt;The Best Defensive Players&lt;/h3&gt;
&lt;p&gt;Although typically overlooked, defensive players are those who do most of the grunt work and actively contribute to the success of a team. Without a great defensive player, no matter how many great offensive players you might have, you’ll end up on the worse side of things.&lt;/p&gt;
&lt;p&gt;Rebounds are another important metric for measuring the efficiency of these defensive, lockdown players. A Rebound refers to a player retrieving the ball after a missed field goal or free throw.&lt;/p&gt;
&lt;p&gt;There are 2 types of rebounds, offensive and defensive. A superstar defensive player ideally has a high number of both types of rebounds.&lt;/p&gt;
&lt;h4 id=&#34;8-who-were-the-best-players-defensively-in-terms-of-rebounds&#34;&gt;8. Who were the best players defensively, in terms of rebounds?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-average-offensive-rebound--vs-average-defensive-rebound--color-indicates-average-games-played-perseason&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__J__bshpoWo7kVCuVHRheu8w.png&#34; data-caption=&#34;Average Offensive Rebound % vs Average Defensive Rebound % (Color indicates Average Games played per Season)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__J__bshpoWo7kVCuVHRheu8w.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Average Offensive Rebound % vs Average Defensive Rebound % (Color indicates Average Games played per Season)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;With at least 10 games played per season on average, and a minimum of 2 seasons, these were the top 10 defensive players in terms of rebounds.&lt;/p&gt;
&lt;p&gt;The best defensive rebounder was &lt;strong&gt;Chris Boucher&lt;/strong&gt;, and the best offensive rebounders were &lt;strong&gt;J.R Giddens&lt;/strong&gt; and &lt;strong&gt;Torraye Braggs&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;But what matters most in a defender is their all-round ability, and the best players in terms of both offensive and defensive rebounds are &lt;strong&gt;Dennis Rodman&lt;/strong&gt; and &lt;strong&gt;Andre Drummond&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;double-double-troubletrouble&#34;&gt;Double Double Trouble Trouble&lt;/h3&gt;
&lt;p&gt;Another metric used very often is the “Double Double”, when a player has more than 10 points in 2 of the following metrics: points, assists, rebounds, steals and blocks.&lt;/p&gt;
&lt;p&gt;Most Doubles Doubles are typically made of points, assists and rebounds, which I’ve referred to here as “Most Common Double Double”.&lt;/p&gt;
&lt;h4 id=&#34;9-which-players-had-the-most-seasons-averaging-the-most-common-doubledouble&#34;&gt;9. Which players had the most seasons averaging the “Most Common” Double Double?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-number-of-seasons-averaging-the-most-common-doubledouble&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__uu6YsO4n2nOQo3N3__3prHw.png&#34; data-caption=&#34;Number of Seasons averaging the “Most Common” Double Double&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__uu6YsO4n2nOQo3N3__3prHw.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Number of Seasons averaging the “Most Common” Double Double
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Dwight Howard&lt;/strong&gt; (14 Double Double seasons) narrowly edges over &lt;strong&gt;Tim Duncan&lt;/strong&gt; (13 Double Double seasons) to have the highest number of seasons averaging the “Most Common” Double Double.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Anthony Davis&lt;/strong&gt;, a younger and current player, also being on this list indicates that with a couple of great seasons, he has a very good chance of moving up this list.&lt;/p&gt;
&lt;p&gt;And it’s amazing how &lt;strong&gt;Chris Paul&lt;/strong&gt; features in this graph as well, showcasing his brilliant shooting ability as well as his all-rounded abilities.&lt;/p&gt;
&lt;h3 id=&#34;the-evolvinggame&#34;&gt;The Evolving Game?&lt;/h3&gt;
&lt;p&gt;There have been many conversations on how the game has changed over time. Many people think the aggressiveness and intensity has decreased over time, while others believe that the game has evolved into a more graceful and watchable format.&lt;/p&gt;
&lt;p&gt;Arguably, some of the most important metrics used so far were Points, Assists and Rebounds.&lt;/p&gt;
&lt;h4 id=&#34;10-how-much-have-these-important-metrics-points-assists-rebounds-actually-changed-overtime&#34;&gt;10. How much have these important metrics (Points, Assists, Rebounds) actually changed over time?&lt;/h4&gt;





  











&lt;figure id=&#34;figure-points-rebounds-and-assists-over-time-199697-to201920&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__UJ1vqbbMDBGjc11__2qBdRQ.png&#34; data-caption=&#34;Points, Rebounds and Assists Over Time (1996–97 to 2019–20)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__UJ1vqbbMDBGjc11__2qBdRQ.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Points, Rebounds and Assists Over Time (1996–97 to 2019–20)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Total assists per season&lt;/strong&gt; and &lt;strong&gt;total rebounds per season&lt;/strong&gt; seem &lt;strong&gt;constant&lt;/strong&gt; for the most part, with a slightly increasing tendency.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Total points per season&lt;/strong&gt;, on the other hand, has &lt;strong&gt;significantly increased&lt;/strong&gt;. The 1996–97 season had a total of 3,540 points whereas the 2018–19 season had 4,565 points. Even the incomplete 2019–20 season already has 4,434 points, well on track to go past the 2018–19 season, becoming the highest scoring season of all time (whenever and however the season resumes). &lt;br&gt;
This increase in points could be attributed to the rise of the 3-Pointer revolution, with more and more 3-Pointers being scored now than ever before. It could also be due to the fact that there are a lot more high scoring matches now than before.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I hope you were able to take away some kind of insight from these graphs, because I certainly did. It’s just another way of showing that &lt;strong&gt;the best way to learn is by doing.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I’m open to any kind of suggestions or modifications to these visualizations, or ideas for new ones. Definitely do use them in whatever basketball conversations and arguments you have, as you see fit.&lt;/p&gt;
&lt;p&gt;And I hope it helps fill the void left by the lack of sports right now. (Come back soon, normal life)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Feel free to reach out to me on&lt;/em&gt; 
&lt;a href=&#34;https://www.linkedin.com/in/vishnu-bharadwaj-796877175/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;LinkedIn&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, or check out my&lt;/em&gt; 
&lt;a href=&#34;https://github.com/vishnubharadwaj00&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;GitHub&lt;/em&gt;&lt;/a&gt; &lt;em&gt;for other projects I’ve done.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Analysis of Literacy Rates using Indian Census Data</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/publication/journal-article/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/publication/journal-article/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tesla Stock Prediction using Web Scraping and Recurrent Neural Networks</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/tesla-stock-predictor/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/tesla-stock-predictor/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;Stock Prediction is a versatile and extensive field on its own. With increasingly sophisticated computational capabilities, Stock Prediction is becoming a more and more important application of fields like Machine Learning, Deep Learning and AI.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This project deals with web scraping Tesla stock prices from the Yahoo Finance page, using Beautiful Soup and Selenium, and using Recurrent Neural Networks (particularly LSTMs) to build a Deep Learning model to predict future stock prices.&lt;/p&gt;
&lt;p&gt;Importing the necessary libraries:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
import matplotlib
import pandas as pd
from datetime import datetime
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this is executed in Google Colab, the Google Drive containing the web-scraped data is contained here.&lt;/p&gt;
&lt;p&gt;The code for the webscraping, executed with BeautifulSoup and Selenium, can be found 
&lt;a href=&#34;https://github.com/rmacaraeg/yahoo_finance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;, force_remount=True)
root_dir = &amp;quot;/content/gdrive/My Drive/&amp;quot;
data_dir=os.path.join(root_dir,&amp;quot;tsla.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;amp;response_type=code&amp;amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly

Enter your authorization code:
··········
Mounted at /content/gdrive
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Creating a function to plot Time Series Data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def plot_series(time, series, format=&amp;quot;-&amp;quot;, start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel(&amp;quot;Time&amp;quot;)
    plt.ylabel(&amp;quot;Value&amp;quot;)
    plt.grid(True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reading in the data, using Pandas:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df=pd.read_csv(data_dir)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close*&lt;/th&gt;
      &lt;th&gt;Adj Close**&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Apr 29, 2020&lt;/td&gt;
      &lt;td&gt;790.17&lt;/td&gt;
      &lt;td&gt;803.20&lt;/td&gt;
      &lt;td&gt;783.16&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;15812100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Apr 28, 2020&lt;/td&gt;
      &lt;td&gt;795.64&lt;/td&gt;
      &lt;td&gt;805.00&lt;/td&gt;
      &lt;td&gt;756.69&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;15222000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Apr 27, 2020&lt;/td&gt;
      &lt;td&gt;737.61&lt;/td&gt;
      &lt;td&gt;799.49&lt;/td&gt;
      &lt;td&gt;735.00&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;20681400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Apr 24, 2020&lt;/td&gt;
      &lt;td&gt;710.81&lt;/td&gt;
      &lt;td&gt;730.73&lt;/td&gt;
      &lt;td&gt;698.18&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;13237600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Apr 23, 2020&lt;/td&gt;
      &lt;td&gt;727.60&lt;/td&gt;
      &lt;td&gt;734.00&lt;/td&gt;
      &lt;td&gt;703.13&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;13236700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;248&lt;/th&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;May 06, 2019&lt;/td&gt;
      &lt;td&gt;250.02&lt;/td&gt;
      &lt;td&gt;258.35&lt;/td&gt;
      &lt;td&gt;248.50&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;10833900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;249&lt;/th&gt;
      &lt;td&gt;249&lt;/td&gt;
      &lt;td&gt;May 03, 2019&lt;/td&gt;
      &lt;td&gt;243.86&lt;/td&gt;
      &lt;td&gt;256.61&lt;/td&gt;
      &lt;td&gt;243.49&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;23706800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;250&lt;/th&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;May 02, 2019&lt;/td&gt;
      &lt;td&gt;245.52&lt;/td&gt;
      &lt;td&gt;247.13&lt;/td&gt;
      &lt;td&gt;237.72&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;18159300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;251&lt;/th&gt;
      &lt;td&gt;251&lt;/td&gt;
      &lt;td&gt;May 01, 2019&lt;/td&gt;
      &lt;td&gt;238.85&lt;/td&gt;
      &lt;td&gt;240.00&lt;/td&gt;
      &lt;td&gt;231.50&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;10704400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;252&lt;/th&gt;
      &lt;td&gt;252&lt;/td&gt;
      &lt;td&gt;Apr 30, 2019&lt;/td&gt;
      &lt;td&gt;242.06&lt;/td&gt;
      &lt;td&gt;244.21&lt;/td&gt;
      &lt;td&gt;237.00&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;9464600&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;253 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s a year&amp;rsquo;s worth of stock prices here, from April 30, 2019 to April 29, 2020, with a few days&amp;rsquo; data missing.&lt;/p&gt;
&lt;p&gt;But first, there is some Data Cleaning that needs to be done:&lt;/p&gt;
&lt;p&gt;The columns &amp;ldquo;Close&amp;rdquo; and &amp;ldquo;Adj Close&amp;rdquo; have additional * symbols which have to be removed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df.drop(labels=&amp;quot;Unnamed: 0&amp;quot;, axis=1, inplace=True)
df.rename(columns={&amp;quot;Close*&amp;quot;: &amp;quot;Close&amp;quot;, &amp;quot;Adj Close**&amp;quot;: &amp;quot;Adj Close&amp;quot;},inplace=True)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close&lt;/th&gt;
      &lt;th&gt;Adj Close&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Apr 29, 2020&lt;/td&gt;
      &lt;td&gt;790.17&lt;/td&gt;
      &lt;td&gt;803.20&lt;/td&gt;
      &lt;td&gt;783.16&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;15812100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Apr 28, 2020&lt;/td&gt;
      &lt;td&gt;795.64&lt;/td&gt;
      &lt;td&gt;805.00&lt;/td&gt;
      &lt;td&gt;756.69&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;15222000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Apr 27, 2020&lt;/td&gt;
      &lt;td&gt;737.61&lt;/td&gt;
      &lt;td&gt;799.49&lt;/td&gt;
      &lt;td&gt;735.00&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;20681400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Apr 24, 2020&lt;/td&gt;
      &lt;td&gt;710.81&lt;/td&gt;
      &lt;td&gt;730.73&lt;/td&gt;
      &lt;td&gt;698.18&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;13237600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Apr 23, 2020&lt;/td&gt;
      &lt;td&gt;727.60&lt;/td&gt;
      &lt;td&gt;734.00&lt;/td&gt;
      &lt;td&gt;703.13&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;13236700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;248&lt;/th&gt;
      &lt;td&gt;May 06, 2019&lt;/td&gt;
      &lt;td&gt;250.02&lt;/td&gt;
      &lt;td&gt;258.35&lt;/td&gt;
      &lt;td&gt;248.50&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;10833900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;249&lt;/th&gt;
      &lt;td&gt;May 03, 2019&lt;/td&gt;
      &lt;td&gt;243.86&lt;/td&gt;
      &lt;td&gt;256.61&lt;/td&gt;
      &lt;td&gt;243.49&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;23706800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;250&lt;/th&gt;
      &lt;td&gt;May 02, 2019&lt;/td&gt;
      &lt;td&gt;245.52&lt;/td&gt;
      &lt;td&gt;247.13&lt;/td&gt;
      &lt;td&gt;237.72&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;244.10&lt;/td&gt;
      &lt;td&gt;18159300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;251&lt;/th&gt;
      &lt;td&gt;May 01, 2019&lt;/td&gt;
      &lt;td&gt;238.85&lt;/td&gt;
      &lt;td&gt;240.00&lt;/td&gt;
      &lt;td&gt;231.50&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;10704400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;252&lt;/th&gt;
      &lt;td&gt;Apr 30, 2019&lt;/td&gt;
      &lt;td&gt;242.06&lt;/td&gt;
      &lt;td&gt;244.21&lt;/td&gt;
      &lt;td&gt;237.00&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;9464600&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;253 rows × 7 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Next, it would be much easier if the order of the data was reversed, since the data was originally arranged as latest to earliest.&lt;/p&gt;
&lt;p&gt;This would make it easier to use the earlier data for training and the later data for validation/testing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df=pd.DataFrame(df.values[::-1], df.index, df.columns)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Open&lt;/th&gt;
      &lt;th&gt;High&lt;/th&gt;
      &lt;th&gt;Low&lt;/th&gt;
      &lt;th&gt;Close&lt;/th&gt;
      &lt;th&gt;Adj Close&lt;/th&gt;
      &lt;th&gt;Volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Apr 30, 2019&lt;/td&gt;
      &lt;td&gt;242.06&lt;/td&gt;
      &lt;td&gt;244.21&lt;/td&gt;
      &lt;td&gt;237&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;238.69&lt;/td&gt;
      &lt;td&gt;9464600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;May 01, 2019&lt;/td&gt;
      &lt;td&gt;238.85&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
      &lt;td&gt;231.5&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;234.01&lt;/td&gt;
      &lt;td&gt;10704400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;May 02, 2019&lt;/td&gt;
      &lt;td&gt;245.52&lt;/td&gt;
      &lt;td&gt;247.13&lt;/td&gt;
      &lt;td&gt;237.72&lt;/td&gt;
      &lt;td&gt;244.1&lt;/td&gt;
      &lt;td&gt;244.1&lt;/td&gt;
      &lt;td&gt;18159300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;May 03, 2019&lt;/td&gt;
      &lt;td&gt;243.86&lt;/td&gt;
      &lt;td&gt;256.61&lt;/td&gt;
      &lt;td&gt;243.49&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;255.03&lt;/td&gt;
      &lt;td&gt;23706800&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;May 06, 2019&lt;/td&gt;
      &lt;td&gt;250.02&lt;/td&gt;
      &lt;td&gt;258.35&lt;/td&gt;
      &lt;td&gt;248.5&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;255.34&lt;/td&gt;
      &lt;td&gt;10833900&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;248&lt;/th&gt;
      &lt;td&gt;Apr 23, 2020&lt;/td&gt;
      &lt;td&gt;727.6&lt;/td&gt;
      &lt;td&gt;734&lt;/td&gt;
      &lt;td&gt;703.13&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;705.63&lt;/td&gt;
      &lt;td&gt;13236700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;249&lt;/th&gt;
      &lt;td&gt;Apr 24, 2020&lt;/td&gt;
      &lt;td&gt;710.81&lt;/td&gt;
      &lt;td&gt;730.73&lt;/td&gt;
      &lt;td&gt;698.18&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;725.15&lt;/td&gt;
      &lt;td&gt;13237600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;250&lt;/th&gt;
      &lt;td&gt;Apr 27, 2020&lt;/td&gt;
      &lt;td&gt;737.61&lt;/td&gt;
      &lt;td&gt;799.49&lt;/td&gt;
      &lt;td&gt;735&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;798.75&lt;/td&gt;
      &lt;td&gt;20681400&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;251&lt;/th&gt;
      &lt;td&gt;Apr 28, 2020&lt;/td&gt;
      &lt;td&gt;795.64&lt;/td&gt;
      &lt;td&gt;805&lt;/td&gt;
      &lt;td&gt;756.69&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;769.12&lt;/td&gt;
      &lt;td&gt;15222000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;252&lt;/th&gt;
      &lt;td&gt;Apr 29, 2020&lt;/td&gt;
      &lt;td&gt;790.17&lt;/td&gt;
      &lt;td&gt;803.2&lt;/td&gt;
      &lt;td&gt;783.16&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;800.51&lt;/td&gt;
      &lt;td&gt;15812100&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;253 rows × 7 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Creating a seperate Data Frame to visualize the stock prices better:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;adj_close=df[&#39;Adj Close&#39;]
adj_close.index = df[&#39;Date&#39;]

adj_close.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Date
Apr 30, 2019    238.69
May 01, 2019    234.01
May 02, 2019     244.1
May 03, 2019    255.03
May 06, 2019    255.34
Name: Adj Close, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;type(df[&#39;Date&#39;][0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;str
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &amp;ldquo;Date&amp;rdquo; column in the original Data Frame is of type &amp;ldquo;String&amp;rdquo;, but it has to be of &amp;ldquo;Datetime&amp;rdquo; format, to make it easier to plot it in the x-axis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dates=df[&#39;Date&#39;]
dates1=[]
for date in dates:
    dates1.append(datetime.strptime(date, &#39;%b %d, %Y&#39;))
dates=pd.core.series.Series(dates1)

type(dates[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;pandas._libs.tslibs.timestamps.Timestamp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting the Adjusted Close Stock Prices over the last year:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.figure(figsize=(13,8))
plt.style.use(&#39;fivethirtyeight&#39;)
plt.plot(dates1,adj_close)
plt.title(&amp;quot;Tesla Adjusted Close Prices&amp;quot;,loc=&#39;left&#39;)
plt.rcParams.update({&#39;font.size&#39;: 14})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-plot-of-adjusted-close-stock-prices&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/TeslaStockPredictor_20_0.png&#34; data-caption=&#34;Plot of Adjusted Close Stock Prices&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/TeslaStockPredictor_20_0.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Plot of Adjusted Close Stock Prices
  &lt;/figcaption&gt;


&lt;/figure&gt;

There&amp;rsquo;s a huge up-tick in the price sometime after January 2020, which was when Tesla announced strong fourth-quarter financials, which exceeded all expectations.&lt;/p&gt;
&lt;p&gt;The dip in the price in March corresponds to the COVID-19 pandemic and the financial crisis that it brought with it.&lt;/p&gt;
&lt;p&gt;It does seem to be on the rise now, so it will be interesting to see if the model can predict all these ups and downs.&lt;/p&gt;
&lt;p&gt;Creating Series and Time arrays, and converting them to the right type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;series = np.array(adj_close.values)
time = np.array(dates)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;series=pd.to_numeric(series,errors=&#39;coerce&#39;,downcast=&#39;float&#39;)
series
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([238.69, 234.01, 244.1 , 255.03, 255.34, 247.06, 244.84, 241.98,
       239.52, 227.01, 232.31, 231.95, 228.33, 211.03, 205.36, 205.08,
       192.73, 195.49, 190.63, 188.7 , 189.86, 188.22, 185.16, 178.97,
       193.6 , 196.59, 205.95, 204.5 , 212.88, 217.1 , 209.26, 213.91,
       214.92, 225.03, 224.74, 226.43, 219.62, 221.86, 223.64, 219.76,
       219.27, 222.84, 223.46, 227.17, 224.55, 234.9 , 233.1 , 230.34,
       230.06, 238.92, 238.6 , 245.08, 253.5 , 252.38, 254.86, 253.54,
       258.18, 255.68, 260.17, 264.88, 228.82, 228.04, 235.77, 242.26,
       241.61, 233.85, 234.34, 228.32, 230.75, 233.42, 238.3 , 235.01,
       229.01, 235.  , 219.62, 215.64, 219.94, 226.83, 225.86, 220.83,
       222.15, 211.4 , 215.  , 214.08, 215.59, 221.71, 225.61, 225.01,
       220.68, 229.58, 227.45, 231.79, 235.54, 247.1 , 245.87, 245.2 ,
       242.81, 244.79, 243.49, 246.6 , 240.62, 241.23, 223.21, 228.7 ,
       242.56, 242.13, 240.87, 244.69, 243.13, 233.03, 231.43, 237.72,
       240.05, 244.53, 244.74, 247.89, 256.96, 257.89, 259.75, 261.97,
       256.95, 253.5 , 255.58, 254.68, 299.68, 328.13, 327.71, 316.22,
       315.01, 314.92, 313.31, 317.47, 317.22, 326.58, 335.54, 337.14,
       345.09, 349.93, 346.11, 349.35, 352.17, 349.99, 359.52, 352.22,
       354.83, 333.04, 336.34, 328.92, 331.29, 329.94, 334.87, 336.2 ,
       333.03, 330.37, 335.89, 339.53, 348.84, 352.7 , 359.68, 358.39,
       381.5 , 378.99, 393.15, 404.04, 405.59, 419.22, 425.25, 430.94,
       430.38, 414.7 , 418.33, 430.26, 443.01, 451.54, 469.06, 492.14,
       481.34, 478.15, 524.86, 537.92, 518.5 , 513.49, 510.5 , 547.2 ,
       569.56, 572.2 , 564.82, 558.02, 566.9 , 580.99, 640.81, 650.57,
       780.  , 887.06, 734.7 , 748.96, 748.07, 771.28, 774.38, 767.29,
       804.  , 800.03, 858.4 , 917.42, 899.41, 901.  , 833.79, 799.91,
       778.8 , 679.  , 667.99, 743.62, 745.51, 749.5 , 724.54, 703.48,
       608.  , 645.33, 634.23, 560.55, 546.62, 445.07, 430.2 , 361.22,
       427.64, 427.53, 434.29, 505.  , 539.25, 528.16, 514.36, 502.13,
       524.  , 481.56, 454.47, 480.01, 516.24, 545.45, 548.84, 573.  ,
       650.95, 709.89, 729.83, 745.21, 753.89, 746.36, 686.72, 732.11,
       705.63, 725.15, 798.75, 769.12, 800.51], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;series.dtype
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dtype(&#39;float32&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are about 250 tuples, so a 80-20 split is somewhere around 210 for training set and 40 for testing set. The window size (for creating a windowed_dataset) and batch size can also be specified here.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;split_time = 210
adj_train = series[:split_time]
adj_valid = series[split_time:]
dates_train=dates[:split_time]
dates_valid=dates[split_time:]

window_size = 16
batch_size = 32
shuffle_buffer_size = 50
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A function for creating a windowed dataset can be specified here. This is particularly helpful because it helps to create specific sized data slices, to train on them, make predictions, and subsequently learn from those predictions as well.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A function for forecasting data, given the series and the model can be specified here:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def model_forecast(model, series, window_size):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size))
    ds = ds.batch(32).prefetch(1)
    forecast = model.predict(ds)
    return forecast
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Creating our windowed training set:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
window_size = 64
batch_size = 256
train_set = windowed_dataset(adj_train, window_size, batch_size, shuffle_buffer_size)
print(train_set)
print(adj_train.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float32, tf.float32)&amp;gt;
(210,)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifying the layers of our Keras model.&lt;/p&gt;
&lt;p&gt;It has 1 Convolutional Layer, which complements the windowing of the dataset. Simply through extensive trial and error, the configuration of 2 LSTMs (64 and 32 nodes), and 3 Dense Layers (24, 12 and 1 nodes) was selected here.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=60, kernel_size=5,
                      strides=1, padding=&amp;quot;causal&amp;quot;,
                      activation=&amp;quot;relu&amp;quot;,
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(32, return_sequences=True),
  tf.keras.layers.Dense(24, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(12, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400)
])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before actually running our model in its entirity, it might help to specify a learning rate, so the model can be run for a specified number of epochs, and seeing how the model does, a optimized learning rate can be found. This hyperparameter tuning will prove to be very useful later.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))
optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=[&amp;quot;mae&amp;quot;])
history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/100
1/1 [==============================] - 0s 40ms/step - loss: 269.6010 - mae: 270.1010 - lr: 1.0000e-08
Epoch 2/100
1/1 [==============================] - 0s 2ms/step - loss: 269.5738 - mae: 270.0738 - lr: 1.1220e-08
Epoch 3/100
1/1 [==============================] - 0s 2ms/step - loss: 269.5186 - mae: 270.0186 - lr: 1.2589e-08
Epoch 4/100
1/1 [==============================] - 0s 2ms/step - loss: 269.4341 - mae: 269.9341 - lr: 1.4125e-08
Epoch 5/100
1/1 [==============================] - 0s 2ms/step - loss: 269.3180 - mae: 269.8180 - lr: 1.5849e-08
....
....
....
Epoch 96/100
1/1 [==============================] - 0s 2ms/step - loss: 72.6451 - mae: 73.1451 - lr: 5.6234e-04
Epoch 97/100
1/1 [==============================] - 0s 2ms/step - loss: 117.5780 - mae: 118.0779 - lr: 6.3096e-04
Epoch 98/100
1/1 [==============================] - 0s 2ms/step - loss: 81.4628 - mae: 81.9628 - lr: 7.0795e-04
Epoch 99/100
1/1 [==============================] - 0s 2ms/step - loss: 105.4503 - mae: 105.9502 - lr: 7.9433e-04
Epoch 100/100
1/1 [==============================] - 0s 2ms/step - loss: 81.7803 - mae: 82.2790 - lr: 8.9125e-04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting the results of the limited run of the model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.title(&amp;quot;Loss vs Learning Rate&amp;quot;)
plt.xlabel(&amp;quot;Loss&amp;quot;)
plt.ylabel(&amp;quot;Learning Rate&amp;quot;)
plt.semilogx(history.history[&amp;quot;lr&amp;quot;], history.history[&amp;quot;loss&amp;quot;])
#plt.axis([1e-8, 1e-3,135,250])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D at 0x7f3e82620048&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-plot-of-loss-vs-learning-rate&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/TeslaStockPredictor_39_1.png&#34; data-caption=&#34;Plot of Loss vs Learning Rate&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/TeslaStockPredictor_39_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Plot of Loss vs Learning Rate
  &lt;/figcaption&gt;


&lt;/figure&gt;

There&amp;rsquo;s a huge dip in the learning rate somewhere between $10^-7$ and $10^-6$, which helps the gradient descent process in training, helping the model to learn quickly and more effectively, so that can be fixed as the learning rate, and the model is run completely this time:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=60, kernel_size=5,
                      strides=1, padding=&amp;quot;causal&amp;quot;,
                      activation=&amp;quot;relu&amp;quot;,
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(256, return_sequences=True),
  tf.keras.layers.LSTM(128, return_sequences=True),
  tf.keras.layers.Dense(128, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(64, activation=&amp;quot;relu&amp;quot;),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 500)
])



optimizer = tf.keras.optimizers.SGD(lr=3e-7, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=[&amp;quot;mae&amp;quot;])
history = model.fit(train_set,epochs=350)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/350
1/1 [==============================] - 0s 7ms/step - loss: 330.9964 - mae: 331.4964
Epoch 2/350
1/1 [==============================] - 0s 901us/step - loss: 314.5646 - mae: 315.0646
Epoch 3/350
1/1 [==============================] - 0s 849us/step - loss: 295.7275 - mae: 296.2275
Epoch 4/350
1/1 [==============================] - 0s 986us/step - loss: 277.2317 - mae: 277.7317
Epoch 5/350
1/1 [==============================] - 0s 835us/step - loss: 244.2985 - mae: 244.7985
Epoch 6/350
1/1 [==============================] - 0s 860us/step - loss: 225.8815 - mae: 226.3815
Epoch 7/350
1/1 [==============================] - 0s 836us/step - loss: 208.5272 - mae: 209.0272
Epoch 8/350
1/1 [==============================] - 0s 2ms/step - loss: 184.0172 - mae: 184.5172
Epoch 9/350
1/1 [==============================] - 0s 895us/step - loss: 158.5788 - mae: 159.0788
Epoch 10/350
1/1 [==============================] - 0s 988us/step - loss: 140.6364 - mae: 141.1363
...
...
...
Epoch 101/350
1/1 [==============================] - 0s 927us/step - loss: 48.9045 - mae: 49.4025
Epoch 102/350
1/1 [==============================] - 0s 833us/step - loss: 47.7830 - mae: 48.2795
Epoch 103/350
1/1 [==============================] - 0s 844us/step - loss: 47.8659 - mae: 48.3605
Epoch 104/350
1/1 [==============================] - 0s 1ms/step - loss: 48.0967 - mae: 48.5908
Epoch 105/350
1/1 [==============================] - 0s 832us/step - loss: 46.6607 - mae: 47.1546
...
...
...
Epoch 200/350
1/1 [==============================] - 0s 2ms/step - loss: 25.1572 - mae: 25.6457
Epoch 201/350
1/1 [==============================] - 0s 842us/step - loss: 25.0279 - mae: 25.5159
Epoch 202/350
1/1 [==============================] - 0s 1ms/step - loss: 25.0110 - mae: 25.4993
Epoch 203/350
1/1 [==============================] - 0s 889us/step - loss: 24.9164 - mae: 25.4044
Epoch 204/350
1/1 [==============================] - 0s 933us/step - loss: 24.9017 - mae: 25.3895
Epoch 205/350
1/1 [==============================] - 0s 1ms/step - loss: 24.7882 - mae: 25.2767
...
...
...
Epoch 300/350
1/1 [==============================] - 0s 1ms/step - loss: 22.3528 - mae: 22.8462
Epoch 301/350
1/1 [==============================] - 0s 844us/step - loss: 20.8384 - mae: 21.3259
Epoch 302/350
1/1 [==============================] - 0s 779us/step - loss: 19.6541 - mae: 20.1386
Epoch 303/350
1/1 [==============================] - 0s 820us/step - loss: 19.2455 - mae: 19.7316
Epoch 304/350
1/1 [==============================] - 0s 1ms/step - loss: 19.0995 - mae: 19.5838
Epoch 305/350
1/1 [==============================] - 0s 922us/step - loss: 19.0408 - mae: 19.5255
...
...
...
Epoch 345/350
1/1 [==============================] - 0s 914us/step - loss: 19.7900 - mae: 20.2793
Epoch 346/350
1/1 [==============================] - 0s 836us/step - loss: 18.8462 - mae: 19.3341
Epoch 347/350
1/1 [==============================] - 0s 963us/step - loss: 18.4283 - mae: 18.9135
Epoch 348/350
1/1 [==============================] - 0s 852us/step - loss: 18.3297 - mae: 18.8162
Epoch 349/350
1/1 [==============================] - 0s 963us/step - loss: 18.4796 - mae: 18.9658
Epoch 350/350
1/1 [==============================] - 0s 1ms/step - loss: 19.0140 - mae: 19.5016
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The MAE (Mean Absolute Error) seems pretty low after running it for 350 epochs, which is an excellent sign. It started at ~330 and ended around ~19. Now we can build a forecast using the model_forecast function specified earlier:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)
rnn_forecast = rnn_forecast[split_time - window_size:-1,-1, 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are mainly 2 metrics used for evaluating the accuracy of time series data, Mean Absolute Error and Root Mean Squared Error.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;n = tf.keras.metrics.MeanAbsoluteError()
n.update_state(adj_valid, rnn_forecast)
print(&#39;Mean Absolute Error: &#39;, n.result().numpy())
m = tf.keras.metrics.RootMeanSquaredError()
m.update_state(adj_valid, rnn_forecast)
print(&#39;Root Mean Squared Error: &#39;, m.result().numpy())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean Absolute Error:  152.27446
Root Mean Squared Error:  171.10997
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Considering the fact that there was only 250 data points, and a pretty simple RNN built, a MAE of 150 and RMSE of 171 is extremely good. With more data, and a bigger model, it&amp;rsquo;s very possible to reduce these numbers.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Plotting the predicted data against what actually happened:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.figure(figsize=(20, 6))
plt.style.use(&#39;fivethirtyeight&#39;)
plt.title(&amp;quot;Predictions vs Reality&amp;quot;, loc=&amp;quot;left&amp;quot;)
plt.plot(dates_valid, adj_valid, label=&amp;quot;Actual&amp;quot;)
plt.plot(dates_valid, rnn_forecast, label= &amp;quot;Prediction&amp;quot;)
plt.legend()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend at 0x7f12360679e8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-plot-of-predicted-vs-actual&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/TeslaStockPredictor_47_1.png&#34; data-caption=&#34;Plot of Predicted vs Actual&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/TeslaStockPredictor_47_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Plot of Predicted vs Actual
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model seemed to have gotten it pretty spot-on. It dips when the actual value dipped and seems to be on the rise, as is the case towards the end.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Pneumonia with Chest X-Ray Images</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/chestxray/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/chestxray/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/PneumoniaDetection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Healthcare is an extremely important part of the technological revolution, with deep learning techniques being applied to more and more medical problems.&lt;/p&gt;
&lt;p&gt;This dataset for the detection of pneumonia, using chest x-ray images (&lt;a href=&#34;https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia&#34;&gt;https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia&lt;/a&gt;) is used for binary classification (whether the person is normal or has pneumonia).&lt;/p&gt;
&lt;p&gt;This notebook was executed in a Google Colab environment, and used transfer learning from a ResNet50 architecture, and after just 10 epochs of training, I was able to achieve &lt;strong&gt;&amp;gt;80% accuracy&lt;/strong&gt;. Some minor tweaks and additional architectural changes can definitely increase the accuracy to close to 90% and maybe, even beyond that.&lt;/p&gt;
&lt;p&gt;Using this very simple code, we can use the Kaggle API to download the dataset into our environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
os.environ[&#39;KAGGLE_USERNAME&#39;] = &amp;quot;##########&amp;quot; # username from the json file
os.environ[&#39;KAGGLE_KEY&#39;] = &amp;quot;###################&amp;quot; # key from the json file
!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading chest-xray-pneumonia.zip to /content
100% 2.29G/2.29G [00:30&amp;lt;00:00, 24.3MB/s]
100% 2.29G/2.29G [00:30&amp;lt;00:00, 80.1MB/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, the file has to be unzipped (it is in zip format) and the directories are specified for the train, test and validation sets, as well as the normal and pneumonia directories for the train and validation sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import zipfile
local_zip = &#39;/content/chest-xray-pneumonia.zip&#39;
zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;)
zip_ref.extractall(&#39;/content&#39;)
zip_ref.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;base_dir = &#39;/content/chest_xray&#39;
train_dir = os.path.join(base_dir, &#39;train&#39;)
validation_dir = os.path.join(base_dir, &#39;val&#39;)
test_dir = os.path.join(base_dir, &#39;test&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;train_normal_dir = os.path.join(train_dir,&#39;NORMAL&#39;)
train_pneu_dir = os.path.join(train_dir,&#39;PNEUMONIA&#39;)
validation_normal_dir = os.path.join(validation_dir,&#39;NORMAL&#39;)
validation_pneu_dir = os.path.join(validation_dir,&#39;PNEUMONIA&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;print(&#39;total training normal images:&#39;, len(os.listdir(train_normal_dir)))
print(&#39;total training pneu images:&#39;, len(os.listdir(train_pneu_dir)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;total training normal images: 1341
total training pneu images: 3875
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 1341 normal images and 3875 pneumonia images in the training set, which seems to be more than sufficient.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import Model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We specify a constant image size to make things uniform for the model to input. We then input the ResNet50 architecture, and ensure it cannot be trained, and that we retain all the weights.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;img_size=[224,224]
model=tf.keras.applications.resnet50.ResNet50(input_shape=img_size + [3], weights=&#39;imagenet&#39;, include_top=False)
model.trainable = False
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
94773248/94765736 [==============================] - 1s 0us/step
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last layer of the architecture is the &amp;lsquo;conv5_block3_out&amp;rsquo;, with a shape of (7,7,2048) which we will use later.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;last_layer = model.get_layer(&#39;conv5_block3_out&#39;)
print(&#39;last layer output shape: &#39;, last_layer.output_shape)
last_output = last_layer.output
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;last layer output shape:  (None, 7, 7, 2048)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then specify 2 last layers, a dense layer with 1024 nodes, with a 20% dropout rate to prevent overfitting, as well as a final dense layer with a single node and a sigmoid activation (which outputs 0 or 1 to classify)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = layers.Flatten()(last_output)
x = layers.Dense(1024, activation=&#39;relu&#39;)(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense  (1, activation=&#39;sigmoid&#39;)(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;amodel=Model(model.input,x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The layers of our new model (amodel) and their shapes can be seen using the summary() function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;amodel.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model_2&amp;quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_2 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_2[0][0]
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]
                                                                 conv2_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]
                                                                 conv2_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]
                                                                 conv2_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]
                                                                 conv3_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]
                                                                 conv3_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]
                                                                 conv3_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]
                                                                 conv3_block4_3_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]
                                                                 conv4_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]
                                                                 conv4_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]
                                                                 conv4_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]
                                                                 conv4_block4_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]
                                                                 conv4_block5_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]
                                                                 conv4_block6_3_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]
                                                                 conv5_block1_3_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]
                                                                 conv5_block2_3_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]
                                                                 conv5_block3_3_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 100352)       0           conv5_block3_out[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         102761472   flatten_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 1024)         0           dense_2[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            1025        dropout_1[0][0]
==================================================================================================
Total params: 126,350,209
Trainable params: 102,762,497
Non-trainable params: 23,587,712
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;from tensorflow.keras.optimizers import RMSprop
amodel.compile(optimizer = RMSprop(lr=0.0001),
              loss = &#39;binary_crossentropy&#39;,
              metrics = [&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compiling the model using RMSProp as the optimizer and a binary_crossentropy loss because of the 2 classes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Next, we use the ImageDataGenerator function in keras to import the images and perform some augmentation on them, as well as to specify some arguments.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255.,
                                   rotation_range = 40,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator( rescale = 1.0/255. )

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    batch_size = 20,
                                                    class_mode = &#39;binary&#39;,
                                                    target_size = (224, 224))

validation_generator =  test_datagen.flow_from_directory( validation_dir,
                                                          batch_size  = 20,
                                                          class_mode  = &#39;binary&#39;,
                                                          target_size = (224, 224))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Found 5216 images belonging to 2 classes.
Found 16 images belonging to 2 classes.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 5216 training images and 16 validation images.&lt;/p&gt;
&lt;p&gt;All that&amp;rsquo;s left is to run the model for just 10 epochs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;history = amodel.fit(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = 100,
            epochs = 10,
            validation_steps = 50,
            verbose = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
100/100 - 47s - loss: 0.8222 - accuracy: 0.6995 - val_loss: 0.6554 - val_accuracy: 0.5625
Epoch 2/10
100/100 - 45s - loss: 0.5184 - accuracy: 0.7490 - val_loss: 0.6276 - val_accuracy: 0.6250
Epoch 3/10
100/100 - 46s - loss: 0.4954 - accuracy: 0.7555 - val_loss: 0.7007 - val_accuracy: 0.5625
Epoch 4/10
100/100 - 46s - loss: 0.4552 - accuracy: 0.7710 - val_loss: 0.9272 - val_accuracy: 0.5625
Epoch 5/10
100/100 - 45s - loss: 0.4410 - accuracy: 0.7835 - val_loss: 0.9437 - val_accuracy: 0.5625
Epoch 6/10
100/100 - 45s - loss: 0.4338 - accuracy: 0.7871 - val_loss: 0.5694 - val_accuracy: 0.6250
Epoch 7/10
100/100 - 45s - loss: 0.4113 - accuracy: 0.7920 - val_loss: 0.9337 - val_accuracy: 0.5625
Epoch 8/10
100/100 - 45s - loss: 0.4150 - accuracy: 0.8001 - val_loss: 0.9175 - val_accuracy: 0.5625
Epoch 9/10
100/100 - 45s - loss: 0.4003 - accuracy: 0.8055 - val_loss: 0.7112 - val_accuracy: 0.6875
Epoch 10/10
100/100 - 46s - loss: 0.3825 - accuracy: 0.8155 - val_loss: 0.6212 - val_accuracy: 0.6250
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get 81% accuracy after 10 epochs. Validation accuracy is around 62%. Other architectures such as VGG16 or VGG19 can also be used, and may increase the accuracy.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We can plot the training and validation accuracy and see how training occurred.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import matplotlib.pyplot as plt
acc = history.history[&#39;accuracy&#39;]
val_acc = history.history[&#39;val_accuracy&#39;]
loss = history.history[&#39;loss&#39;]
val_loss = history.history[&#39;val_loss&#39;]

epochs = range(len(acc))

plt.plot(epochs, acc, &#39;r&#39;, label=&#39;Training accuracy&#39;)
plt.plot(epochs, val_acc, &#39;b&#39;, label=&#39;Validation accuracy&#39;)
plt.title(&#39;Training and validation accuracy&#39;)
plt.legend(loc=0)
plt.figure()


plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/projectimages/ChestXRayResNet50_files/ChestXRayResNet50_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 432x288 with 0 Axes&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training accuracy seems to be steadily increasing, but validation accuracy seems to have peaks and valleys, but overall it does increase. This might be a sign of some overfitting, but definitely not a large amount of overfitting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bird Sound Classifier</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/birdsound/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/birdsound/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/BirdSoundClassifier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;bird-sound-classifier&#34;&gt;&lt;strong&gt;Bird Sound Classifier&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;fast.ai&amp;rsquo;s courses and software make it extremely easy to start working on difficult projects very quickly. This is just another example of that.&lt;/p&gt;
&lt;p&gt;This is a Bird Sound Classifying Deep Learning model, which takes in bird sounds, converts them into images (spectograms), and then classifies those images based on what type of bird call it is.&lt;/p&gt;
&lt;p&gt;The data is from: &lt;a href=&#34;https://datadryad.org/resource/doi:10.5061/dryad.4g8b7/1&#34;&gt;https://datadryad.org/resource/doi:10.5061/dryad.4g8b7/1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are 6 types of bird calls: distance,hat,kackle,song,stack,tet.&lt;/p&gt;
&lt;p&gt;This model gets around 80% accuracy, which is not bad at all for something that relies on so many different factors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Libraries&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The necessary libraries and functions have to be imported:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from fastai.vision import *
%reload_ext autoreload
%autoreload 2
%matplotlib inline
from fastai import *
import matplotlib.pyplot as plt
from matplotlib.pyplot import specgram
import librosa
import numpy as np
import librosa.display
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this was done on Google&amp;rsquo;s Colab environment, it is necessary to link up Google Drive to the project, so that the data can be imported.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;, force_remount=True)
root_dir = &amp;quot;/content/gdrive/My Drive/&amp;quot;
base_dir = root_dir + &#39;bird-recognition&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mounted at /content/gdrive
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;path = Path(base_dir+&#39;/wav_files_playback&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Creating spectograms:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next, a very simple function, create_fold_spectrograms, which takes in the folder name as input, and creates spectrograms in corresponding folders in a seperate path. This uses the librosa package. The code is similar to the one used in: &lt;a href=&#34;https://github.com/etown/dl1/blob/master/UrbanSoundClassification.ipynb&#34;&gt;https://github.com/etown/dl1/blob/master/UrbanSoundClassification.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def create_fold_spectrograms(folder):
    spectrogram_path = Path(base_dir+&#39;/specto&#39;)
    audio_path = path
    os.makedirs(spectrogram_path/folder,exist_ok=True)
    for audio_file in list(Path(audio_path/f&#39;{folder}&#39;).glob(&#39;*.wav&#39;)):
        samples, sample_rate = librosa.load(audio_file)
        fig = plt.figure(figsize=[0.72,0.72])
        ax = fig.add_subplot(111)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        ax.set_frame_on(False)
        filename  = spectrogram_path/folder/Path(audio_file).name.replace(&#39;.wav&#39;,&#39;.png&#39;)
        S = librosa.feature.melspectrogram(y=samples, sr=sample_rate)
        librosa.display.specshow(librosa.power_to_db(S, ref=np.max))
        plt.savefig(filename, dpi=400, bbox_inches=&#39;tight&#39;,pad_inches=0)
        plt.close(&#39;all&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folds=[&#39;distance&#39;,&#39;hat&#39;,&#39;kackle&#39;,&#39;song&#39;,&#39;stack&#39;,&#39;tet&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;for i in folds:
  create_fold_spectrograms(str(i))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Data Bunch&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once, the sound files are converted into image files, the data can be extracted from the folders and seperated into training and validation sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;np.random.seed(42)
spectrogram_path = Path(base_dir+&#39;/specto&#39;)
tfms = get_transforms(do_flip=False)
# don&#39;t use any transformations because it doesn&#39;t make sense in the case of a spectrogram
# i.e. flipping a spectrogram changes the meaning
data = ImageDataBunch.from_folder(spectrogram_path, train=&amp;quot;.&amp;quot;, ds_tfms=tfms, valid_pct=0.2, size=224)
data.normalize(imagenet_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ImageDataBunch;

Train: LabelList (152 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
distance,distance,distance,distance,distance
Path: /content/gdrive/My Drive/bird-recognition/specto;

Valid: LabelList (37 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
tet,tet,distance,distance,hat
Path: /content/gdrive/My Drive/bird-recognition/specto;

Test: None
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;data.show_batch(rows=3,figsize=(7,7))
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/BirdSoundClassifier_12_0.png&#34; &gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/BirdSoundClassifier_12_0.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;pre&gt;&lt;code&gt;data.classes, data.c, len(data.train_ds), len(data.valid_ds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;([&#39;distance&#39;, &#39;hat&#39;, &#39;kackle&#39;, &#39;song&#39;, &#39;stack&#39;, &#39;tet&#39;], 6, 152, 37)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that there are 6 different classes, and a good split between training and validation sets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next, cnn_learner can be used, with a resnet34 architecture, to train the model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn = cnn_learner(data, models.resnet34, metrics=[error_rate,accuracy])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading: &amp;quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&amp;quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth
100%|██████████| 87306240/87306240 [00:00&amp;lt;00:00, 101948611.46it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;learn.fit_one_cycle(6,max_lr=slice(3e-03))
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;accuracy&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.438323&lt;/td&gt;
      &lt;td&gt;0.588238&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.810811&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.361946&lt;/td&gt;
      &lt;td&gt;0.716108&lt;/td&gt;
      &lt;td&gt;0.324324&lt;/td&gt;
      &lt;td&gt;0.675676&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.333349&lt;/td&gt;
      &lt;td&gt;1.141138&lt;/td&gt;
      &lt;td&gt;0.297297&lt;/td&gt;
      &lt;td&gt;0.702703&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.290415&lt;/td&gt;
      &lt;td&gt;1.483750&lt;/td&gt;
      &lt;td&gt;0.297297&lt;/td&gt;
      &lt;td&gt;0.702703&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.271594&lt;/td&gt;
      &lt;td&gt;1.513314&lt;/td&gt;
      &lt;td&gt;0.324324&lt;/td&gt;
      &lt;td&gt;0.675676&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.240999&lt;/td&gt;
      &lt;td&gt;1.303614&lt;/td&gt;
      &lt;td&gt;0.297297&lt;/td&gt;
      &lt;td&gt;0.702703&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There&amp;rsquo;s only about 70% accuracy, which can be made higher with the right learning rate:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.lr_find()
learn.recorder.plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/BirdSoundClassifier_19_2.png&#34; &gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/BirdSoundClassifier_19_2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;pre&gt;&lt;code&gt;learn.unfreeze()
learn.fit_one_cycle(6,max_lr=slice(3e-03))
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;accuracy&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.089979&lt;/td&gt;
      &lt;td&gt;1.076153&lt;/td&gt;
      &lt;td&gt;0.324324&lt;/td&gt;
      &lt;td&gt;0.675676&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.085291&lt;/td&gt;
      &lt;td&gt;0.820889&lt;/td&gt;
      &lt;td&gt;0.243243&lt;/td&gt;
      &lt;td&gt;0.756757&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.080007&lt;/td&gt;
      &lt;td&gt;0.758169&lt;/td&gt;
      &lt;td&gt;0.189189&lt;/td&gt;
      &lt;td&gt;0.810811&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.099773&lt;/td&gt;
      &lt;td&gt;0.824883&lt;/td&gt;
      &lt;td&gt;0.216216&lt;/td&gt;
      &lt;td&gt;0.783784&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.106347&lt;/td&gt;
      &lt;td&gt;0.963399&lt;/td&gt;
      &lt;td&gt;0.243243&lt;/td&gt;
      &lt;td&gt;0.756757&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.101405&lt;/td&gt;
      &lt;td&gt;0.916323&lt;/td&gt;
      &lt;td&gt;0.216216&lt;/td&gt;
      &lt;td&gt;0.783784&lt;/td&gt;
      &lt;td&gt;00:03&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;78% accuracy is the final accuracy. With some tinkering, this can be increased to slightly above 80% as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting Results:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the ClassificationInterpretation function, the results of the training model can be interepreted:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;interp=ClassificationInterpretation.from_learner(learn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;interp.plot_confusion_matrix()
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/BirdSoundClassifier_24_0.png&#34; &gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/BirdSoundClassifier_24_0.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;pre&gt;&lt;code&gt;interp.most_confused(min_val=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(&#39;tet&#39;, &#39;hat&#39;, 5), (&#39;tet&#39;, &#39;stack&#39;, 2)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this, it is evident that tet is the one causing the most problem, with it being misclassified 7 times, and only correctly classified once. Otherwise, the model is almost fully accurate.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Laptop Brand Classifier</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/laptop/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/laptop/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/LaptopBrandRecognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;what-brand-is-this-laptop&#34;&gt;&lt;strong&gt;What brand is this laptop?&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;Based on Lesson 2 of fast.ai&amp;rsquo;s Deep Learning course, it is possible to scrape images of the internet (particularly Google Images) to build our own classifier, which is actually extremely useful and can be applied to any number of applications.&lt;/p&gt;
&lt;p&gt;Here, I chose a really simple problem, to classify laptops based on their brands using images of them. Although it may not seem so simple, since all laptops look similar to a certain extent, the highly efficient Deep Learning models will beg to differ.&lt;/p&gt;
&lt;p&gt;This model gets around &lt;strong&gt;83% accuracy&lt;/strong&gt;, which is a very good result considering how similar laptops from different brands look.&lt;/p&gt;
&lt;p&gt;This is the code used to carry out this task:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from fastai.vision import *
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After going on Google Images, and searching for whatever images we want (e.g Macbooks), we can insert a simple Javascript command into the browser:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;urls = Array.from(document.querySelectorAll(&amp;quot;.rg_di .rg_meta&amp;quot;)).map(
  (el) =&amp;gt; JSON.parse(el.textContent).ou
);
window.open(&amp;quot;data:text/csv;charset=utf-8,&amp;quot; + escape(urls.join(&amp;quot;\n&amp;quot;)));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create the necessary folder and file name for the data to be imported into.&lt;/p&gt;
&lt;p&gt;I am using Google&amp;rsquo;s Colab so all the images will be stored in Google Drive, from which the images are easily accesible.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;, force_remount=True)
root_dir = &amp;quot;/content/gdrive/My Drive/&amp;quot;
base_dir = root_dir + &#39;fastai-v3&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;amp;response_type=code

Enter your authorization code:
··········
Mounted at /content/gdrive
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folder = &#39;macbook&#39;
file = &#39;macbook.txt&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folder = &#39;hp&#39;
file = &#39;hp.txt&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;folder = &#39;lenovo&#39;
file = &#39;lenovo.txt&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Code has to be run once for every category.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;path = Path(base_dir+&#39;/data/images&#39;)
dest = path/folder
dest.mkdir(parents=True, exist_ok=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;path.ls()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/macbook.txt&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/macbook&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/lenovo.txt&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/hp&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/hp.txt&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/lenovo&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/models&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/cleaned.csv&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/export.pkl&#39;),
 PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/images/mactest.jpg&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, the files (txt files with urls of images) has to be uploaded into Drive.&lt;/p&gt;
&lt;p&gt;Once that is done, the images can be downloaded into Drive, into the specified folders, from the urls using the download_images function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;download_images(path/file, dest, max_pics=200)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;classes = [&#39;macbook&#39;,&#39;hp&#39;,&#39;lenovo&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can remove any images that cannot be opened:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for c in classes:
    print(c)
    verify_images(path/c, delete=True, max_size=500)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we can extract the images from the folders, and seperate them into training and validation sets, using the ImageDataBunch function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;np.random.seed(42)
data = ImageDataBunch.from_folder(path, train=&amp;quot;.&amp;quot;, valid_pct=0.2,
        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/fastai/data_block.py:534: UserWarning: You are labelling your items with CategoryList.
Your valid set contained the following unknown labels, the corresponding items have been discarded.
images
  if getattr(ds, &#39;warn&#39;, False): warn(ds.warn)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at some of the pictures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data.show_batch(rows=3, figsize=(7,8))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data.classes, data.c, len(data.train_ds), len(data.valid_ds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;([&#39;hp&#39;, &#39;lenovo&#39;, &#39;macbook&#39;], 3, 306, 75)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training the model, using the cnn_learner function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn = cnn_learner(data, models.resnet34, metrics=error_rate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Downloading: &amp;quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&amp;quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth
100%|██████████| 87306240/87306240 [00:00&amp;lt;00:00, 162957184.69it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;learn.fit_one_cycle(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.305020&lt;/td&gt;
      &lt;td&gt;0.848843&lt;/td&gt;
      &lt;td&gt;0.346667&lt;/td&gt;
      &lt;td&gt;00:54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1.121091&lt;/td&gt;
      &lt;td&gt;0.731948&lt;/td&gt;
      &lt;td&gt;0.293333&lt;/td&gt;
      &lt;td&gt;00:06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.956481&lt;/td&gt;
      &lt;td&gt;0.663035&lt;/td&gt;
      &lt;td&gt;0.293333&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.809013&lt;/td&gt;
      &lt;td&gt;0.651194&lt;/td&gt;
      &lt;td&gt;0.266667&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.718085&lt;/td&gt;
      &lt;td&gt;0.661706&lt;/td&gt;
      &lt;td&gt;0.240000&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;learn.lr_find(start_lr=1e-5, end_lr=1e-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interpreting the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.recorder.plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_26_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.fit_one_cycle(2,max_lr=slice(1e-03,1e-02))
&lt;/code&gt;&lt;/pre&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: left;&#34;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;error_rate&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.152411&lt;/td&gt;
      &lt;td&gt;0.790561&lt;/td&gt;
      &lt;td&gt;0.173333&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.102961&lt;/td&gt;
      &lt;td&gt;0.861176&lt;/td&gt;
      &lt;td&gt;0.186667&lt;/td&gt;
      &lt;td&gt;00:05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code&gt;interp = ClassificationInterpretation.from_learner(learn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;interp.plot_confusion_matrix()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_29_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;interp.most_confused(min_val=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(&#39;lenovo&#39;, &#39;hp&#39;, 4),
 (&#39;hp&#39;, &#39;macbook&#39;, 3),
 (&#39;lenovo&#39;, &#39;macbook&#39;, 3),
 (&#39;macbook&#39;, &#39;hp&#39;, 3)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lenovo&amp;rsquo;s are being mistaken for HP&amp;rsquo;s 4 times, but the reverse doesn&amp;rsquo;t seem to happen. Macbooks are the ones that are creating most of the error.&lt;/p&gt;
&lt;p&gt;Using an unused picture, and checking if our model can predict what laptop brand it is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn.export()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;defaults.device = torch.device(&#39;cpu&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;img = open_image(path/&#39;mactest.jpg&#39;)
img
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;learn = load_learner(path)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;pred_class,pred_idx,outputs = learn.predict(img)
pred_class
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Category macbook
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;img1  = open_image(path/&#39;hptest.jpg&#39;)
img1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/projectimages/LaptopBrandClassifier_files/LaptopBrandClassifier_37_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pred_class,pred_idx,outputs = learn.predict(img1)
pred_class
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Category hp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model is able to predict these new images perfectly as well.&lt;/p&gt;
&lt;p&gt;A very simple application to do something pretty complex.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What makes a good movie, asked Bayes</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/bayes/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/bayes/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/What-makes-a-good-movie-asked-Bayes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;how-to-make-a-good-movie&#34;&gt;How to make a good movie&lt;/h1&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;load-packages&#34;&gt;Load packages&lt;/h3&gt;
&lt;p&gt;Let us load the 4 packages needed for this analysis. ggplot2 and gridExtra are required for the data visualizations, dplyr is needed for data manipulation and wrangling, statsr consists of all the statistical functions needed, BAS has the Bayesian functions and MASS contains the stepAIC function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;library(ggplot2)
library(gridExtra)
library(dplyr)
library(statsr)
library(BAS)
library(MASS)

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;load-data&#34;&gt;Load data&lt;/h3&gt;
&lt;p&gt;The dataset can be loaded in two ways, either by using going to File-&amp;gt;Open File and clicking on the R Workspace file to load the data, or using the load() function. Here, we use the latter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;load(&amp;quot;movies.Rdata&amp;quot;)
dim(movies)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset imported has 651 rows and 32 columns.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-1-data&#34;&gt;Part 1: Data&lt;/h2&gt;
&lt;p&gt;Rotten Tomatoes and the TomatometerT rating is the most trusted measurement of quality entertainment. As the leading online aggregator of movie and TV show reviews from professional critics, Rotten Tomatoes offers the most comprehensive guide to what&amp;rsquo;s fresh. The world famous TomatometerT rating represents the percentage of positive professional reviews for films and TV shows and is used by millions every day, to help with their entertainment viewing decisions. Rotten Tomatoes designates the best reviewed movies and TV shows as Certified Fresh. That accolade is awarded with Tomatometer ratings of 75% and higher, and a required minimum number of reviews. Weekly Rotten Tomatoes podcasts can be found on RottenTomatoes.com, iTunes, Soundcloud and Stitcher, and Rotten Tomatoes&amp;rsquo; entertainment experts make regular TV and radio appearances across the US.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data Collection&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalizability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The present data were derived from an observational study. The data set is comprised of 651 randomly sampled movies produced and released from 1970 to 2014. According to IMDb, there have 9,962 movies been release from 1972 to 2016 so that the 10% condition (9,962*0.01 = 996) is met. Since the sampling size is large enough and less than 10% of population, it can assume that the random sampling is conducted. Therefore we can conclude that the sample is indeed generalizable to the entire population.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Causality&lt;/strong&gt;
The data cannot be used to establish a causal relation between the variables of interest as there was no random assignment to the explanatory and independent variables.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-2-data-manipulation&#34;&gt;Part 2: Data manipulation&lt;/h2&gt;
&lt;p&gt;In the original dataset , not all of the required features have been provided, so we will perform some feature engineering to create the required features. For the analysis, new features as oscar_season, summer_season, mpaa_rating_R, drama and feature_film. All of them can be derived from existing variables in the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;movies &amp;lt;-movies %&amp;gt;% mutate(feature_film = as.factor(ifelse(title_type == &amp;quot;Feature Film&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(drama = as.factor(ifelse(genre == &#39;Drama&#39;, &#39;Yes&#39;, &#39;No&#39;)))
movies &amp;lt;- movies %&amp;gt;% mutate(mpaa_rating_R=as.factor(ifelse(mpaa_rating==&amp;quot;R&amp;quot;,&amp;quot;Yes&amp;quot;,&amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(oscar_season=as.factor(ifelse(thtr_rel_month %in% c(&#39;10&#39;,&#39;11&#39;,&#39;12&#39;),&amp;quot;Yes&amp;quot;,&amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(summer_season=as.factor(ifelse(thtr_rel_month %in% c(&#39;6&#39;,&#39;7&#39;,&#39;8&#39;),&amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-3-exploratory-data-analysis&#34;&gt;Part 3: Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;Firstly, let us see the analyze the audience_score variable:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Audience Score:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$audience_score)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The lowest rating is 11 (Battlefield Earth) and the highest rating is 97.00 (The Godfather Part 2).
The median score is 65.00, with a mean of 62.36.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=movies,aes(x=audience_score)) + geom_histogram(binwidth=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a binwidth=5, we get a readable display. It is evident that this data is left-skewed, with more values on the right side of the mean than the left.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=movies,aes(x=audience_score)) + geom_density()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot clearly shows the left skew of the audience_score variable.&lt;/p&gt;
&lt;p&gt;Now, using the variables created in the previous section, plots and summary statistics make it easier to understand the data we have created.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Feature Film:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it is a Feature Film and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$feature_film)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows that there is mainly a huge majority of feature films. Actually, (591*100)/651= 90.738 % of the data are feature films.&lt;/p&gt;
&lt;p&gt;Plotting this data against audience_score and IMDB rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g1=ggplot(data=movies,aes(x=feature_film,y=audience_score,fill=feature_film)) +geom_boxplot()
g2=ggplot(data=movies,aes(x=feature_film,y=imdb_rating,fill=feature_film)) + geom_boxplot()
grid.arrange(g1,g2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there are fewer feature films, the distribution shows that feature films generally have a lower score than non-feature films. But this could also be attributed to the fewer number of feature films.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Drama:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it is a Drama movie and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;p&gt;Let us check the summary statistics for the drama variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$drama)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the number of drama and non-drama movies are close in count, but there are slightly more non-drama movies.&lt;/p&gt;
&lt;p&gt;Plotting this variable against audience_score and IMDB rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g3=ggplot(data=movies,aes(x=drama,y=audience_score,fill=drama)) +geom_boxplot()
g4=ggplot(data=movies,aes(x=drama,y=imdb_rating,fill=drama)) + geom_boxplot()
grid.arrange(g3,g4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There isn&amp;rsquo;t a huge difference but the non-drama movies have slightly lower media score compared to the drama movies. The non-drama movies are also slightly more distributed, but not by a whole lot.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MPAA Rating:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if the movies has an R MPAA Rating and a &amp;ldquo;No&amp;rdquo; value if it is not R-rated.&lt;/p&gt;
&lt;p&gt;The summary statistics for the MPAA rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$mpaa_rating_R)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The number of R-rated movies and movies with other ratings are very close.&lt;/p&gt;
&lt;p&gt;Plotting this variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g5=ggplot(data=movies,aes(x=mpaa_rating_R,y=audience_score,fill=mpaa_rating_R)) +geom_boxplot()
g6=ggplot(data=movies,aes(x=mpaa_rating_R,y=imdb_rating,fill=mpaa_rating_R)) + geom_boxplot()
grid.arrange(g5,g6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ratings are very similar for R-rated and non R-rated movies.&lt;/p&gt;
&lt;p&gt;Their distributions are also extremely similar, with not much to split the two variables.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Oscar Season:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it was released in the Oscar season and a &amp;ldquo;No&amp;rdquo; value if it was not released in the Oscar season.&lt;/p&gt;
&lt;p&gt;The summary of the Oscar variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$oscar_season)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are fewer movies that are released in the Oscar season and only about (191*100)/651=29.3394% are released in the Oscar season.&lt;/p&gt;
&lt;p&gt;Let us plot the variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g7=ggplot(data=movies,aes(x=oscar_season,y=audience_score,fill=oscar_season)) +geom_boxplot()
g8=ggplot(data=movies,aes(x=oscar_season,y=imdb_rating,fill=oscar_season)) + geom_boxplot()
grid.arrange(g7,g8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are slightly higher scores for the movies released in the Oscar Season, but the distributions seem similar.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Summer season:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it was released in the Oscar season and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;p&gt;The summary of the summer variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$summer_season)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, most movies are not released in the summer months. Only about (164*100)/651= 25.192% of the movies are released in the summer.&lt;/p&gt;
&lt;p&gt;Plotting this variable against ratings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g9=ggplot(data=movies,aes(x=summer_season,y=audience_score,fill=summer_season)) +geom_boxplot()
g10=ggplot(data=movies,aes(x=summer_season,y=imdb_rating,fill=summer_season)) + geom_boxplot()
grid.arrange(g9,g10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plots are almost identical, with very minute differences between them, if any. The movies not released in the summer season have very slightly higher scores, but the difference looks insignificant.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-4-modeling&#34;&gt;Part 4: Modeling&lt;/h2&gt;
&lt;p&gt;The best model is not always the most complicated. Sometimes including variables that are not evidently important, can actually reduce the accuracy of predictions. In practice, the model that includes all available explanatory variables is often referred to as the full model. The full model may not be the best model, and if it isn&amp;rsquo;t, we want to identify a smaller model that is preferable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Full model:&lt;/em&gt;
audience_score ~ feature_film + drama + runtime + mpaa_rating_R + thtr_rel_year + oscar_season + summer_season + imdb_rating + imdb_num_votes + critics_score + best_pic_nom + best_pic_win + best_actor_win + best_actress_win + best_dir_win + top200_box&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayesian Model Averaging (BMA):&lt;/em&gt;
A comprehensive approach to address model uncertainty is Bayesian model averaging, which allows us to assess the robustness of results to alternative specifications by calculating posterior distributions over coefficients and models. Given the 17 features (n) there can be 2^n = 2^17 possible models. We will explore model uncertainty using posterior probabilities of models based on BIC.
We will use BIC as a way to approximate the log of the marginal likelihood. The Bayesian information criterion (BIC) runs through several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2log-likelihood + nparlog(nobs), where npar represents the number of parameters and nobs the number of observations in the fitted model.&lt;/p&gt;
&lt;p&gt;Seperating the required features into a seperate dataframe:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;features &amp;lt;- c(&#39;audience_score&#39;, &#39;feature_film&#39;, &#39;drama&#39;, &#39;runtime&#39;, &#39;mpaa_rating_R&#39;, &#39;thtr_rel_year&#39;, &#39;oscar_season&#39;, &#39;summer_season&#39;, &#39;imdb_rating&#39;, &#39;imdb_num_votes&#39;, &#39;critics_score&#39;, &#39;best_pic_nom&#39;, &#39;best_pic_win&#39;, &#39;best_actor_win&#39;, &#39;best_actress_win&#39;, &#39;best_dir_win&#39;,&#39;top200_box&#39;)
moviesmodel=movies[ , features]
summary(moviesmodel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seem to be no NA values, so we can proceed with the model selection process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian Information Criterion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we create a multiple linear regression model, with all the factors included.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audmodel=lm(audience_score~. , data=moviesmodel)
summary(audmodel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very evident that the number of factors that are not useful is very high. We can use the BIC (Bayesian Information Criterion) to eliminate the factors that are not significant in this model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audienceBIC=bas.lm(audience_score~ ., data=moviesmodel,prior=&amp;quot;BIC&amp;quot;,modelprior=uniform())
audienceBIC
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These values denote the marginal posterior inclusion probabilities. We can actually see that IMDB rating and critics score do play a big role.&lt;/p&gt;
&lt;p&gt;From this object, we can get the top 5 most probable models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(audienceBIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the most probable model contains only 3 variables, runtime, IMDB score and Critics score. The second most probable model contains 2 variables, IMDB score and Critics score.&lt;/p&gt;
&lt;p&gt;The posterior probability for the top 2 most probably models are about 27%.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Coeffecients:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can extract the coeffecients from the Bayesian model into a seperate variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audiencecoeff=coef(audienceBIC)
#95% Credible Intervals for coeffecients:
audinterval=confint(audiencecoeff)
audinterval
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us look at what the 3 most important variables mean.&lt;/p&gt;
&lt;p&gt;For every 1 point increase in the runtime, the audience score is -2.5e-02 minutes lesser. Similarly, for every 1 point increase in the IMDB rating the audience score is 1.49e+01 points more. And for the Critics score there is an audience score increase of 6.33e-02 points.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Model space:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can visualize the model space using the image() function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;image(audienceBIC,rotate=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to size constraints, all 17 variables are not shown in this picture. By opening this plot in a new window, all 17 variables are visible and it is evident that &amp;lsquo;runtime + imdb_rating + critics_score&amp;rsquo; is the best model. Also imdb rating and critics score are present in all the top models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zellner-Siow Cauchy:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using Zellner-Siow Cauchy, with an MCMC method, we can get a different model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audiencezs=bas.lm(audience_score~ .,  data=moviesmodel,prior=&amp;quot;ZS-null&amp;quot;,modelprior=uniform(),method=&amp;quot;MCMC&amp;quot;)
summary(audiencezs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the most probable model, with a posterior probability of 14% has only IMDB rating and Critics score. And the 2nd most probable model, with a posterior probability of 12% has runtime, IMDB rating and Critics score. These results are very similar to the BIC method, with only a swap in the first two models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Model space:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can visualize the models created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;image(audiencezs,rotate=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, expanding the image, we can see that IMDB rating and Critics score are the major factors. Here, runtime doesn&amp;rsquo;t seem to be playing a huge role.&lt;/p&gt;
&lt;p&gt;So we can clearly see that while BIC proposes 3 variables (runtime, imdb_rating, critics_score), the ZSC method only proposes 2 (imdb_rating, critics_score).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AIC Model Selection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Aikake information criterion (AIC) is a measure of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Hence, AIC provides a means for model selection.&lt;/p&gt;
&lt;p&gt;We can use backward elimination to find the best model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;lmaic=lm(audience_score~ ., data=moviesmodel)
audienceaic=stepAIC(lmaic,direction=&#39;backward&#39;,trace=FALSE)
audienceaic$anova
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that there are a lot more variables in this method: summer_season, top200_box, best_dir_win, best_pic_win, oscar_season, feature_film, drama, imdb_num_votes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audienceaic$coefficients
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, for example, for every 1 point increase in the IMDB rating, the Audience Score increases by 15 points, which is a lot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Final Model:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our final model, we are going to use IMDB ratings and Critics score as the two variables, with the Zellner-Siow Cauchy prior, and MCMC method, with 10^6 iterations of MCMC.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;feat=c(&amp;quot;audience_score&amp;quot;,&amp;quot;imdb_rating&amp;quot;,&amp;quot;critics_score&amp;quot;)
moviesfinal=movies[,feat]
audiencezsfin=bas.lm(audience_score~.,data=moviesfinal,prior=&amp;quot;ZS-null&amp;quot;,modelprior=uniform(),method=&amp;quot;MCMC&amp;quot;,MCMC.iteration=10^6)
summary(audiencezsfin)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that in this, the first model, which has both variables, has 89% posterior probability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model Diagnostics:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will now look at the best model created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;diagnostics(audiencezs, type = &amp;quot;model&amp;quot;, pch = 16, cex = 1.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It shows about a straight line at the intercept, which shows a converged posterior probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which = 1, pch=16)
abline(a = 0, b = 0, lwd = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seem to be few outliers, but the points are randomly scattered across the 0 line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which=2,add.smooth = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability starts to straigthen around the 800th model appx, meaning all the models after that don&amp;rsquo;t make much of a difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which=3, ask=F)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These log marginal probabilites are pretty evenly distributed, somewhat favouring 4 or 5 factors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which = 4, ask = F, col.in = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see again, that imdb_rating and critics_score matter the most.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-5-prediction&#34;&gt;Part 5: Prediction&lt;/h2&gt;
&lt;p&gt;The movie we are going to pick is Money Monster, a movie directed by Jodie Foster, starring George Clooney and Julia Roberts. (&lt;a href=&#34;https://www.rottentomatoes.com/m/money_monster/&#34;&gt;https://www.rottentomatoes.com/m/money_monster/&lt;/a&gt;) The audience score is 60%.&lt;/p&gt;
&lt;p&gt;We create the BMA object first:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;BMA=predict(audiencezsfin,estimator=&amp;quot;BMA&amp;quot;, se.fit=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a data frame with the values to be predicted:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;pred=data.frame(imdb_rating=6.5,critics_score=58)
aud=predict(audiencezsfin,newdata=pred,estimator=&amp;quot;BMA&amp;quot;,se.fit=TRUE)
aud$fit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get an estimated audience score of 62.49, which is a little higher than the actual score&lt;/p&gt;
&lt;p&gt;The 95% credible interval is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audinterval=confint(aud,parm=&amp;quot;mean&amp;quot;)
round(audinterval,3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a confidence interval close to the 60% we were looking for.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-6-conclusion&#34;&gt;Part 6: Conclusion&lt;/h2&gt;
&lt;p&gt;The predictive model presented here is used to predict the audience scores for a movie. Using Bayesian model averaging and many factors like BIC, ZSC, AIC, etc, many models can be constructed to perform better predictions.&lt;/p&gt;
&lt;p&gt;The proposed linear model shows a &amp;lsquo;fairly good&amp;rsquo; prediction rate, but it should be noted that the model is based on a very small sample. The fact is that imdb_rating has the highest posterior probability, and that basically all of the newly created features were not that useful to support a better prediction. Creating a model, which has a high predictive power is not so easy to reach. Using Bayes for better prediction is only one part of the game. It might be beneficial to gather more data or try to extend the feature engineering part, which means to creating new meaningful features from existing or gather data for new features.&lt;/p&gt;
&lt;p&gt;Perhaps in a future project, for higher accuracy, we could have included all the remaining factors as well, which was done in the project for the 3rd course of this specialization, and then eliminated them one by one. Even though such models might be prone to overfitting or underfitting, these problems can certainly be mitigated using expert opinion on which factors are actually useful.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Literacy Rate Analysis using GSS Data</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/project/litrates/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/project/litrates/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/Literacy-Rate-Analysis-Using-GSS-Dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;load-packages&#34;&gt;Load packages&lt;/h3&gt;
&lt;p&gt;For this project, we require ggplot2 and gridExtra package for the plots, dplyr for data manipulation and statsr for the statistical functions used in this course. These three packages contain a wide range of functions to be used, and should encompass all the functions we require to do this project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;library(ggplot2)
library(gridExtra)
library(dplyr)
library(statsr)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;load-data&#34;&gt;Load data&lt;/h3&gt;
&lt;p&gt;We can either go to File -&amp;gt; Open File and select our RData file or load the RData file as it is. This automatically imports our gss dataset into the workspace, as gss. The latter method is used here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;load(&amp;quot;gss.Rdata&amp;quot;)
dim(gss)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now loaded a dataset with 57061 rows and 114 columns.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-1-data&#34;&gt;Part 1: Data&lt;/h2&gt;
&lt;p&gt;According to the GSS website, the General Social Survey (GSS) has studied the growing complexity of American society. It is the only full-probability, personal-interview survey designed to monitor changes in both social characteristics and attitudes currently being conducted in the United States.&lt;/p&gt;
&lt;p&gt;The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.&lt;/p&gt;
&lt;p&gt;As to how the data is collected, the GSS sample is drawn using an area probability design that randomly selects respondents in households across the United States to take part in the survey. The respondents are from a mix of urban, suburban and rural geographic areas.
Therefore, random sampling does take place in this survey, meaning the results of the survey can to an extent, be generalized to the adult population of the United States.&lt;/p&gt;
&lt;p&gt;There are a few things which might not make it completely accurate. Firstly, the GSS is strictly voluntary, meaning even when selected, participants may choose to not attend the survey. Secondly, up until a few years ago, only English speaking participants were chosen, and now Spanish speaking participants have been added. This raises another concern about other languages, as well.&lt;/p&gt;
&lt;p&gt;Random assignment has not been used here to seperate the sampled population into further groups, which means that causality cannot be inferred from the results of this survey, since we cannot be sure that the only difference between different groups is what we are studying.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-2-research-question&#34;&gt;Part 2: Research question&lt;/h2&gt;
&lt;p&gt;With such a vast trove of data, it is possible to create many interesting and insightful research questions, each with their own meaningful conclusions.&lt;/p&gt;
&lt;p&gt;Here, I will focus on one particular area: education levels. We will analyzing 3 questions based on this area:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 1: As of 2012 (latest year of the survey), is there a disparity in the education provided to males and females?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, for the 2012 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 2012?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 2: In 1972 (first year of the survey), was there a disparity in the education provided to males and females?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, for the 1972 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 1972?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 3: Is there a difference in the education levels of 1972 and 2012?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taking gender out of the equation, is there a difference in education levels of the years 1972 and 2012? Has the situation of education improved or declined in those 50 years?&lt;/p&gt;
&lt;p&gt;This is becoming an all-important question in today&amp;rsquo;s world, with rising calls for equality between men and women. Education is an essential part of gaining knowledge, and an equal footing in education can open up equal opportunities, for men and women. Educational reforms have also been implemented over the years, to provide a higher level of education for all. But has education actually improved, overall?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-3-exploratory-data-analysis&#34;&gt;Part 3: Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Question 1:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First let us use only the section of the data that needs to be used, the 2012 data, by filtering it into another dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012 = gss %&amp;gt;%
  filter(year==&amp;quot;2012&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2012 is the latest year of the survey, providing us with the latest results to this research question.&lt;/p&gt;
&lt;p&gt;Let us see a summary of the education levels in the new subset of data we have:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(gss2012$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the mean is 13.53, the maximum is 20, and the minimum is 0, with a median of 13.
There are also 2 NA&amp;rsquo;s, out of 1974 entries, which is around 1% of the dataset, so we can remove those 2 rows to increase accuracy without affecting the accuracy of the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012= gss2012 %&amp;gt;%
  filter(educ!=&amp;quot;NA&amp;quot;)
summary(gss2012$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the NA values have been successfully removed.&lt;/p&gt;
&lt;p&gt;Let us look at the means and medians of education levels, grouped by gender:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012 %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarise(meaned=mean(educ),medec=median(educ))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of female education level is slightly higher than males, and the median is the same for both.&lt;/p&gt;
&lt;p&gt;Let us create a simple boxplot to visualise these statistics, helping us get a better understanding:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss2012 ,aes(x=factor(sex),y=educ)) + geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The boxplot does not seem to show much of a change between the two genders, with the two boxplots looking almost identical.&lt;/p&gt;
&lt;p&gt;A barplot to compare the counts of the education levels, with grouping done on gender might help us understand the data better:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss2012,aes(x=educ,fill=sex)) +geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that as the number of years of education is more than 10, females tend to get more education that men in that education level, except on one or two levels, but the difference is not a lot in most cases.&lt;/p&gt;
&lt;p&gt;Looking at the maximum and minimum, there are slightly more females than males who get 0 years of education, but at the maximum of 20 years, the difference is almost non-existent.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 2:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let us now look at the data from 1972, by subsetting it into a seperate data frame, and doing a similar EDA on it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972=gss %&amp;gt;%
  filter(year==&amp;quot;1972&amp;quot;)
summary(gss1972$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get 5 NA values out of a total of 1613 values, which is around 3% of the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972 = gss1972 %&amp;gt;%
  filter(gss1972$educ!=&amp;quot;NA&amp;quot;)
summary(gss1972$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Already there is a clear difference in the median and mean, without looking at the gender difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972 %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarise(meaned=mean(educ),medianed=median(educ))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of education levels of females was lower than the males in 1972, a sharp contrast to the results in 2012.&lt;/p&gt;
&lt;p&gt;Let us see a simple boxplot to visualise these statistics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss1972,aes(x=sex,y=educ)) + geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very evident from this that, although the male median education level and female median education level are almost the same, the average education level for males is definitely higher, and the 75th percentile level is much higher than that of females.&lt;/p&gt;
&lt;p&gt;Creating a plot to see each year of education:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss1972,aes(x=educ,fill=sex)) +geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In 1972, it is evident that beyond 14 years of education, there are more men than women. These results are much different to the 2012 results.&lt;/p&gt;
&lt;p&gt;Comparing the two datasets using scatterplots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g1=ggplot(data=gss1972,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(&amp;quot;1972&amp;quot;)
g2=ggplot(data=gss2012,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(&amp;quot;2012&amp;quot;)
grid.arrange(g1,g2,ncol=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 2 scatter plots on the left are the 1972 data, arranged according to sex, and the 2 scatter plots on the right are the 2012 data, arranged according to sex. It is clear that in 2012, there is a higher concentration at a higher education level, whereas in 1972, that is only for males.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 3:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, to analyse the difference in education levels overall, between 1972 and 2012.&lt;/p&gt;
&lt;p&gt;First let us combine them into a single dataset to make the analysis easier:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gssdata=rbind(gss1972,gss2012)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already have the datasets as well as their summary statistics, so all that is left is to plot them:&lt;/p&gt;
&lt;p&gt;Creating a simple boxplot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gssdata,aes(x=factor(year),y=educ))+geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Immediately, it is evident that there is a significant difference in the education levels between the two years. Although the median of 2012 is only slightly higher than 1972, the range and IQR of 2012 is much higher.&lt;/p&gt;
&lt;p&gt;Let us create a bar graph between the two years:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gssdata,aes(x=educ,fill=factor(year)))+geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upto about 12 years of education, 1972 had a higher proportion, but after 12 years, 2012 had the higher proportion. This means that more people are getting more education in 2012, compared to 1972.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-4-inference&#34;&gt;Part 4: Inference&lt;/h2&gt;
&lt;p&gt;First, let us check the conditions for doing a hypothesis test using the CLT method:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.Independence:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Within groups:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Random sampling was used.&lt;/p&gt;
&lt;p&gt;1972 observations is well below 10% of the population.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Between groups:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The two groups are independent of each other, as it is a representative of the population. Also the likelihood of dependence otherwise, is very small with this sample size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.Sample Size/Skew:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sample is slightly skewed, and n is well above 30.&lt;/p&gt;
&lt;p&gt;Therefore the conditions for CLT are satisfied and we can use the theoretical method.&lt;/p&gt;
&lt;p&gt;The method to be used for all 3 questions is that to be used when comparing 2 independent means, as independence has already been established. The critical score will be the t-score corresponding to the degree of freedom. The degree of freedom is the $df =min(n_1 -1 ,n_2 - 1)$. Here, $n_1$=884 and $n_2$=1088, so the degree of freedom is 883.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Question 1:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 2012.
$\mu_{male(2012)}-\mu_{female(2012)}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 2012.
$\mu_{male(2012)}-\mu_{female(2012)} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{male(2012)}$ and $\mu_{female(2012)}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{male(2012)}$ and $\bar{x}_{female(2012)}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss2012,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results:
Response variable: numerical
Explanatory variable: categorical (2 levels)
n_Male = 884, y_bar_Male = 13.5057, s_Male = 3.1721
n_Female = 1088, y_bar_Female = 13.546, s_Female = 3.0904
H0: mu_Male = mu_Female
HA: mu_Male != mu_Female
t = -0.2838, df = 883
p_value = 0.7766&lt;/p&gt;
&lt;p&gt;The high p-value of 0.7766 which is much greater than 0.05, means we &lt;strong&gt;fail to reject the null hypothesis $H_0$&lt;/strong&gt;. We might still run the risk of a type 2 error, but the big p-value offsets the effects of a larger significance level. What the p-value here indicates is the probability of observing extreme data given that the null hypothesis is true, is high.&lt;/p&gt;
&lt;p&gt;We can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss2012,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This 95% Confidence Interval means that we are 95% confident that the difference in the education levels of males and females is between -0.319 and 0.2384. The - sign shows that females getting more education that males.&lt;/p&gt;
&lt;p&gt;This confidence interval method is well in agreement with the hypothesis test method as the 0 is in the confidence interval that has been produced. The differences shown by the Confidence Interval method are also not that significant, which are the same results that can be inferred from the hypothesis test method.&lt;/p&gt;
&lt;p&gt;In conclusion, the results of both the confidence interval method and the hypothesis test method indicate that &lt;strong&gt;there is an insignificant difference in the education levels of males and females in 2012.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 2:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 1972.
$\mu_{male(1972)}-\mu_{female(1972)}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 1972.
$\mu_{male(1972)}-\mu_{female(1972)} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{male(1972)}$ and $\mu_{female(1972)}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{male(1972)}$ and $\bar{x}_{female(1972)}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss1972,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this inference, we get a p-value of 0.01, which is much lower than our significance level of 0.05, which means we &lt;strong&gt;reject the $H_0$ hypothesis&lt;/strong&gt;. Again, we run the risk of a Type-1 error, but even a smaller significance level will not give a different result.&lt;/p&gt;
&lt;p&gt;We can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss1972,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results show that males get about 0.07 to 0.75 years more education then females. Although this does not seem like a lot, since it is above the significance level of 0.05, it is a significant difference, as the difference when extrapolated to the entire population, becomes much bigger.&lt;/p&gt;
&lt;p&gt;The conclusion from this test is that &lt;strong&gt;there was a significant difference in the education levels of males and females in 1972&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 3:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 2012.
$\mu_{1972}-\mu_{2012}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 2012.
$\mu_{1972}-\mu_{2012} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{1972}$ and $\mu_{2012}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{1972}$ and $\bar{x}_{2012}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=factor(year),data=gssdata,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value obtained is extremely small, &amp;lt;0.0001, which is obviously lesser than the significance level of 0.05, hence we &lt;strong&gt;reject the $H_0$ hypothesis&lt;/strong&gt;. The risk of a type 1 error is even smaller here, as it is such a small p-value.&lt;/p&gt;
&lt;p&gt;A confidence interval can also be created similar to the above questions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=factor(year),data=gssdata,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confidence interval indicates that there is a 95% chance that on average, people in 2012 had an 1.9825 to 2.4191 more years of education than people in 2012. This is in agreement with the hypothesis test done earlier.&lt;/p&gt;
&lt;p&gt;Therefore, it can be concluded that &lt;strong&gt;there is a significant difference in the education levels of 1972 and 2012&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-5-conclusion&#34;&gt;Part 5: Conclusion&lt;/h2&gt;
&lt;p&gt;It can be concluded that:
(1) There is an insignificant difference in the education levels of males and females in the year 2012, with a 95% confidence interval of (-0.319,0.2384).&lt;/p&gt;
&lt;p&gt;(2) There is a significant difference in the education levels of males and females in the year 1972, with a 95% confidence interval of (0.0783,0.7542).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Football Revolution and Why It’s Almost Here</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/post/footballrevolution/</link>
      <pubDate>Mon, 02 Jul 2018 12:10:02 +0000</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/post/footballrevolution/</guid>
      <description>&lt;p&gt;This World Cup season, the whole world, football fans or not, are abuzz about the ongoing tournament being held in Russia. (Ah yes, that football.) So why not learn a bit more about ‘The Beautiful Game’ to try and show off to your die-hard football friends and family members, eh?&lt;/p&gt;
&lt;p&gt;Let’s start somewhere completely unrelated.&lt;/p&gt;
&lt;p&gt;Brad Pitt.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/0__hYYRCQ0jHL32T5WG.jpg&#34; &gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/0__hYYRCQ0jHL32T5WG.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Do I have your attention now?&lt;/p&gt;
&lt;p&gt;Now, what does this annoyingly good-looking specimen have to do with football? I’ll get to that.&lt;/p&gt;
&lt;p&gt;You may know him as the lead actor in some of those iconic movies you’ve seen, such as Fight Club, Inglourious Basterds, the Ocean’s trilogy, Se7en, Mr and Mrs. Smith, to name a few.&lt;/p&gt;
&lt;p&gt;Or you may know him as the person who has been in relationships with Hollywood’s most famous actresses like Jennifer Aniston and Angelina Jolie. Whatever the means, chances are you’ve heard of him.&lt;/p&gt;
&lt;p&gt;Speaking of chances, Brad Pitt has also acted in a lesser known 2011 film called Moneyball.&lt;/p&gt;





  











&lt;figure id=&#34;figure-brad-pitt-in-moneyball-2011&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__CrOuAJrcAi__5D5mwMNA9FQ.jpeg&#34; data-caption=&#34;Brad Pitt in Moneyball (2011)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__CrOuAJrcAi__5D5mwMNA9FQ.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Brad Pitt in Moneyball (2011)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;He plays Billy Beane, the General Manager of the Oakland Athletics’, a baseball team in the Major League Baseball in the USA. &lt;strong&gt;The story revolves around his role in reinventing the team using an experimental technique based on statistics, with the help of a Yale economics graduate, to help them win the league with a significantly lesser amount of money.&lt;/strong&gt; The movie, nominated for an Oscar, is actually based on a book, written about the events which actually transpired during the 2002 season, during which (spoiler alert) &lt;strong&gt;the Oakland A’s defied all odds to win the league with a record breaking 20 game winning streak.&lt;/strong&gt;&lt;/p&gt;





  











&lt;figure id=&#34;figure-the-record-breaking-2002-oaklandas&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__oHTrEKZeP85WYQcC7L3Uxg.jpeg&#34; data-caption=&#34;The record breaking 2002 Oakland A’s&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__oHTrEKZeP85WYQcC7L3Uxg.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The record breaking 2002 Oakland A’s
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;What did Billy Beane do? He threw the old methods of intuition and expertise in the trash and built a team from scratch purely using statistics for the 2002 season. He knew he didn’t have the budget that the bigger teams such as the Boston Red Sox ($108 million) and the New York Yankees ($125 million). In fact, they had the 3rd lowest budget for that season with just $39 million.&lt;/p&gt;





  











&lt;figure id=&#34;figure-oakland-as-had-the-3rd-lowest-budget-in-theleague&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__801pKgbYtI1GoZqtYf4fDQ.png&#34; data-caption=&#34;Oakland A’s had the 3rd lowest budget in the league&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__801pKgbYtI1GoZqtYf4fDQ.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Oakland A’s had the 3rd lowest budget in the league
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;But they used that to their advantage and bought players that scouts had previously overlooked because of their physical quirks such as throwing the ball an unconventional way or having a weird batting stance, but in fact had excellent statistics. He trusted in the statistics despite opposition from almost everyone else managing the team.&lt;/p&gt;
&lt;p&gt;And mind you, this was in 2002, before computers and technology was as prevalent as it is now. But he preserved, and he built a dream team to win the league, with minimal financial aid.&lt;/p&gt;
&lt;p&gt;And this started a revolution in the sports world.&lt;/p&gt;
&lt;p&gt;Soon, teams from all different sports began to adopt this “Moneyball” technique. It became a commonly used term within sporting professionals, referring to using statistics to revamp a team. And with advancements in technology, this began to be more and more of a trend, popping up all over the world in all different sports.&lt;/p&gt;
&lt;p&gt;Currently, the leading field in statistical analytics has to be machine learning. Machine learning is a prerequisite to Artificial Intelligence, a term you’re sure to have heard of, thanks to Siri, Google Assistant and Cortana. Machine learning is basically defined as the subfield in computer science to give computers the ability to learn without being programmed explicitly, which is basically what Artificial Intelligence deals with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what do Siri and Cortana have to do with sports?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Right now, everything.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To give a few examples, the Kolkata Knight Riders, in the Indian Premier League which is a cricket tournament held in India annually, used the help of SAP Labs and an auction analytics tool to give insights on impact players to buy during the 2014 season, which they then went on to win.&lt;/p&gt;
&lt;p&gt;Most teams in the National Football League (NFL) in the USA use machine learning algorithms to pick players during the drafts.&lt;/p&gt;
&lt;p&gt;Predicting match outcomes using such algorithms is becoming a huge trend, with big players such as Microsoft getting into the act and becoming a frontrunner in predicting NBA and NFL matches.&lt;/p&gt;
&lt;p&gt;But one sport almost seemingly untouched by this revolution is football (yes yes, soccer). The reason being that is a much more complicated sport with a larger number of variables, making it more and more difficult to implement the same tactics used in other sports.&lt;/p&gt;
&lt;p&gt;Predicting football games using the complex data and getting successful outcomes has become a trend, with many using it to bet on games based on the results of various simulations. UK sports betting company Stratagem is using artificial intelligence (deep neural networks, to be precise) to analyze patterns found in football matches, use them to predict outcomes of matches using the proprietary data, and possibly, earn a bit more than usual.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__uSlhcWavKzce5XIAqZDmrQ.jpeg&#34; &gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__uSlhcWavKzce5XIAqZDmrQ.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;For such a money driven sport, it’s a surprise that statistical analysis hasn’t yet been implemented on a large scale.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Despite all the negative criticism of using a statistical intervention in football clubs, it is evident that the sport will soon be turning towards that direction and there are clear signs showcasing exactly that. There have been many attempts in the past, but none effective enough to make a big enough impact.&lt;/p&gt;
&lt;p&gt;Immediately after the release of the movie, John W. Henry, who had implemented this “Moneyball” technique in the Boston Red Sox as its owner (the beginning of which are showcased at the end of the movie) and had a huge success, looked to do that with his investment in the English Premier League, Liverpool FC. As owner of Liverpool FC, he had hopes of making Liverpool the Boston Red Sox of the Premier League. He even tried to bring on the legendary Billy Beane himself as an advisor.&lt;/p&gt;





  











&lt;figure id=&#34;figure-john-w-henry-owner-of-the-boston-red-sox-and-liverpool-fc&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__Fzv0LD7__0KxJnsICC9LuaA.jpeg&#34; data-caption=&#34;John W. Henry, owner of the Boston Red Sox and Liverpool FC&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__Fzv0LD7__0KxJnsICC9LuaA.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    John W. Henry, owner of the Boston Red Sox and Liverpool FC
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;He brought in Sabermetrics analysts at Anfield, Liverpool’s home, scrutinising every aspect of the team’s game, from current player performances to tactical moves. Liverpool immediately signed on Jose Enrique, a left full-back, only for the reason that he had a high pass completion rate and that he was one of the best players in the league to break into the attacking third of the field. But this soon turned into a highly expensive campaign and after about 18 unsuccessful months, this system was called off&lt;/p&gt;
&lt;p&gt;Even smaller teams in the Premier League, like Stoke City FC, who had been trying to implement similar tactics, gave in and the method became a distant reality for the game of football and people soon forgot about such methods.&lt;/p&gt;





  











&lt;figure id=&#34;figure-jose-enrique-had-a-disappointing-spell-at-liverpool&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__deEVoKILe4VP__yGRHIe7hw.jpeg&#34; data-caption=&#34;Jose Enrique had a disappointing spell at Liverpool&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__deEVoKILe4VP__yGRHIe7hw.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Jose Enrique had a disappointing spell at Liverpool
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Enter FC Midtjylland.&lt;/p&gt;
&lt;p&gt;Can’t even pronounce it, can you? (It’s Mid-Jee-Lund)&lt;/p&gt;
&lt;p&gt;A small Danish club, on the brink of relegation to lower leagues, was bought and revamped by Matthew Benham. Matthew Benham is not your typical football club owner. He made himself a fortune by using mathematical models to bet on football matches, and eventually bought him childhood team, Brentford, a lowly League One team, languishing in the lower leagues of English Football. A strong believer in the “Moneyball” technique, he revamped the entire team based on his pure belief in statistics, very similar to Billy Beane. Brentford were promoted to the Championship that season and very nearly clinched promotion into the Premier League the very next season.&lt;/p&gt;
&lt;p&gt;Matthew Benham then went on to buy FC Midtjylland, a team barely even heard of. He used the same techniques there, and had an even greater success. In a fairytale story (very similar to the Oakland A’s of 2002), within a matter of a season, FC Midtjylland went on to win the Danish Superliga title, their very first major trophy. &lt;strong&gt;The match which sealed the deal, a 2–0 win against FC Copenhagen, is said to have been the one of the best matches ever played tactically and was a true testament to the statistical revolution they underwent.&lt;/strong&gt; Their story went viral when they were matched up with Premier League team Southampton FC, who were strong contenders, in the Europa League of 2015, and beat them convincingly.&lt;/p&gt;





  











&lt;figure id=&#34;figure-fc-midtjylland-pulled-off-an-impossible-feat-purely-with-statistics&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__w79V1PhJkboWmSYzFKRtLw.jpeg&#34; data-caption=&#34;FC Midtjylland pulled off an impossible feat purely with statistics&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__w79V1PhJkboWmSYzFKRtLw.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    FC Midtjylland pulled off an impossible feat purely with statistics
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;At Brentford, Benham’s team identified underrated players and transformed them into superstars for the team. As an example, they made a staggering profit on striker Andre Grey, who was signed for just £500,000, and was eventually sold for a massive £9,000,000. Andre Grey was identified by Benham’s statistical team.&lt;/p&gt;
&lt;p&gt;Rasmus Ankersen, who works alongside Benham at both clubs, said: “I met Matthew a few years ago. At the time, Brentford were third in League One and there were a couple of games to go. &lt;strong&gt;I said to him, ‘What are the chances of getting promoted?’ When you ask that question you expect an emotional ‘yes’ or ‘no’ from a football owner. But he just said rationally, ‘At the moment, there’s a 42.3 per cent chance we will get promoted’.&lt;/strong&gt; I knew then he was a guy who was thinking very differently about football than I have ever ­experienced before.”&lt;/p&gt;
&lt;p&gt;Matthew Benham has been hailed as the Billy Beane of modern day football.&lt;/p&gt;





  











&lt;figure id=&#34;figure-matthew-benhamleft-with-rasmusankersen&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1____jY5rV9U__L9DaZ9zIBWGXQ.jpeg&#34; data-caption=&#34;Matthew Benham(left) with Rasmus Ankersen&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1____jY5rV9U__L9DaZ9zIBWGXQ.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Matthew Benham(left) with Rasmus Ankersen
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Next, a team you’ve probably heard of.&lt;/p&gt;
&lt;p&gt;Leicester City FC.&lt;/p&gt;
&lt;p&gt;I’m disappointed if you can’t pronounce this one.&lt;/p&gt;
&lt;p&gt;Surprise winners of the Premier League title in the 2015/16 season who didn’t use the exact same tactics as Matthew Benham but they did have possibly the greatest underdog story of all time. At 5000–1 odds of winning the Premier League at the start of the season, they pulled off one of the greatest upsets in football history.&lt;/p&gt;
&lt;p&gt;Their best players were bought at terribly low prices and were previously unheard of. N’Golo Kante, who was playing for a team called Caen in France, was bought for a meagre £5.8 million. Later, he was sold to Chelsea FC for a whopping £30 million. The team’s top two scorers, Jaime Vardy and Riyad Mahrez were bought for £1 million from a non league team in 2012 and £400,000 from a French Ligue 2 team in 2014 respectively. Now, Vardy has a £20 million transfer value and Mahrez with £15 million.&lt;/p&gt;





  











&lt;figure id=&#34;figure-the-trio-that-won-leicester-the-premier-leaguel-to-r-kante-vardymahrez&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__U5xkYHZ__lcZda6g5aFFz__w.jpeg&#34; data-caption=&#34;The trio that won Leicester the Premier League(L to R: Kante, Vardy, Mahrez)&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__U5xkYHZ__lcZda6g5aFFz__w.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The trio that won Leicester the Premier League(L to R: Kante, Vardy, Mahrez)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Even though it doesn’t follow the same statistical beliefs that Brentford and FC Midtjylland used, it is a very similar approach and eventually, had an even more successful result. These are all small-time examples of how statistics and analytics can influence the game of football. But it has not been implemented on a large scale yet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Then the question must arise; if small teams with a lesser amount of money can get significant gains, why don’t the bigger teams with larger amounts of money do the same and get an even bigger profit?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Because they have a lot more to lose. By injecting a lot of money, the owners of the big teams are afraid of using experimental methods which don’t have a guaranteed success rate.&lt;/p&gt;
&lt;p&gt;Recently, private equity firm Vista invested a huge amount into STATS, a leading sports data and technology company. STATS is at the forefront of football revolution, providing newer methods to efficiently and accurately track player performance as well as make predictions and suggest tactics which could help teams get an edge over one another. They recently used Spatiotemporal data to track teams in the Premier League, and published a paper on the same, which has reached critical acclaim in the footballing world.&lt;/p&gt;





  











&lt;figure id=&#34;figure-image-of-ball-movement-patterns-in-the-20-premier-league-teams-in-the-201011season&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__dtZE__VuNFKfsEF5t184PXQ.png&#34; data-caption=&#34;Image of ball movement patterns in the 20 Premier League teams in the 2010–11 season&#34;&gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__dtZE__VuNFKfsEF5t184PXQ.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Image of ball movement patterns in the 20 Premier League teams in the 2010–11 season
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;With more and more advancements in technology itself, it’s a given that there has been a major improvement in the field of football analytics and there have been substantial results to show for it. Perhaps it is time for the teams, big and small, to adapt with the times and give in to this wave of reform. It is without a doubt that the way football itself is viewed will undergo a drastic shift over the next decade.&lt;/p&gt;
&lt;p&gt;And who knows what the future holds? Maybe Brad Pitt will be playing the role of a young football manager on the big screen soon enough.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__YAu__sgUdlSSg5qnEQlMwMg.jpeg&#34; &gt;


  &lt;img src=&#34;https://deshmukhakshay321.github.io/cloneakshay123/img/1__YAu__sgUdlSSg5qnEQlMwMg.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Or maybe not.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>https://deshmukhakshay321.github.io/cloneakshay123/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://deshmukhakshay321.github.io/cloneakshay123/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
