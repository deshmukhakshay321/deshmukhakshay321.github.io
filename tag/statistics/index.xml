<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics | Vishnu Bharadwaj</title>
    <link>https://vishnubharadwaj00.github.io/tag/statistics/</link>
      <atom:link href="https://vishnubharadwaj00.github.io/tag/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 19 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://vishnubharadwaj00.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Statistics</title>
      <link>https://vishnubharadwaj00.github.io/tag/statistics/</link>
    </image>
    
    <item>
      <title>What makes a good movie, asked Bayes</title>
      <link>https://vishnubharadwaj00.github.io/project/bayes/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/bayes/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/What-makes-a-good-movie-asked-Bayes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;how-to-make-a-good-movie&#34;&gt;How to make a good movie&lt;/h1&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;load-packages&#34;&gt;Load packages&lt;/h3&gt;
&lt;p&gt;Let us load the 4 packages needed for this analysis. ggplot2 and gridExtra are required for the data visualizations, dplyr is needed for data manipulation and wrangling, statsr consists of all the statistical functions needed, BAS has the Bayesian functions and MASS contains the stepAIC function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;library(ggplot2)
library(gridExtra)
library(dplyr)
library(statsr)
library(BAS)
library(MASS)

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;load-data&#34;&gt;Load data&lt;/h3&gt;
&lt;p&gt;The dataset can be loaded in two ways, either by using going to File-&amp;gt;Open File and clicking on the R Workspace file to load the data, or using the load() function. Here, we use the latter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;load(&amp;quot;movies.Rdata&amp;quot;)
dim(movies)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset imported has 651 rows and 32 columns.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-1-data&#34;&gt;Part 1: Data&lt;/h2&gt;
&lt;p&gt;Rotten Tomatoes and the TomatometerT rating is the most trusted measurement of quality entertainment. As the leading online aggregator of movie and TV show reviews from professional critics, Rotten Tomatoes offers the most comprehensive guide to what&amp;rsquo;s fresh. The world famous TomatometerT rating represents the percentage of positive professional reviews for films and TV shows and is used by millions every day, to help with their entertainment viewing decisions. Rotten Tomatoes designates the best reviewed movies and TV shows as Certified Fresh. That accolade is awarded with Tomatometer ratings of 75% and higher, and a required minimum number of reviews. Weekly Rotten Tomatoes podcasts can be found on RottenTomatoes.com, iTunes, Soundcloud and Stitcher, and Rotten Tomatoes&amp;rsquo; entertainment experts make regular TV and radio appearances across the US.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data Collection&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalizability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The present data were derived from an observational study. The data set is comprised of 651 randomly sampled movies produced and released from 1970 to 2014. According to IMDb, there have 9,962 movies been release from 1972 to 2016 so that the 10% condition (9,962*0.01 = 996) is met. Since the sampling size is large enough and less than 10% of population, it can assume that the random sampling is conducted. Therefore we can conclude that the sample is indeed generalizable to the entire population.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Causality&lt;/strong&gt;
The data cannot be used to establish a causal relation between the variables of interest as there was no random assignment to the explanatory and independent variables.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-2-data-manipulation&#34;&gt;Part 2: Data manipulation&lt;/h2&gt;
&lt;p&gt;In the original dataset , not all of the required features have been provided, so we will perform some feature engineering to create the required features. For the analysis, new features as oscar_season, summer_season, mpaa_rating_R, drama and feature_film. All of them can be derived from existing variables in the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;movies &amp;lt;-movies %&amp;gt;% mutate(feature_film = as.factor(ifelse(title_type == &amp;quot;Feature Film&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(drama = as.factor(ifelse(genre == &#39;Drama&#39;, &#39;Yes&#39;, &#39;No&#39;)))
movies &amp;lt;- movies %&amp;gt;% mutate(mpaa_rating_R=as.factor(ifelse(mpaa_rating==&amp;quot;R&amp;quot;,&amp;quot;Yes&amp;quot;,&amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(oscar_season=as.factor(ifelse(thtr_rel_month %in% c(&#39;10&#39;,&#39;11&#39;,&#39;12&#39;),&amp;quot;Yes&amp;quot;,&amp;quot;No&amp;quot;)))
movies &amp;lt;- movies %&amp;gt;% mutate(summer_season=as.factor(ifelse(thtr_rel_month %in% c(&#39;6&#39;,&#39;7&#39;,&#39;8&#39;),&amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-3-exploratory-data-analysis&#34;&gt;Part 3: Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;Firstly, let us see the analyze the audience_score variable:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Audience Score:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$audience_score)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The lowest rating is 11 (Battlefield Earth) and the highest rating is 97.00 (The Godfather Part 2).
The median score is 65.00, with a mean of 62.36.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=movies,aes(x=audience_score)) + geom_histogram(binwidth=5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a binwidth=5, we get a readable display. It is evident that this data is left-skewed, with more values on the right side of the mean than the left.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=movies,aes(x=audience_score)) + geom_density()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot clearly shows the left skew of the audience_score variable.&lt;/p&gt;
&lt;p&gt;Now, using the variables created in the previous section, plots and summary statistics make it easier to understand the data we have created.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Feature Film:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it is a Feature Film and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$feature_film)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows that there is mainly a huge majority of feature films. Actually, (591*100)/651= 90.738 % of the data are feature films.&lt;/p&gt;
&lt;p&gt;Plotting this data against audience_score and IMDB rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g1=ggplot(data=movies,aes(x=feature_film,y=audience_score,fill=feature_film)) +geom_boxplot()
g2=ggplot(data=movies,aes(x=feature_film,y=imdb_rating,fill=feature_film)) + geom_boxplot()
grid.arrange(g1,g2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there are fewer feature films, the distribution shows that feature films generally have a lower score than non-feature films. But this could also be attributed to the fewer number of feature films.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Drama:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it is a Drama movie and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;p&gt;Let us check the summary statistics for the drama variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$drama)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the number of drama and non-drama movies are close in count, but there are slightly more non-drama movies.&lt;/p&gt;
&lt;p&gt;Plotting this variable against audience_score and IMDB rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g3=ggplot(data=movies,aes(x=drama,y=audience_score,fill=drama)) +geom_boxplot()
g4=ggplot(data=movies,aes(x=drama,y=imdb_rating,fill=drama)) + geom_boxplot()
grid.arrange(g3,g4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There isn&amp;rsquo;t a huge difference but the non-drama movies have slightly lower media score compared to the drama movies. The non-drama movies are also slightly more distributed, but not by a whole lot.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MPAA Rating:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if the movies has an R MPAA Rating and a &amp;ldquo;No&amp;rdquo; value if it is not R-rated.&lt;/p&gt;
&lt;p&gt;The summary statistics for the MPAA rating:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$mpaa_rating_R)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The number of R-rated movies and movies with other ratings are very close.&lt;/p&gt;
&lt;p&gt;Plotting this variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g5=ggplot(data=movies,aes(x=mpaa_rating_R,y=audience_score,fill=mpaa_rating_R)) +geom_boxplot()
g6=ggplot(data=movies,aes(x=mpaa_rating_R,y=imdb_rating,fill=mpaa_rating_R)) + geom_boxplot()
grid.arrange(g5,g6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ratings are very similar for R-rated and non R-rated movies.&lt;/p&gt;
&lt;p&gt;Their distributions are also extremely similar, with not much to split the two variables.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Oscar Season:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it was released in the Oscar season and a &amp;ldquo;No&amp;rdquo; value if it was not released in the Oscar season.&lt;/p&gt;
&lt;p&gt;The summary of the Oscar variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$oscar_season)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are fewer movies that are released in the Oscar season and only about (191*100)/651=29.3394% are released in the Oscar season.&lt;/p&gt;
&lt;p&gt;Let us plot the variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g7=ggplot(data=movies,aes(x=oscar_season,y=audience_score,fill=oscar_season)) +geom_boxplot()
g8=ggplot(data=movies,aes(x=oscar_season,y=imdb_rating,fill=oscar_season)) + geom_boxplot()
grid.arrange(g7,g8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are slightly higher scores for the movies released in the Oscar Season, but the distributions seem similar.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Summer season:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This variable contains a &amp;ldquo;Yes&amp;rdquo; value if it was released in the Oscar season and a &amp;ldquo;No&amp;rdquo; value if it is not.&lt;/p&gt;
&lt;p&gt;The summary of the summer variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(movies$summer_season)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, most movies are not released in the summer months. Only about (164*100)/651= 25.192% of the movies are released in the summer.&lt;/p&gt;
&lt;p&gt;Plotting this variable against ratings:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g9=ggplot(data=movies,aes(x=summer_season,y=audience_score,fill=summer_season)) +geom_boxplot()
g10=ggplot(data=movies,aes(x=summer_season,y=imdb_rating,fill=summer_season)) + geom_boxplot()
grid.arrange(g9,g10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plots are almost identical, with very minute differences between them, if any. The movies not released in the summer season have very slightly higher scores, but the difference looks insignificant.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-4-modeling&#34;&gt;Part 4: Modeling&lt;/h2&gt;
&lt;p&gt;The best model is not always the most complicated. Sometimes including variables that are not evidently important, can actually reduce the accuracy of predictions. In practice, the model that includes all available explanatory variables is often referred to as the full model. The full model may not be the best model, and if it isn&amp;rsquo;t, we want to identify a smaller model that is preferable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Full model:&lt;/em&gt;
audience_score ~ feature_film + drama + runtime + mpaa_rating_R + thtr_rel_year + oscar_season + summer_season + imdb_rating + imdb_num_votes + critics_score + best_pic_nom + best_pic_win + best_actor_win + best_actress_win + best_dir_win + top200_box&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayesian Model Averaging (BMA):&lt;/em&gt;
A comprehensive approach to address model uncertainty is Bayesian model averaging, which allows us to assess the robustness of results to alternative specifications by calculating posterior distributions over coefficients and models. Given the 17 features (n) there can be 2^n = 2^17 possible models. We will explore model uncertainty using posterior probabilities of models based on BIC.
We will use BIC as a way to approximate the log of the marginal likelihood. The Bayesian information criterion (BIC) runs through several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2log-likelihood + nparlog(nobs), where npar represents the number of parameters and nobs the number of observations in the fitted model.&lt;/p&gt;
&lt;p&gt;Seperating the required features into a seperate dataframe:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;features &amp;lt;- c(&#39;audience_score&#39;, &#39;feature_film&#39;, &#39;drama&#39;, &#39;runtime&#39;, &#39;mpaa_rating_R&#39;, &#39;thtr_rel_year&#39;, &#39;oscar_season&#39;, &#39;summer_season&#39;, &#39;imdb_rating&#39;, &#39;imdb_num_votes&#39;, &#39;critics_score&#39;, &#39;best_pic_nom&#39;, &#39;best_pic_win&#39;, &#39;best_actor_win&#39;, &#39;best_actress_win&#39;, &#39;best_dir_win&#39;,&#39;top200_box&#39;)
moviesmodel=movies[ , features]
summary(moviesmodel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seem to be no NA values, so we can proceed with the model selection process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian Information Criterion:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we create a multiple linear regression model, with all the factors included.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audmodel=lm(audience_score~. , data=moviesmodel)
summary(audmodel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very evident that the number of factors that are not useful is very high. We can use the BIC (Bayesian Information Criterion) to eliminate the factors that are not significant in this model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audienceBIC=bas.lm(audience_score~ ., data=moviesmodel,prior=&amp;quot;BIC&amp;quot;,modelprior=uniform())
audienceBIC
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These values denote the marginal posterior inclusion probabilities. We can actually see that IMDB rating and critics score do play a big role.&lt;/p&gt;
&lt;p&gt;From this object, we can get the top 5 most probable models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(audienceBIC)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the most probable model contains only 3 variables, runtime, IMDB score and Critics score. The second most probable model contains 2 variables, IMDB score and Critics score.&lt;/p&gt;
&lt;p&gt;The posterior probability for the top 2 most probably models are about 27%.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Coeffecients:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can extract the coeffecients from the Bayesian model into a seperate variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audiencecoeff=coef(audienceBIC)
#95% Credible Intervals for coeffecients:
audinterval=confint(audiencecoeff)
audinterval
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us look at what the 3 most important variables mean.&lt;/p&gt;
&lt;p&gt;For every 1 point increase in the runtime, the audience score is -2.5e-02 minutes lesser. Similarly, for every 1 point increase in the IMDB rating the audience score is 1.49e+01 points more. And for the Critics score there is an audience score increase of 6.33e-02 points.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Model space:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can visualize the model space using the image() function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;image(audienceBIC,rotate=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to size constraints, all 17 variables are not shown in this picture. By opening this plot in a new window, all 17 variables are visible and it is evident that &amp;lsquo;runtime + imdb_rating + critics_score&amp;rsquo; is the best model. Also imdb rating and critics score are present in all the top models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zellner-Siow Cauchy:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using Zellner-Siow Cauchy, with an MCMC method, we can get a different model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audiencezs=bas.lm(audience_score~ .,  data=moviesmodel,prior=&amp;quot;ZS-null&amp;quot;,modelprior=uniform(),method=&amp;quot;MCMC&amp;quot;)
summary(audiencezs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the most probable model, with a posterior probability of 14% has only IMDB rating and Critics score. And the 2nd most probable model, with a posterior probability of 12% has runtime, IMDB rating and Critics score. These results are very similar to the BIC method, with only a swap in the first two models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Model space:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can visualize the models created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;image(audiencezs,rotate=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, expanding the image, we can see that IMDB rating and Critics score are the major factors. Here, runtime doesn&amp;rsquo;t seem to be playing a huge role.&lt;/p&gt;
&lt;p&gt;So we can clearly see that while BIC proposes 3 variables (runtime, imdb_rating, critics_score), the ZSC method only proposes 2 (imdb_rating, critics_score).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AIC Model Selection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Aikake information criterion (AIC) is a measure of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Hence, AIC provides a means for model selection.&lt;/p&gt;
&lt;p&gt;We can use backward elimination to find the best model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;lmaic=lm(audience_score~ ., data=moviesmodel)
audienceaic=stepAIC(lmaic,direction=&#39;backward&#39;,trace=FALSE)
audienceaic$anova
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that there are a lot more variables in this method: summer_season, top200_box, best_dir_win, best_pic_win, oscar_season, feature_film, drama, imdb_num_votes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audienceaic$coefficients
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, for example, for every 1 point increase in the IMDB rating, the Audience Score increases by 15 points, which is a lot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Final Model:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our final model, we are going to use IMDB ratings and Critics score as the two variables, with the Zellner-Siow Cauchy prior, and MCMC method, with 10^6 iterations of MCMC.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;feat=c(&amp;quot;audience_score&amp;quot;,&amp;quot;imdb_rating&amp;quot;,&amp;quot;critics_score&amp;quot;)
moviesfinal=movies[,feat]
audiencezsfin=bas.lm(audience_score~.,data=moviesfinal,prior=&amp;quot;ZS-null&amp;quot;,modelprior=uniform(),method=&amp;quot;MCMC&amp;quot;,MCMC.iteration=10^6)
summary(audiencezsfin)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that in this, the first model, which has both variables, has 89% posterior probability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model Diagnostics:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will now look at the best model created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;diagnostics(audiencezs, type = &amp;quot;model&amp;quot;, pch = 16, cex = 1.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It shows about a straight line at the intercept, which shows a converged posterior probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which = 1, pch=16)
abline(a = 0, b = 0, lwd = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There seem to be few outliers, but the points are randomly scattered across the 0 line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which=2,add.smooth = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability starts to straigthen around the 800th model appx, meaning all the models after that don&amp;rsquo;t make much of a difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which=3, ask=F)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These log marginal probabilites are pretty evenly distributed, somewhat favouring 4 or 5 factors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;plot(audiencezs, which = 4, ask = F, col.in = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see again, that imdb_rating and critics_score matter the most.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-5-prediction&#34;&gt;Part 5: Prediction&lt;/h2&gt;
&lt;p&gt;The movie we are going to pick is Money Monster, a movie directed by Jodie Foster, starring George Clooney and Julia Roberts. (&lt;a href=&#34;https://www.rottentomatoes.com/m/money_monster/&#34;&gt;https://www.rottentomatoes.com/m/money_monster/&lt;/a&gt;) The audience score is 60%.&lt;/p&gt;
&lt;p&gt;We create the BMA object first:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;BMA=predict(audiencezsfin,estimator=&amp;quot;BMA&amp;quot;, se.fit=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a data frame with the values to be predicted:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;pred=data.frame(imdb_rating=6.5,critics_score=58)
aud=predict(audiencezsfin,newdata=pred,estimator=&amp;quot;BMA&amp;quot;,se.fit=TRUE)
aud$fit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get an estimated audience score of 62.49, which is a little higher than the actual score&lt;/p&gt;
&lt;p&gt;The 95% credible interval is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;audinterval=confint(aud,parm=&amp;quot;mean&amp;quot;)
round(audinterval,3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a confidence interval close to the 60% we were looking for.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-6-conclusion&#34;&gt;Part 6: Conclusion&lt;/h2&gt;
&lt;p&gt;The predictive model presented here is used to predict the audience scores for a movie. Using Bayesian model averaging and many factors like BIC, ZSC, AIC, etc, many models can be constructed to perform better predictions.&lt;/p&gt;
&lt;p&gt;The proposed linear model shows a &amp;lsquo;fairly good&amp;rsquo; prediction rate, but it should be noted that the model is based on a very small sample. The fact is that imdb_rating has the highest posterior probability, and that basically all of the newly created features were not that useful to support a better prediction. Creating a model, which has a high predictive power is not so easy to reach. Using Bayes for better prediction is only one part of the game. It might be beneficial to gather more data or try to extend the feature engineering part, which means to creating new meaningful features from existing or gather data for new features.&lt;/p&gt;
&lt;p&gt;Perhaps in a future project, for higher accuracy, we could have included all the remaining factors as well, which was done in the project for the 3rd course of this specialization, and then eliminated them one by one. Even though such models might be prone to overfitting or underfitting, these problems can certainly be mitigated using expert opinion on which factors are actually useful.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Literacy Rate Analysis using GSS Data</title>
      <link>https://vishnubharadwaj00.github.io/project/litrates/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://vishnubharadwaj00.github.io/project/litrates/</guid>
      <description>&lt;p&gt;&lt;em&gt;Full code can be accessed at the 
&lt;a href=&#34;https://github.com/vishnubharadwaj00/Literacy-Rate-Analysis-Using-GSS-Dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;load-packages&#34;&gt;Load packages&lt;/h3&gt;
&lt;p&gt;For this project, we require ggplot2 and gridExtra package for the plots, dplyr for data manipulation and statsr for the statistical functions used in this course. These three packages contain a wide range of functions to be used, and should encompass all the functions we require to do this project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;library(ggplot2)
library(gridExtra)
library(dplyr)
library(statsr)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;load-data&#34;&gt;Load data&lt;/h3&gt;
&lt;p&gt;We can either go to File -&amp;gt; Open File and select our RData file or load the RData file as it is. This automatically imports our gss dataset into the workspace, as gss. The latter method is used here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34;&gt;load(&amp;quot;gss.Rdata&amp;quot;)
dim(gss)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now loaded a dataset with 57061 rows and 114 columns.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-1-data&#34;&gt;Part 1: Data&lt;/h2&gt;
&lt;p&gt;According to the GSS website, the General Social Survey (GSS) has studied the growing complexity of American society. It is the only full-probability, personal-interview survey designed to monitor changes in both social characteristics and attitudes currently being conducted in the United States.&lt;/p&gt;
&lt;p&gt;The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.&lt;/p&gt;
&lt;p&gt;As to how the data is collected, the GSS sample is drawn using an area probability design that randomly selects respondents in households across the United States to take part in the survey. The respondents are from a mix of urban, suburban and rural geographic areas.
Therefore, random sampling does take place in this survey, meaning the results of the survey can to an extent, be generalized to the adult population of the United States.&lt;/p&gt;
&lt;p&gt;There are a few things which might not make it completely accurate. Firstly, the GSS is strictly voluntary, meaning even when selected, participants may choose to not attend the survey. Secondly, up until a few years ago, only English speaking participants were chosen, and now Spanish speaking participants have been added. This raises another concern about other languages, as well.&lt;/p&gt;
&lt;p&gt;Random assignment has not been used here to seperate the sampled population into further groups, which means that causality cannot be inferred from the results of this survey, since we cannot be sure that the only difference between different groups is what we are studying.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-2-research-question&#34;&gt;Part 2: Research question&lt;/h2&gt;
&lt;p&gt;With such a vast trove of data, it is possible to create many interesting and insightful research questions, each with their own meaningful conclusions.&lt;/p&gt;
&lt;p&gt;Here, I will focus on one particular area: education levels. We will analyzing 3 questions based on this area:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 1: As of 2012 (latest year of the survey), is there a disparity in the education provided to males and females?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, for the 2012 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 2012?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 2: In 1972 (first year of the survey), was there a disparity in the education provided to males and females?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In other words, for the 1972 subset,is there any difference between the education levels of males and females? Is there any correlation between education level and gender in 1972?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 3: Is there a difference in the education levels of 1972 and 2012?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taking gender out of the equation, is there a difference in education levels of the years 1972 and 2012? Has the situation of education improved or declined in those 50 years?&lt;/p&gt;
&lt;p&gt;This is becoming an all-important question in today&amp;rsquo;s world, with rising calls for equality between men and women. Education is an essential part of gaining knowledge, and an equal footing in education can open up equal opportunities, for men and women. Educational reforms have also been implemented over the years, to provide a higher level of education for all. But has education actually improved, overall?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-3-exploratory-data-analysis&#34;&gt;Part 3: Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Question 1:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First let us use only the section of the data that needs to be used, the 2012 data, by filtering it into another dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012 = gss %&amp;gt;%
  filter(year==&amp;quot;2012&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2012 is the latest year of the survey, providing us with the latest results to this research question.&lt;/p&gt;
&lt;p&gt;Let us see a summary of the education levels in the new subset of data we have:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;summary(gss2012$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the mean is 13.53, the maximum is 20, and the minimum is 0, with a median of 13.
There are also 2 NA&amp;rsquo;s, out of 1974 entries, which is around 1% of the dataset, so we can remove those 2 rows to increase accuracy without affecting the accuracy of the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012= gss2012 %&amp;gt;%
  filter(educ!=&amp;quot;NA&amp;quot;)
summary(gss2012$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the NA values have been successfully removed.&lt;/p&gt;
&lt;p&gt;Let us look at the means and medians of education levels, grouped by gender:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss2012 %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarise(meaned=mean(educ),medec=median(educ))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of female education level is slightly higher than males, and the median is the same for both.&lt;/p&gt;
&lt;p&gt;Let us create a simple boxplot to visualise these statistics, helping us get a better understanding:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss2012 ,aes(x=factor(sex),y=educ)) + geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The boxplot does not seem to show much of a change between the two genders, with the two boxplots looking almost identical.&lt;/p&gt;
&lt;p&gt;A barplot to compare the counts of the education levels, with grouping done on gender might help us understand the data better:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss2012,aes(x=educ,fill=sex)) +geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that as the number of years of education is more than 10, females tend to get more education that men in that education level, except on one or two levels, but the difference is not a lot in most cases.&lt;/p&gt;
&lt;p&gt;Looking at the maximum and minimum, there are slightly more females than males who get 0 years of education, but at the maximum of 20 years, the difference is almost non-existent.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 2:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let us now look at the data from 1972, by subsetting it into a seperate data frame, and doing a similar EDA on it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972=gss %&amp;gt;%
  filter(year==&amp;quot;1972&amp;quot;)
summary(gss1972$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get 5 NA values out of a total of 1613 values, which is around 3% of the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972 = gss1972 %&amp;gt;%
  filter(gss1972$educ!=&amp;quot;NA&amp;quot;)
summary(gss1972$educ)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Already there is a clear difference in the median and mean, without looking at the gender difference.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gss1972 %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarise(meaned=mean(educ),medianed=median(educ))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of education levels of females was lower than the males in 1972, a sharp contrast to the results in 2012.&lt;/p&gt;
&lt;p&gt;Let us see a simple boxplot to visualise these statistics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss1972,aes(x=sex,y=educ)) + geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very evident from this that, although the male median education level and female median education level are almost the same, the average education level for males is definitely higher, and the 75th percentile level is much higher than that of females.&lt;/p&gt;
&lt;p&gt;Creating a plot to see each year of education:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gss1972,aes(x=educ,fill=sex)) +geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In 1972, it is evident that beyond 14 years of education, there are more men than women. These results are much different to the 2012 results.&lt;/p&gt;
&lt;p&gt;Comparing the two datasets using scatterplots:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;g1=ggplot(data=gss1972,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(&amp;quot;1972&amp;quot;)
g2=ggplot(data=gss2012,aes(x=sex,y=educ))+geom_point()+geom_jitter()+ggtitle(&amp;quot;2012&amp;quot;)
grid.arrange(g1,g2,ncol=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 2 scatter plots on the left are the 1972 data, arranged according to sex, and the 2 scatter plots on the right are the 2012 data, arranged according to sex. It is clear that in 2012, there is a higher concentration at a higher education level, whereas in 1972, that is only for males.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 3:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, to analyse the difference in education levels overall, between 1972 and 2012.&lt;/p&gt;
&lt;p&gt;First let us combine them into a single dataset to make the analysis easier:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;gssdata=rbind(gss1972,gss2012)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already have the datasets as well as their summary statistics, so all that is left is to plot them:&lt;/p&gt;
&lt;p&gt;Creating a simple boxplot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gssdata,aes(x=factor(year),y=educ))+geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Immediately, it is evident that there is a significant difference in the education levels between the two years. Although the median of 2012 is only slightly higher than 1972, the range and IQR of 2012 is much higher.&lt;/p&gt;
&lt;p&gt;Let us create a bar graph between the two years:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;ggplot(data=gssdata,aes(x=educ,fill=factor(year)))+geom_bar(position=&amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upto about 12 years of education, 1972 had a higher proportion, but after 12 years, 2012 had the higher proportion. This means that more people are getting more education in 2012, compared to 1972.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-4-inference&#34;&gt;Part 4: Inference&lt;/h2&gt;
&lt;p&gt;First, let us check the conditions for doing a hypothesis test using the CLT method:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.Independence:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Within groups:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Random sampling was used.&lt;/p&gt;
&lt;p&gt;1972 observations is well below 10% of the population.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Between groups:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The two groups are independent of each other, as it is a representative of the population. Also the likelihood of dependence otherwise, is very small with this sample size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.Sample Size/Skew:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sample is slightly skewed, and n is well above 30.&lt;/p&gt;
&lt;p&gt;Therefore the conditions for CLT are satisfied and we can use the theoretical method.&lt;/p&gt;
&lt;p&gt;The method to be used for all 3 questions is that to be used when comparing 2 independent means, as independence has already been established. The critical score will be the t-score corresponding to the degree of freedom. The degree of freedom is the $df =min(n_1 -1 ,n_2 - 1)$. Here, $n_1$=884 and $n_2$=1088, so the degree of freedom is 883.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Question 1:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 2012.
$\mu_{male(2012)}-\mu_{female(2012)}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 2012.
$\mu_{male(2012)}-\mu_{female(2012)} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{male(2012)}$ and $\mu_{female(2012)}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{male(2012)}$ and $\bar{x}_{female(2012)}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss2012,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results:
Response variable: numerical
Explanatory variable: categorical (2 levels)
n_Male = 884, y_bar_Male = 13.5057, s_Male = 3.1721
n_Female = 1088, y_bar_Female = 13.546, s_Female = 3.0904
H0: mu_Male = mu_Female
HA: mu_Male != mu_Female
t = -0.2838, df = 883
p_value = 0.7766&lt;/p&gt;
&lt;p&gt;The high p-value of 0.7766 which is much greater than 0.05, means we &lt;strong&gt;fail to reject the null hypothesis $H_0$&lt;/strong&gt;. We might still run the risk of a type 2 error, but the big p-value offsets the effects of a larger significance level. What the p-value here indicates is the probability of observing extreme data given that the null hypothesis is true, is high.&lt;/p&gt;
&lt;p&gt;We can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss2012,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This 95% Confidence Interval means that we are 95% confident that the difference in the education levels of males and females is between -0.319 and 0.2384. The - sign shows that females getting more education that males.&lt;/p&gt;
&lt;p&gt;This confidence interval method is well in agreement with the hypothesis test method as the 0 is in the confidence interval that has been produced. The differences shown by the Confidence Interval method are also not that significant, which are the same results that can be inferred from the hypothesis test method.&lt;/p&gt;
&lt;p&gt;In conclusion, the results of both the confidence interval method and the hypothesis test method indicate that &lt;strong&gt;there is an insignificant difference in the education levels of males and females in 2012.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 2:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 1972.
$\mu_{male(1972)}-\mu_{female(1972)}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 1972.
$\mu_{male(1972)}-\mu_{female(1972)} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{male(1972)}$ and $\mu_{female(1972)}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{male(1972)}$ and $\bar{x}_{female(1972)}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss1972,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this inference, we get a p-value of 0.01, which is much lower than our significance level of 0.05, which means we &lt;strong&gt;reject the $H_0$ hypothesis&lt;/strong&gt;. Again, we run the risk of a Type-1 error, but even a smaller significance level will not give a different result.&lt;/p&gt;
&lt;p&gt;We can also create a confidence interval of the difference in the education levels between males and females. We can use the same function, with some small modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=sex,data=gss1972,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results show that males get about 0.07 to 0.75 years more education then females. Although this does not seem like a lot, since it is above the significance level of 0.05, it is a significant difference, as the difference when extrapolated to the entire population, becomes much bigger.&lt;/p&gt;
&lt;p&gt;The conclusion from this test is that &lt;strong&gt;there was a significant difference in the education levels of males and females in 1972&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Question 3:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The null and alternative hypothesis are as follows:&lt;/p&gt;
&lt;p&gt;$H_{0}$ : There is no difference in the education levels of males and females in 2012.
$\mu_{1972}-\mu_{2012}=0$&lt;/p&gt;
&lt;p&gt;$H_{a}$ There is a difference in the education levels of males and females in 2012.
$\mu_{1972}-\mu_{2012} \ne 0$&lt;/p&gt;
&lt;p&gt;Our parameters of interest are $\mu_{1972}$ and $\mu_{2012}$ but since we do not have access to that, we will use our point estimates $\bar{x}_{1972}$ and $\bar{x}_{2012}$.&lt;/p&gt;
&lt;p&gt;The significance level $\alpha=0.05$, which is the standard $\alpha$.&lt;/p&gt;
&lt;p&gt;Using the inference function to calculate the p-value for this hypothesis test:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=factor(year),data=gssdata,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ht&amp;quot;,null=0,alternative=&amp;quot;twosided&amp;quot;,method=&amp;quot;theoretical&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value obtained is extremely small, &amp;lt;0.0001, which is obviously lesser than the significance level of 0.05, hence we &lt;strong&gt;reject the $H_0$ hypothesis&lt;/strong&gt;. The risk of a type 1 error is even smaller here, as it is such a small p-value.&lt;/p&gt;
&lt;p&gt;A confidence interval can also be created similar to the above questions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;inference(y=educ,x=factor(year),data=gssdata,statistic=&amp;quot;mean&amp;quot;,type=&amp;quot;ci&amp;quot;,method=&amp;quot;theoretical&amp;quot;,conf_level=0.95)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confidence interval indicates that there is a 95% chance that on average, people in 2012 had an 1.9825 to 2.4191 more years of education than people in 2012. This is in agreement with the hypothesis test done earlier.&lt;/p&gt;
&lt;p&gt;Therefore, it can be concluded that &lt;strong&gt;there is a significant difference in the education levels of 1972 and 2012&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;part-5-conclusion&#34;&gt;Part 5: Conclusion&lt;/h2&gt;
&lt;p&gt;It can be concluded that:
(1) There is an insignificant difference in the education levels of males and females in the year 2012, with a 95% confidence interval of (-0.319,0.2384).&lt;/p&gt;
&lt;p&gt;(2) There is a significant difference in the education levels of males and females in the year 1972, with a 95% confidence interval of (0.0783,0.7542).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
